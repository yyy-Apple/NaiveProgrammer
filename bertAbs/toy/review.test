- This paper simply proposes to use UNet for the segmentation of stagnant zones in X-ray CTs. While the applicability of this model may represent an advance in the particular field of the authors, the technical contribution of this paper is far from the level expected in this conference. - As the paper reads, the main contribution of the paper is the modified version of UNet 'proposed' by the authors, which major modification consists on replacing SGD by Adam. Nevertheless, this cannot be considered a contribution, as changing the optimizer in a deep model is a marginal change, from a methodological point of view. - Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length).- The submission is not anonymized (authors included their names and affiliations).
The authors try to address the issue of data efficiency in deep reinforcement learning by meta-learning a reinforcement learning algorithm using a hand-designed reinforcement learning algorithm (TRPO in this case). The experiments suggest comparable performance to models with prior knowledge of the distribution over environments for bandit tasks, and experiments on random maze navigation from vision is shown as well, though the random maze experiments would benefit from a clearer explanation. It was not obvious from the text how their experiments supported the thesis of the paper that the learned RL algorithm was effectively performing one-shot learning. The subject of the paper is also strikingly similar to the recently-posted paper Learning to Reinforcement Learn (https://arxiv.org/pdf/1611.05763.pdf), and while this paper was posted after the ICLR deadline, the authors should probably update the text to reflect the state of this rapidly-advancing field.
Summary:The paper presents an analysis and numerical evaluation of SGD with momentum for non-convex optimization problems. In particular, the main contribution of the paper, as the authors claim in the abstract, is showing that stochastic momentum improves deep network training because it modifies SGD to escape saddle points faster. Comments:SGD with heavy ball momentum (or stochastic heavy ball method) is one of the most popular methods for training neural networks. As the authors mentioned the method is widely employed in practice, especially in the area of deep learning. However, despite the popularity of the method both in convex and non-convex optimization, its convergence properties are not very well understood.  I consider any meaningful direction for understanding the convergence of this method a nice fit to ICLR, however i find the presentation of this paper somehow confusing, especially Section 3 which is supposed to be the main contribution of the paper. 1) I understand the motivation of the authors and what they tried to communicate but i find that there is no satisfactory explanation of what Theorem 1 is actually saying. For example In case that $\beta=0$ the method is the popular stochastic gradient descent method. Does the theorem covers the known results of SGD appeared in previous works? 2) In addition, the theorem states" If SGD with momentum (Algorithm 2) has ....APAG...APCG_T...GrACE ...". How we can guarantee that the above 3 conditions is satisfied from the Algorithm 2? There is also no satisfactory explanation of why the authors analyze Algorithm 2 and not Algorithm 1. 3) The presentation of Section 3.2.1 is also not clear. I am suggesting to the authors to explain in more details the theoretical results of their paper and highlight why the 5 lemmas of this section are important to be in the main part of the paper. I think a notation subsection will be useful for the reader.4) I am suggesting the authors to include in the introduction the more known variant of SGD with momentum:$$\omega_{t+1}=\omega_t-\eta \nabla f(\omega_t, \xi_t)+\beta(\omega_t-\omega_{t-1})$$5) The authors name their methods "SGD with stochastic momentum". The method is either "SGD with momentum" or "Stochastic heavy ball method". SGD with stochastic momentum is something different (see for example [Loizou, Richtarik 2017] from paper's reference section)Minor Comment:page 3, bellow eq(4): the first (3) ----> Problem (3)Missing references:The authors did an excellent work on reviewing the literature review on SGD with momentum. Bellow find three recent related works that could be mentioned:Aybat, Necdet Serhat, Alireza Fallah, Mert Gurbuzbalaban, and Asuman Ozdaglar. "A universally optimal multistage accelerated stochastic gradient method." arXiv preprint arXiv:1901.08022 (2019).Loizou, Nicolas, and Peter Richtárik. "Linearly convergent stochastic heavy ball method for minimizing generalization error." arXiv preprint arXiv:1710.10737 (2017).Loizou, Nicolas, and Peter Richtárik. "Accelerated gossip via stochastic heavy ball method." In 2018 56th Annual Allerton Conference on Communication, Control, and Computing (Allerton), pp. 927-934. IEEE, 2018.========= after rebuttal ============= I would like to thank the authors for the reply. After reading their response and the comments of the other reviewers I  decide to update my score to "weak accept".
- This paper simply proposes to use UNet for the segmentation of stagnant zones in X-ray CTs. While the applicability of this model may represent an advance in the particular field of the authors, the technical contribution of this paper is far from the level expected in this conference. - As the paper reads, the main contribution of the paper is the modified version of UNet 'proposed' by the authors, which major modification consists on replacing SGD by Adam. Nevertheless, this cannot be considered a contribution, as changing the optimizer in a deep model is a marginal change, from a methodological point of view. - Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length).- The submission is not anonymized (authors included their names and affiliations).
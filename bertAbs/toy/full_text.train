In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixels (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), and acquiring advanced manipulation and locomotion skills (Levine et al., 2016; Lillicrap et al., 2015; Watter et al., 2015; Heess et al., 2015b; Schulman et al., 2015; 2016). However, many of the successes come at the expense of high sample complexity. For example, the state-of-the-art Atari results require tens of thousands of episodes of experience (Mnih et al., 2015) per game. To master a game, one would need to spend nearly 40 days playing it with no rest. In contrast, humans and animals are capable of learning a new task in a very small number of trials. Continuing the previous example, the human player in Mnih et al. (2015) only needed 2 hours of experience before mastering a game. We argue that the reason for this sharp contrast is largely due to the lack of a good prior, which results in these deep RL agents needing to rebuild their knowledge about the world from scratch.Although Bayesian reinforcement learning provides a solid framework for incorporating prior knowledge into the learning process (Strens, 2000; Ghavamzadeh et al., 2015; Kolter & Ng, 2009), exact computation of the Bayesian update is intractable in all but the simplest cases. Thus, practical reinforcement learning algorithms often incorporate a mixture of Bayesian and domain-specific ideas to bring down sample complexity and computational burden. Notable examples include guided policy search with unknown dynamics (Levine & Abbeel, 2014) and PILCO (Deisenroth & Rasmussen, 2011). These methods can learn a task using a few minutes to a few hours of real experience, compared to days or even weeks required by previous methods (Schulman et al., 2015; 2016; Lillicrap et al., 2015). However, these methods tend to make assumptions about the environment (e.g., instrumentation for access to the state at learning time), or become computationally intractable in high-dimensional settings (Wahlström et al., 2015).Rather than hand-designing domain-specific reinforcement learning algorithms, we take a different approach in this paper: we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent. We structure the agent as a recurrent neural network, which receives past rewards, actions, and termination flags as inputs in addition to the normally received observations. Furthermore, its internal state is preserved across episodes, so that it has the capacity to perform learning in its own hidden activations. The learned agent thus also acts as the learning algorithm, and can adapt to the task at hand when deployed.We evaluate this approach on two sets of classical problems, multi-armed bandits and tabular MDPs. These problems have been extensively studied, and there exist algorithms that achieve asymptotically optimal performance. We demonstrate that our method, named RL2, can achieve performance comparable with these theoretically justified algorithms. Next, we evaluate RL2 on a vision-based navigation task implemented using the ViZDoom environment (Kempka et al., 2016), showing that RL2 can also scale to high-dimensional problems.We define a discrete-time finite-horizon discounted Markov decision process (MDP) by a tupleM = (S,A,P, r, ρ0, γ, T ), in which S is a state set, A an action set, P : S × A × S → R+ a transition probability distribution, r : S ×A → [−Rmax, Rmax] a bounded reward function, ρ0 : S → R+ an initial state distribution, γ ∈ [0, 1] a discount factor, and T the horizon. In policy search methods, we typically optimize a stochastic policy πθ : S × A → R+ parametrized by θ. The objective is to maximize its expected discounted return, η(πθ) = Eτ [ ∑T t=0 γtr(st, at)], where τ = (s0, a0, . . .) denotes the whole trajectory, s0 ∼ ρ0(s0), at ∼ πθ(at|st), and st+1 ∼ P(st+1|st, at).We now describe our formulation, which casts learning an RL algorithm as a reinforcement learning problem, and hence the name RL2.We assume knowledge of a set of MDPs, denoted byM, and a distribution over them: ρM :M→ R+. We only need to sample from this distribution. We use n to denote the total number of episodes allowed to spend with a specific MDP. We define a trial to be such a series of episodes of interaction with a fixed MDP.This process of interaction between an agent and the environment is illustrated in Figure 1. Here, each trial happens to consist of two episodes, hence n = 2. For each trial, a separate MDP is drawn from ρM, and for each episode, a fresh s0 is drawn from the initial state distribution specific to the corresponding MDP. Upon receiving an action at produced by the agent, the environment computes reward rt, steps forward, and computes the next state st+1. If the episode has terminated, it sets termination flag dt to 1, which otherwise defaults to 0. Together, the next state st+1, actionat, reward rt, and termination flag dt, are concatenated to form the input to the policy1, which, conditioned on the hidden state ht+1, generates the next hidden state ht+2 and action at+1. At the end of an episode, the hidden state of the policy is preserved to the next episode, but not preserved between trials.The objective under this formulation is to maximize the expected total discounted reward accumulated during a single trial rather than a single episode. Maximizing this objective is equivalent to minimizing the cumulative pseudo-regret (Bubeck & Cesa-Bianchi, 2012). Since the underlying MDP changes across trials, as long as different strategies are required for different MDPs, the agent must act differently according to its belief over which MDP it is currently in. Hence, the agent is forced to integrate all the information it has received, including past actions, rewards, and termination flags, and adapt its strategy continually. Hence, we have set up an end-to-end optimization process, where the agent is encouraged to learn a “fast” reinforcement learning algorithm.For clarity of exposition, we have defined the “inner” problem (of which the agent sees n each trials) to be an MDP rather than a POMDP. However, the method can also be applied in the partiallyobserved setting without any conceptual changes. In the partially observed setting, the agent is faced with a sequence of POMDPs, and it receives an observation ot instead of state st at time t. The visual navigation experiment in Section 3.3, is actually an instance of the this POMDP setting.We represent the policy as a general recurrent neural network. Each timestep, it receives the tuple (s, a, r, d) as input, which is embedded using a function φ(s, a, r, d) and provided as input to an RNN. To alleviate the difficulty of training RNNs due to vanishing and exploding gradients (Bengio et al., 1994), we use Gated Recurrent Units (GRUs) (Cho et al., 2014) which have been demonstrated to have good empirical performance (Chung et al., 2014; Józefowicz et al., 2015). The output of the GRU is fed to a fully connected layer followed by a softmax function, which forms the distribution over actions.We have also experimented with alternative architectures which explicitly reset part of the hidden state each episode of the sampled MDP, but we did not find any improvement over the simple architecture described above.After formulating the task as a reinforcement learning problem, we can readily use standard off-theshelf RL algorithms to optimize the policy. We use a first-order implementation of Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), because of its excellent empirical performance, and because it does not require excessive hyperparameter tuning. For more details, we refer the reader to the original paper. To reduce variance in the stochastic gradient estimation, we use a baseline which is also represented as an RNN using GRUs as building blocks. We optionally apply Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to further reduce the variance.We designed experiments to answer the following questions:• Can RL2 learn algorithms that achieve good performance on MDP classes with special structure, relative to existing algorithms tailored to this structure that have been proposed in the literature?• Can RL2 scale to high-dimensional tasks?For the first question, we evaluate RL2 on two sets of tasks, multi-armed bandits (MAB) and tabular MDPs. These problems have been studied extensively in the reinforcement learning literature, and this body of work includes algorithms with guarantees of asymptotic optimality. We demonstrate that our approach achieves comparable performance to these theoretically justified algorithms.1To make sure that the inputs have a consistent dimension, we use placeholder values for the initial input to the policy.For the second question, we evaluate RL2 on a vision-based navigation task. Our experiments show that the learned policy makes effective use of the learned visual information and also short-term information acquired from previous episodes.Multi-armed bandit problems are a subset of MDPs where the agent’s environment is stateless. Specifically, there are k arms (actions), and at every time step, the agent pulls one of the arms, say i, and receives a reward drawn from an unknown distribution: our experiments take each arm to be a Bernoulli distribution with parameter pi. The goal is to maximize the total reward obtained over a fixed number of time steps. The key challenge is balancing exploration and exploitation— “exploring” each arm enough times to estimate its distribution (pi), but eventually switching over to “exploitation” of the best arm. Despite the simplicity of multi-arm bandit problems, their study has led to a rich theory and a collection of algorithms with optimality guarantees.Using RL2, we can train an RNN policy to solve bandit problems by training it on a given distribution ρM. If the learning is successful, the resulting policy should be able to perform competitively with the theoretically optimal algorithms. We randomly generated bandit problems by sampling each parameter pi from the uniform distribution on [0, 1]. After training the RNN policy with RL2, we compared it against the following strategies:• Random: this is a baseline strategy, where the agent pulls a random arm each time.• Gittins index (Gittins, 1979): this method gives the Bayes optimal solution in the discounted infinite-horizon case, by computing an index separately for each arm, and taking the arm with the largest index. While this work shows it is sufficient to independently compute an index for each arm (hence avoiding combinatorial explosion with the number of arms), it doesn’t show how to tractably compute these individual indices exactly. We follow the practical approximations described in Gittins et al. (2011), Chakravorty & Mahajan (2013), and Whittle (1982), and choose the best-performing approximation for each setup.• UCB1 (Auer, 2002): this method estimates an upper-confidence bound, and pulls the arm with the largest value of ucbi(t) = µ̂i(t−1)+c √ 2 log t Ti(t−1) , where µ̂i(t−1) is the estimatedmean parameter for the ith arm, Ti(t−1) is the number of times the ith arm has been pulled, and c is a tunable hyperparameter (Audibert & Munos, 2011). We initialize the statistics with exactly one success and one failure, which corresponds to a Beta(1, 1) prior.• Thompson sampling (TS) (Thompson, 1933): this is a simple method which, at each time step, samples a list of arm means from the posterior distribution, and choose the best arm according to this sample. It has been demonstrated to compare favorably to UCB1 empirically (Chapelle & Li, 2011). We also experiment with an optimistic variant (OTS) (May et al., 2012), which samples N times from the posterior, and takes the one with the highest probability.• -Greedy: in this strategy, the agent chooses the arm with the best empirical mean with probability 1 − , and chooses a random arm with probability . We use the same initialization as UCB1.• Greedy: this is a special case of -Greedy with = 0.The Bayesian methods, Gittins index and Thompson sampling, take advantage of the distribution ρM; and we provide these methods with the true distribution. For each method with hyperparameters, we maximize the score with a separate grid search for each of the experimental settings. The hyperparameters used for TRPO are shown in the appendix.The results are summarized in Table 1. Learning curves for various settings are shown in Figure 2. We observe that our approach achieves performance that is almost as good as the the reference methods, which were (human) designed specifically to perform well on multi-armed bandit problems. It is worth noting that the published algorithms are mostly designed to minimize asymptotic regret (rather than finite horizon regret), hence there tends to be a little bit of room to outperform them in the finite horizon settings.We observe that there is a noticeable gap between Gittins index and RL2 in the most challenging scenario, with 50 arms and 500 episodes. This raises the question whether better architectures or better (slow) RL algorithms should be explored. To determine the bottleneck, we trained the same policy architecture using supervised learning, using the trajectories generated by the Gittins index approach as training data. We found that the learned policy, when executed in test domains, achieved the same level of performance as the Gittins index approach, suggesting that there is room for improvement by using better RL algorithms.The bandit problem provides a natural and simple setting to investigate whether the policy learns to trade off between exploration and exploitation. However, the problem itself involves no sequential decision making, and does not fully characterize the challenges in solving MDPs. Hence, we perform further experiments using randomly generated tabular MDPs, where there is a finite number of possible states and actions—small enough that the transition probability distribution can be explicitly given as a table. We compare our approach with the following methods:• Random: the agent chooses an action uniformly at random for each time step; • PSRL (Strens, 2000; Osband et al., 2013): this is a direct generalization of Thompson sam-pling to MDPs, where at the beginning of each episode, we sample an MDP from the posterior distribution, and take actions according to the optimal policy for the entire episode. Similarly, we include an optimistic variant (OPSRL), which has also been explored in Osband & Van Roy (2016). • BEB (Kolter & Ng, 2009): this is a model-based optimistic algorithm that adds an explo-ration bonus to (thus far) infrequently visited states and actions.Setup Random PSRL OPSRL UCRL2 BEB -Greedy Greedy RL2n = 10 100.1 138.1 144.1 146.6 150.2 132.8 134.8 156.2 n = 25 250.2 408.8 425.2 424.1 427.8 377.3 368.8 445.7 n = 50 499.7 904.4 930.7 918.9 917.8 823.3 769.3 936.1 n = 75 749.9 1417.1 1449.2 1427.6 1422.6 1293.9 1172.9 1428.8 n = 100 999.4 1939.5 1973.9 1942.1 1935.1 1778.2 1578.5 1913.7The distribution over MDPs is constructed with |S| = 10, |A| = 5. The rewards follow a Gaussian distribution with unit variance, and the mean parameters are sampled independently from Normal(1, 1). The transitions are sampled from a flat Dirichlet distribution. This construction matches the commonly used prior in Bayesian RL methods. We set the horizon for each episode to be T = 10, and an episode always starts on the first state.The results are summarized in Table 2, and the learning curves are shown in Figure 3. We follow the same evaluation procedure as in the bandit case. We experiment with n ∈ {10, 25, 50, 75, 100}. For fewer episodes, our approach surprisingly outperforms existing methods by a large margin. The advantage is reversed as n increases, suggesting that the reinforcement learning problem in the outer loop becomes more challenging to solve. We think that the advantage for small n comes from the need for more aggressive exploitation: since there are 140 degrees of freedom to estimate in order to characterize the MDP, and by the 10th episode, we will not have enough samples to form a good estimate of the entire dynamics. By directly optimizing the RNN in this setting, our approach should be able to cope with this shortage of samples, and decides to exploit sooner compared to the reference algorithms.The previous two tasks both only involve very low-dimensional state spaces. To evaluate the feasibility of scaling up RL2, we further experiment with a challenging vision-based task, where theagent is asked to navigate a randomly generated maze to find a randomly placed target2. The agent receives a +1 reward when it reaches the target, −0.001 when it hits the wall, and −0.04 per time step to encourage it to reach targets faster. It can interact with the maze for multiple episodes, during which the maze structure and target position are held fixed. The optimal strategy is to explore the maze efficiently during the first episode, and after locating the target, act optimally against the current maze and target based on the collected information. An illustration of the task is given in Figure 4.Visual navigation alone is a challenging task for reinforcement learning. The agent only receives very sparse rewards during training, and does not have the primitives for efficient exploration at the beginning of training. It also needs to make efficient use of memory to decide how it should explore the space, without forgetting about where it has already explored. Previously, Oh et al. (2016) have studied similar vision-based navigation tasks in Minecraft. However, they use higher-level actions for efficient navigation. Similar high-level actions in our task would each require around 5 low-level actions combined in the right way. In contrast, our RL2 agent needs to learn these higher-level actions from scratch.We use a simple training setup, where we use small mazes of size 5× 5, with 2 episodes of interaction, each with horizon up to 250. Here the size of the maze is measured by the number of grid cells along each wall in a discrete representation of the maze. During each trial, we sample 1 out of 1000 randomly generated configurations of map layout and target positions. During testing, we evaluate on 1000 separately generated configurations. In addition, we also study its extrapolation behavior along two axes, by (1) testing on large mazes of size 9× 9 (see Figure 4c) and (2) running the agent for up to 5 episodes in both small and large mazes. For the large maze, we also increase the horizon per episode by 4x due to the increased size of the maze.The results are summarized in Table 3, and the learning curves are shown in Figure 5. We observe that there is a significant reduction in trajectory lengths between the first two episodes in both the smaller and larger mazes, suggesting that the agent has learned how to use information from past episodes. It also achieves reasonable extrapolation behavior in further episodes by maintaining its performance, although there is a small drop in the rate of success in the larger mazes. We also observe that on larger mazes, the ratio of improved trajectories is lower, likely because the agent has not learned how to act optimally in the larger mazes.Still, even on the small mazes, the agent does not learn to perfectly reuse prior information. An illustration of the agent’s behavior is shown in Figure 6. The intended behavior, which occurs most frequently, as shown in 6a and 6b, is that the agent should remember the target’s location, and utilize it to act optimally in the second episode. However, occasionally the agent forgets about where the target was, and continues to explore in the second episode, as shown in 6c and 6d. We believe that better reinforcement learning techniques used as the outer-loop algorithm will improve these results in the future.The concept of using prior experience to speed up reinforcement learning algorithms has been explored in the past in various forms. Earlier studies have investigated automatic tuning of hyperparameters, such as learning rate and temperature (Ishii et al., 2002; Schweighofer & Doya, 2003), as a form of meta-learning. Wilson et al. (2007) use hierarchical Bayesian methods to maintain a posterior over possible models of dynamics, and apply optimistic Thompson sampling according to the posterior. Many works in hierarchical reinforcement learning propose to extract reusable skills from previous tasks to speed up exploration in new tasks (Singh, 1992; Perkins et al., 1999). Werefer the reader to Taylor & Stone (2009) for a more thorough survey on the multi-task and transfer learning aspects.The formulation of searching for a best-performing algorithm, whose performance is averaged over a given distribution over MDPs, have been investigated in the past in more limited forms (Maes et al., 2011; Castronovo et al., 2012). There, they propose to learn an algorithm to solve multiarmed bandits using program search, where the search space consists of simple formulas composed from hand-specified primitives, which needs to be tuned for each specific distribution over MDPs. In comparison, our approach allows for entirely end-to-end training without requiring such domain knowledge.More recently, Fu et al. (2015) propose a model-based approach on top of iLQG with unknown dynamics (Levine & Abbeel, 2014), which uses samples collected from previous tasks to build a neural network prior for the dynamics, and can perform one-shot learning on new, but related tasks thanks to reduced sample complexity. There has been a growing interest in using deep neural networks for multi-task learning and transfer learning (Parisotto et al., 2015; Rusu et al., 2015; 2016a; Devin et al., 2016; Rusu et al., 2016b).In the broader context of machine learning, there has been a lot of interest in one-shot learning for object classification (Vilalta & Drissi, 2002; Fei-Fei et al., 2006; Larochelle et al., 2008; Lake et al., 2011; Koch, 2015). Our work draws inspiration from a particular line of work (Younger et al., 2001; Santoro et al., 2016; Vinyals et al., 2016), which formulates meta-learning as an optimization problem, and can thus be optimized end-to-end via gradient descent. While these work applies to the supervised learning setting, our work applies in the more general reinforcement learning setting. Although the reinforcement learning setting is more challenging, the resulting behavior is far richer: our agent must not only learn to exploit existing information, but also learn to explore, a problem that is usually not a factor in supervised learning. Another line of work (Hochreiter et al., 2001; Younger et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2016) studies meta-learning over the optimization process. There, the meta-learner makes explicit updates to a parametrized model. In comparison, we do not use a directly parametrized policy; instead, the recurrent neural network agent acts as the meta-learner and the resulting policy simultaneously.Our formulation essentially constructs a partially observable MDP (POMDP) which is solved in the outer loop, where the underlying MDP is unobserved by the agent. This reduction of an unknown MDP to a POMDP can be traced back to dual control theory (Feldbaum, 1960), where “dual” refers to the fact that one is controlling both the state and the state estimate. Feldbaum pointed out that the solution can in principle be computed with dynamic programming, but doing so is usually impractical. POMDPs with such structure have also been studied under the name “mixed observability MDPs” (Ong et al., 2010). However, the method proposed there suffers from the usual challenges of solving POMDPs in high dimensions.Apart from the various multiple-episode tasks we investigate in this work, previous literature on training RNN policies have used similar tasks that require memory to test if long-term dependency can be learned. Recent examples include the Labyrinth experiment in the A3C paper (Mnih et al., 2016), and the water maze experiment in the Recurrent DDPG paper (Heess et al., 2015a). Although these tasks can be reformulated under the RL2 framework, the key difference is that they focus on the memory aspect instead of the fast RL aspect.This paper suggests a different approach for designing better reinforcement learning algorithms: instead of acting as the designers ourselves, learn the algorithm end-to-end using standard reinforcement learning techniques. That is, the “fast” RL algorithm is a computation whose state is stored in the RNN activations, and the RNN’s weights are learned by a general-purpose “slow” reinforcement learning algorithm. Our method, RL2, has demonstrated competence comparable with theoretically optimal algorithms in small-scale settings. We have further shown its potential to scale to high-dimensional tasks.In the experiments, we have identified opportunities to improve upon RL2: the outer-loop reinforcement learning algorithm was shown to be an immediate bottleneck, and we believe that for settings with extremely long horizons, better architecture may also be required for the policy. Although wehave used generic methods and architectures for the outer-loop algorithm and the policy, doing this also ignores the underlying episodic structure. We expect algorithms and policy architectures that exploit the problem structure to significantly boost the performance.We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers.Common to all experiments: as mentioned in Section 2.2, we use placeholder values when necessary. For example, at t = 0 there is no previous action, reward, or termination flag. Since all of our experiments use discrete actions, we use the embedding of the action 0 as a placeholder for actions, and 0 for both the rewards and termination flags. To form the input to the GRU, we use the values for the rewards and termination flags as-is, and embed the states and actions as described separately below for each experiments. These values are then concatenated together to form the joint embedding.For the neural network architecture, We use rectified linear units throughout the experiments as the hidden activation, and we apply weight normalization without data-dependent initialization (Salimans & Kingma, 2016) to all weight matrices. The hidden-to-hidden weight matrix uses an orthogonal initialization (Saxe et al., 2013), and all other weight matrices use Xavier initialization (Glorot & Bengio, 2010). We initialize all bias vectors to 0. Unless otherwise mentioned, the policy and the baseline uses separate neural networks with the same architecture until the final layer, where the number of outputs differ.All experiments are implemented using TensorFlow (Abadi et al., 2016) and rllab (Duan et al., 2016). We use the implementations of classic algorithms provided by the TabulaRL package (Osband, 2016).A.1 MULTI-ARMED BANDITSThe parameters for TRPO are shown in Table 1. Since the environment is stateless, we use a constant embedding 0 as a placeholder in place of the states, and a one-hot embedding for the actions.A.2 TABULAR MDPSThe parameters for TRPO are shown in Table 2. We use a one-hot embedding for the states and actions separately, which are then concatenated together.A.3 VISUAL NAVIGATIONThe parameters for TRPO are shown in Table 3. For this task, we use a neural network to form the joint embedding. We rescale the images to have width 40 and height 30 with RGB channels preserved, and we recenter the RGB values to lie within range [−1, 1]. Then, this preprocessedimage is passed through 2 convolution layers, each with 16 filters of size 5 × 5 and stride 2. The action is first embedded into a 256-dimensional vector where the embedding is learned, and then concatenated with the flattened output of the final convolution layer. The joint vector is then fed to a fully connected layer with 256 hidden units.Unlike previous experiments, we let the policy and the baseline share the same neural network. We found this to improve the stability of training baselines and also the end performance of the policy, possibly due to regularization effects and better learned features imposed by weight sharing. Similar weight-sharing techniques have also been explored in Mnih et al. (2016).B.1 MULTI-ARMED BANDITSThere are 3 algorithms with hyperparameters: UCB1, Optimistic Thompson Sampling (OTS), and -Greedy. We perform a coarse grid search to find the best hyperparameter for each of them. More specifically:• UCB1: We test c ∈ {0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The best found parameter for each setting is given in Table 4.B.2 TABULAR MDPSThere are 4 algorithms with hyperparameters: Optimistic PSRL (OPSRL), BEB, -Greedy, UCRL2. Details are given below.• Optimistic PSRL (OPSRL): The hyperparameter is the number of posterior samples. We use up to 20 samples. The best found parameter for each setting is given in Table 7.In this section, we provide further analysis of the behavior of RL2 agent in comparison with the baseline algorithms, on the multi-armed bandit task. Certain algorithms such as UCB1 are designed not in the Bayesian context; instead they are tailored to be robust in adversarial cases. To highlight this aspect, we evaluate the algorithms on a different metric, namely the percentage of trials where the best arm is recovered. We treat the best arm chosen by the policy to be the arm that has been pulled most often, and the ground truth best arm is the arm with the highest mean parameter. In addition, we split the set of all possible bandit tasks into simpler and harder tasks, where the difficulty is measured by the -gap between the mean parameter of the best arm and the second best arm. We compare the percentage of recovering the best arm separately according to the gap, as shown in Table 11.Note that there are two columns associated with the UCB1 algorithm, where UCB1 (without “∗”) is evaluated with c = 0.2, the parameter that gives the best performance as evaluated by the average total reward, and UCB1∗ uses c = 1.0. Surprisingly, although using c = 1.0 performs the best in terms of recovering the best arm, its performance is significantly worse than using c = 0.2 when evaluated under the average total reward (369.2 ± 2.2 vs. 405.8 ± 2.2). This also explains that although RL2 does not perform the best according to this metric (which is totally expected, since it is not optimized under this metric), it achieves comparable average total reward as other bestperforming methods.
SGD with stochastic momentum has been a de facto algorithm in nonconvex optimization and deep learning. It has been widely adopted for training machine learning models in various applications. Modern techniques in computer vision (e.g.Krizhevsky et al. (2012); He et al. (2016); Cubuk et al. (2018); Gastaldi (2017)), speech recognition (e.g. Amodei et al. (2016)), natural language processing (e.g. Vaswani et al. (2017)), and reinforcement learning (e.g. Silver et al. (2017)) use SGD with stochastic momentum to train models. The advantage of SGD with stochastic momentum has been widely observed (Hoffer et al. (2017); Loshchilov & Hutter (2019); Wilson et al. (2017)). Sutskever et al. (2013) demonstrate that training deep neural nets by SGD with stochastic momentum helps achieving in faster convergence compared with the standard SGD (i.e. without momentum). The success of momentum makes it a necessary tool for designing new optimization algorithms in optimization and deep learning. For example, all the popular variants of adaptive stochastic gradient methods like Adam (Kingma & Ba (2015)) or AMSGrad (Reddi et al. (2018b)) include the use of momentum.Despite the wide use of stochastic momentum (Algorithm 1) in practice, 1 justification for the clear empirical improvements has remained elusive, as has any mathematical guidelines for actually setting the momentum parameter—it has been observed that large values (e.g. β = 0.9) work well in practice. It should be noted that Algorithm 1 is the default momentum-method in popular software1Heavy ball momentum is the default choice of momentum method in PyTorch and Tensorflow, instead of Nesterov’s momentum. See the manual pages https://pytorch.org/docs/stable/_modules/ torch/optim/sgd.html and https://www.tensorflow.org/api_docs/python/tf/ keras/optimizers/SGD.Algorithm 1: SGD with stochastic heavy ball momentum 1: Required: Step size parameter η and momentum parameter β. 2: Init: w0 ∈ Rd and m−1 = 0 ∈ Rd. 3: for t = 0 to T do 4: Given current iterate wt, obtain stochastic gradient gt := ∇f(wt; ξt). 5: Update stochastic momentum mt := βmt−1 + gt. 6: Update iterate wt+1 := wt − ηmt. 7: end forpackages such as PyTorch and Tensorflow. In this paper we provide a theoretical analysis for SGD with momentum. We identify some mild conditions that guarantees SGD with stochastic momentum will provably escape saddle points faster than the standard SGD, which provides clear evidence for the benefit of using stochastic momentum. For stochastic heavy ball momentum, a weighted average of stochastic gradients at the visited points is maintained. The new update is computed as the current update minus a step in the direction of the momentum. Our analysis shows that these updates can amplify a component in an escape direction of the saddle points.In this paper, we focus on finding a second-order stationary point for smooth non-convex optimization by SGD with stochastic heavy ball momentum. Specifically, we consider the stochastic nonconvex optimization problem, minw∈Rd f(w) := Eξ∼D[f(w; ξ)], where we overload the notation so that f(w; ξ) represents a stochastic function induced by the randomness ξ while f(w) is the expectation of the stochastic functions. An ( , )-second-order stationary point w satisfies‖∇f(w)‖ ≤ and ∇2f(w) − I. (1)Obtaining a second order guarantee has emerged as a desired goal in the nonconvex optimization community. Since finding a global minimum or even a local minimum in general nonconvex optimization can be NP hard (Anandkumar & Ge (2016); Nie (2015); Murty & Kabadi (1987); Nesterov (2000)), most of the papers in nonconvex optimization target at reaching an approximate second-order stationary point with additional assumptions like Lipschitzness in the gradients and the Hessian (e.g. Allen-Zhu & Li (2018); Carmon & Duchi (2018); Curtis et al. (2017); Daneshmand et al. (2018); Du et al. (2017); Fang et al. (2018; 2019); Ge et al. (2015); Jin et al. (2017; 2019); Kohler & Lucchi (2017); Lei et al. (2017); Lee et al. (2019); Levy (2016); Mokhtari et al. (2018); Nesterov & Polyak (2006); Reddi et al. (2018a); Staib et al. (2019); Tripuraneni et al. (2018); Xu et al. (2018)). 2 We follow these related works for the goal and aim at showing the benefit of the use of the momentum in reaching an ( , )-second-order stationary point.We introduce a required condition, akin to a model assumption made in (Daneshmand et al. (2018)), that ensures the dynamic procedure in Algorithm 2 produces updates with suitable correlation with the negative curvature directions of the function f .Definition 1. Assume, at some time t, that the Hessian Ht = ∇2f(wt) has some eigenvalue smaller than − and ‖∇f(wt)‖ ≤ . Let vt be the eigenvector corresponding to the smallest eigenvalue of ∇2f(wt). The stochastic momentum mt satisfies Correlated Negative Curvature (CNC) at t with parameter γ > 0 ifEt[〈mt, vt〉2] ≥ γ. (2)As we will show, the recursive dynamics of SGD with heavy ball momentum helps in amplifying the escape signal γ, which allows it to escape saddle points faster.Contribution: We show that, under CNC assumption and some minor constraints that upper-bound parameter β, if SGD with momentum has properties called Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE), defined in the later section, then it takes T = O((1−β) log(1/(1−β) ) −10) iterations to return an ( , ) second order stationary point. Alternatively, one can obtain an ( , √ ) second order stationary point in T = O((1 − β) log(1/(1 − β) ) −5) iterations. Our theoretical result demonstrates that a larger momentum parameter β can help in escaping saddle points faster. As saddle points are pervasive in the loss landscape of optimization and deep learning (Dauphin et al.2We apologize that the list is far from exhaustive.(2014); Choromanska et al. (2015)), the result sheds light on explaining why SGD with momentum enables training faster in optimization and deep learning.Notation: In this paper we use Et[·] to represent conditional expectation E[·|w1, w2, . . . , wt], which is about fixing the randomness upto but not including t and notice that wt was determined at t− 1.2.1 A THOUGHT EXPERIMENT.Let us provide some high-level intuition about the benefit of stochastic momentum with respect to escaping saddle points. In an iterative update scheme, at some time t0 the parameters wt0 can enter a saddle point region, that is a place where Hessian∇2f(wt0) has a non-trivial negative eigenvalue, say λmin(∇2f(wt0)) ≤ − , and the gradient ∇f(wt0) is small in norm, say ‖∇f(wt0)‖ ≤ . The challenge here is that gradient updates may drift only very slowly away from the saddle point, and may not escape this region; see (Du et al. (2017); Lee et al. (2019)) for additional details. On the other hand, if the iterates were to move in one particular direction, namely along vt0 the direction of the smallest eigenvector of ∇2f(wt0), then a fast escape is guaranteed under certain constraints on thestep size η; see e.g. (Carmon et al. (2018)). While the negative eigenvector could be computed directly, this 2nd-order method is prohibitively expensive and hence we typically aim to rely on gradient methods. With this in mind, Daneshmand et al. (2018), who study non-momentum SGD, make an assumption akin to our CNC property described above that each stochastic gradient gt0 is strongly non-orthogonal to vt0 the direction of large negative curvature. This suffices to drive the updates out of the saddle point region.In the present paper we study stochastic momentum, and our CNC property requires that the update direction mt0 is strongly non-orthogonal to vt0 ; more precisely, Et0 [〈mt0 , vt0〉2] ≥ γ > 0. We are able to take advantage of the analysis of (Daneshmand et al. (2018)) to establish that updates begin to escape a saddle point region for similar reasons. Further, this effect is amplified in successive iterations through the momentum update when β is close to 1. Assume that at some wt0 we have mt0 which possesses significant correlation with the negative curvature direction vt0 , then on successive rounds mt0+1 is quite close to βmt0 , mt0+2 is quite close to β2mt0 , and so forth; see Figure 1 for an example. This provides an intuitive perspective on how momentum might help accelerate the escape process. Yet one might ask does this procedure provably contribute to the escape process and, if so, what is the aggregate performance improvement of the momentum? We answer the first question in the affirmative, and we answer the second question essentially by showing that momentum can help speed up saddle-point escape by a multiplicative factor of 1− β. On the negative side, we also show that β is constrained and may not be chosen arbitrarily close to 1.Let us now establish, empirically, the clear benefit of stochastic momentum on the problem of saddle-point escape. We construct two stochastic optimization tasks, and each exhibits at least one significant saddle point. The two objectives are as follows.min w f(w) :=1n n∑ i=1 (1 2 w>Hw + b>i w + ‖w‖1010 ) , (3)min w f(w) :=1n n∑ i=1 ( (a>i w) 2 − y )2 . (4)Problem (3) of these was considered by (Staib et al. (2019); Reddi et al. (2018a)) and represents a very straightforward non-convex optimization challenge, with an embedded saddle given by the matrixH := diag([1,−0.1]), and stochastic gaussian perturbations given by bi ∼ N (0, diag([0.1, 0.001])); the small variance in the second component provides lower noise in the escape direction. Here we have set n = 10. Observe that the origin is in the neighborhood of saddle points and has objective value zero. SGD and SGD with momentum are initialized at the origin in the experiment so that they have to escape saddle points before the convergence. The second objective (4) appears in the phase retrieval problem, that has real applications in physical sciences (Candés et al. (2013); Shechtman et al. (2015)). In phase retrieval3, one wants to find an unknown w∗ ∈ Rd with access to but a few samples yi = (a>i w∗)2; the design vector ai is known a priori. Here we have sampled w∗ ∼ N (0, Id/d) and ai ∼ N (0, Id) with d = 10 and n = 200. The empirical findings, displayed in Figure 2, are quite stark: for both objectives, convergence is significantly accelerated by larger choices of β. In the first objective (Figure 4a), we see each optimization trajectory entering a saddle point region, apparent from the “flat” progress, yet we observe that large-momentum trajectories escape the saddle much more quickly than those with smaller momentum. A similar affect appears in Figure 4b. To the best of our knowledge, this is the first reported empirical finding that establishes the dramatic speed up of stochastic momentum for finding an optimal solution in phase retrieval.Heavy ball method: The heavy ball method was originally proposed by Polyak (1964). It has been observed that this algorithm, even in the deterministic setting, provides no convergence speedup over standard gradient descent, except in some highly structure cases such as convex quadratic objectives where an “accelerated” rate is possible (Lessard et al. (2016); Goh (2017); Ghadimi et al. (2015); Sun et al. (2019); Loizou & Richtárik (2017; 2018); Gadat et al. (2016); Yang et al. (2018); Kidambi et al. (2018); Can et al. (2019)). We provide a comprehensive survey of the related works about heavy ball method in Appendix A.Reaching a second order stationary point: As we mentioned earlier, there are many works aim at reaching a second order stationary point. We classify them into two categories: specialized algorithms and simple GD/SGD variants. Specialized algorithms are those designed to exploit the negative curvature explicitly and escape saddle points faster than the ones without the explicit exploitation (e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al. (2018)). Simple GD/SGD variants are those with minimal tweaks of standard GD/SGD or their variants (e.g. Ge et al. (2015); Levy (2016); Fang et al. (2019); Jin et al. (2017; 2018; 2019); Daneshmand et al. (2018);3It is known that phase retrieval is nonconvex and has the so-called strict saddle property: (1) every local minimizer {w∗,−w∗} is global up to phase, (2) each saddle exhibits negative curvature (see e.g. (Sun et al. (2015; 2016); Chen et al. (2018)))Staib et al. (2019)). Our work belongs to this category. In this category, perhaps the pioneer works are (Ge et al. (2015)) and (Jin et al. (2017)). Jin et al. (2017) show that explicitly adding isotropic noise in each iteration guarantees that GD escapes saddle points and finds a second order stationary point with high probability. Following (Jin et al. (2017)), Daneshmand et al. (2018) assume that stochastic gradient inherently has a component to escape. Specifically, they make assumption of the Correlated Negative Curvature (CNC) for stochastic gradient gt so that Et[〈gt, vt〉2] ≥ γ > 0. The assumption allows the algorithm to avoid the procedure of perturbing the updates by adding isotropic noise. Our work is motivated by (Daneshmand et al. (2018)) but assumes CNC for the stochastic momentum mt instead. In Appendix A, we compare the results of our work with the related works.We assume that the gradient∇f is L-Lipschitz; that is, f is L-smooth. Further, we assume that the Hessian∇2f is ρ-Lipschitz. These two properties ensure that ‖∇f(w)−∇f(w′)‖ ≤ L‖w−w′‖ and that ‖∇2f(w)−∇2f(w′)‖ ≤ ρ‖w−w′‖, ∀w,w′. The L-Lipschitz gradient assumption implies that |f(w′)−f(w)−〈∇f(w), w′−w〉| ≤ L2 ‖w−w′‖2,∀w,w′, while the ρ-Lipschitz Hessian assumption implies that |f(w′)−f(w)−〈∇f(w), w′−w〉−(w′−w)>∇2f(w)(w′−w)| ≤ ρ6‖w−w′‖3, ∀w,w′. Furthermore, we assume that the stochastic gradient has bounded noise ‖∇f(w)−∇f(w; ξ)‖2 ≤ σ2 and that the norm of stochastic momentum is bounded so that ‖mt‖ ≤ cm. We denote ΠiMi as the matrix product of matrices {Mi} and we use σmax(M) = ‖M‖2 := supx6=0 〈x,Mx〉 〈x,x〉 to denote the spectral norm of the matrix M .Our analysis of stochastic momentum relies on three properties of the stochastic momentum dynamic. These properties are somewhat unusual, but we argue they should hold in natural settings, and later we aim to demonstrate that they hold empirically in a couple of standard problems of interest. Definition 2. We say that SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) 4 if we haveEt[〈∇f(wt),mt − gt〉] ≥ − 12 ‖∇f(wt)‖2. (5)We say that SGD with stochastic momentum satisfies Almost Positively Correlated with Gradient (APCG) with parameter τ if ∃c′ > 0 such that,Et[〈∇f(wt),Mtmt〉] ≥ −c′ησmax(Mt)‖∇f(wt)‖2, (6)where the PSD matrix Mt is defined asMt = (Π τ−1 s=1Gs,t)(Π τ−1 s=kGs,t) with Gs,t := I−η ∑s j=1 β s−j∇2f(wt) = I−η(1−β s) 1−β ∇ 2f(wt)for any integer 1 ≤ k ≤ τ − 1, and η is any step size chosen that guarantees each Gs,t is PSD. Definition 3. We say that the SGD with momentum exhibits Gradient Alignment or Curvature Exploitation (GrACE) if ∃ch ≥ 0 such thatEt[η〈∇f(wt), gt −mt〉+ η 2 2 m > t ∇2f(wt)mt] ≤ η2ch. (7)APAG requires that the momentum term mt must, in expectation, not be significantly misaligned with the gradient ∇f(wt). This is a very natural condition when one sees that the momentum term is acting as a biased estimate of the gradient of the deterministic f . APAG demands that the bias can not be too large relative to the size of ∇f(wt). Indeed this property is only needed in our analysis when the gradient is large (i.e. ‖∇f(wt)‖ ≥ ) as it guarantees that the algorithm makes progress; our analysis does not require APAG holds when gradient is small.APCG is a related property, but requires that the current momentum term mt is almost positively correlated with the the gradient∇f(wt), but measured in the Mahalanobis norm induced by Mt. It4Note that our analysis still go through if one replaces 1 2 on r.h.s. of (5) with any larger number c < 1; the resulted iteration complexity would be only a constant multiple worse.s=1 Gs,t)(Π3×105s=1 Gs,t) and we onlyreport the values when the update is in the region of saddle points. For (f), we let Mt = (Π500s=1Gs,t)(Π500s=1Gs,t) and we observe that the value is almost always nonnegative. The figures implies that SGD with momentum has APAG and APCG properties in the experiments. Furthermore, an interesting observation is that, for the phase retrieval problem, the expected values might actually be nonnegative.may appear to be an unusual object, but one can view the PSD matrix Mt as measuring something about the local curvature of the function with respect to the trajectory of the SGD with momentum dynamic. We will show that this property holds empirically on two natural problems for a reasonable constant c′. APCG is only needed in our analysis when the update is in a saddle region with significant negative curvature, ‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − . Our analysis does not require APCG holds when the gradient is large or the update is at an ( , )-second order stationary point.For GrACE, the first term on l.h.s of (7) measures the alignment between stochastic momentum mt and the gradient ∇f(wt), while the second term on l.h.s measures the curvature exploitation. The first term is small (or even negative) when the stochastic momentum mt is aligned with the gradient∇f(wt), while the second term is small (or even negative) when the stochastic momentum mt can exploit a negative curvature (i.e. the subspace of eigenvectors that corresponds to the negative eigenvalues of the Hessian∇2f(wt) if exists). Overall, a small sum of the two terms (and, consequently, a small ch) allows one to bound the function value of the next iterate (see Lemma 8).On Figure 3, we report some quantities related to APAG and APCG as well as the gradient norm when solving the previously discussed problems (3) and (4) using SGD with momentum. We also report a quantity regarding GrACE on Figure 4 in the appendix.The high level idea of our analysis follows as a similar template to (Jin et al. (2017); Daneshmand et al. (2018); Staib et al. (2019)). Our proof is structured into three cases: either (a) ‖∇f(w)‖ ≥ , or (b) ‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − , or otherwise (c) ‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≥ − ,Algorithm 2: SGD with stochastic heavy ball momentum 1: Required: Step size parameters r and η, momentum parameter β, and period parameter Tthred. 2: Init: w0 ∈ Rd and m−1 = 0 ∈ Rd. 3: for t = 0 to T do 4: Get stochastic gradient gt at wt, and set stochastic momentum mt := βmt−1 + gt. 5: Set learning rate: η̂ := η unless (t mod Tthred) = 0 in which case η̂ := r 6: wt+1 = wt − η̂mt. 7: end formeaning we have arrived in a second-order stationary region. The precise algorithm we analyze is Algorithm 2, which identical to Algorithm 1 except that we boost the step size to a larger value r on occasion. We will show that the algorithm makes progress in cases (a) and (b). In case (c), when the goal has already been met, further execution of the algorithm only weakly hurts progress. Ultimately, we prove that a second order stationary point is arrived at with high probability. While our proof borrows tools from (Daneshmand et al. (2018); Staib et al. (2019)), much of the momentum analysis is entirely novel to our knowledge. Theorem 1. Assume that the stochastic momentum satisfies CNC. Set 5 r = O( 2), η = O( 5), and Tthred = c(1−β)η log( Lcmσ 2ρc′ch (1−β)δγ ) = O((1 − β) log( Lcmσ 2ρc′ch (1−β)δγ )−6) for some constant c > 0. If SGD with momentum (Algorithm 2) has APAG property when gradient is large (‖∇f(w)‖ ≥ ), APCGTthred property when it enters a region of saddle points that exhibits a negative curvature (‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − ), and GrACE property throughout the iterations, then it reaches an ( , ) second order stationary point in T = 2Tthred(f(w0)−minw f(w))/(δFthred) = O((1− β) log(Lcmσ2ρc′ch (1−β)δγ ) −10) iterations with high probability 1− δ, where Fthred = O( 4).The theorem implies the advantage of using stochastic momentum for SGD. Higher β leads to reaching a second order stationary point faster. As we will show in the following, this is due to that higher β enables escaping the saddle points faster. In Subsection 3.2.1, we provide some key details of the proof of Theorem 1. The interested reader can read a high-level sketch of the proof, as well as the detailed version, in Appendix G.Remark 1: (constraints on β) We also need some minor constraints on β so that β cannot be too close to 1. They are 1) L(1 − β)3 > 1, 2) σ2(1 − β)3 > 1, 3) c′(1 − β)2 > 1, 4) η ≤ 1−βL , 5) η ≤ 1−β , and 6) Tthred ≥ 1 + 2β 1−β . Please see Appendix E.1 for the details and discussions.Remark 2: (escaping saddle points) Note that Algorithm 2 reduces to CNC-SGD of Daneshmand et al. (2018) when β = 0 (i.e. without momentum). Therefore, let us compare the results. We show that the escape time of Algorithm 2 is Tthred := Õ ( (1−β) η ) (see Appendix E.3.3, especially (81-82)).On the other hand, for CNC-SGD, based on Table 3 in their paper, is Tthred = Õ ( 1 η ) . One can clearly see that Tthred of our result has a dependency 1 − β, which makes it smaller than that of Daneshmand et al. (2018) for any same η and consequently demonstrates escaping saddle point faster with momentum.Remark 3: (finding a second order stationary point) Denote ` a number such that ∀t, ‖gt‖ ≤ `. In Appendix G.3, we show that in the high momentum regime where (1− β) << ρ 2`10c9mc 2 hc ′ , Algorithm 2is strictly better than CNC-SGD of Daneshmand et al. (2018), which means that a higher momentum can help find a second order stationary point faster. Empirically, we find out that c′ ≈ 0 (Figure 3) and ch ≈ 0 (Figure 4) in the phase retrieval problem, so the condition is easily satisfied for a wide range of β.In this subsection, we analyze the process of escaping saddle points by SGD with momentum. Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that it enters the region exhibiting a small5See Table 3 in Appendix E for the precise expressions of the parameters. Here, we hide the parameters’ dependencies on γ, L, cm, c′, σ2, ρ, ch, and δ. W.l.o.g, we also assume that cm, L, σ2, c′, ch, and ρ are not less than one and ≤ 1.gradient but a large negative eigenvalue of the Hessian (i.e. ‖∇f(wt0)‖ ≤ and λmin(∇2f(wt0)) ≤ − ). We want to show that it takes at most Tthred iterations to escape the region and whenever it escapes, the function value decreases at least by Fthred = O( 4) on expectation, where the precise expression of Fthred will be determined later in Appendix E. The technique that we use is proving by contradiction. Assume that the function value on expectation does not decrease at least Fthred in Tthred iterations. Then, we get an upper bound of the expected distance Et0 [‖wt0+Tthred − wt0‖2] ≤ Cupper. Yet, by leveraging the negative curvature, we also show a lower bound of the form Et0 [‖wt0+Tthred − wt0‖2] ≥ Clower. The analysis will show that the lower bound is larger than the upper bound (namely, Clower > Cupper), which leads to the contradiction and concludes that the function value must decrease at least Fthred in Tthred iterations on expectation. Since Tthred = O((1− β) log( 1(1−β) )6), the dependency on β suggests that larger β can leads to smaller Tthred, which implies that larger momentum helps in escaping saddle points faster. Lemma 1 below provides an upper bound of the expected distance. The proof is in Appendix C.Lemma 1. Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that Et0 [f(wt0) − f(wt0+t)] ≤ Fthred for any 0 ≤ t ≤ Tthred. Then, Et0 [‖wt0+t − wt0‖2] ≤ Cupper,t := 8ηt ( Fthred+2r2ch+ ρ3 r 3c3m ) (1−β)2 + 8η 2 tσ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m.We see thatCupper,t in Lemma 1 is monotone increasing with t, so we can defineCupper := Cupper,Tthred . Now let us switch to obtaining the lower bound of Et0 [‖wt0+Tthred − wt0‖2]. The key to get the lower bound comes from the recursive dynamics of SGD with momentum.Lemma 2. Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic approximation at wt0 , Q(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉 + 12 (w − wt0)>H(w − wt0), where H := ∇2f(wt0). Also, define Gs := (I − η ∑s k=1 βs−kH). Then we can write wt0+t −wt0 exactly using the following decomposition.qv,t−1︷ ︸︸ ︷( Πt−1j=1Gj )( − rmt0 ) +η qm,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) βsmt0+ η qq,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) )+ η qw,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k∇f(wt0) +ηqξ,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−kξt0+k .The proof of Lemma 2 is in Appendix D. Furthermore, we will use the quantities qv,t−1, qm,t−1, qq,t−1, qw,t−1, qξ,t−1 as defined above throughout the analysis.Lemma 3. Following the notations of Lemma 2, we have thatEt0 [‖wt0+t−wt0‖2] ≥ Et0 [‖qv,t−1‖2]+2ηEt0 [〈qv,t−1, qm,t−1+qq,t−1+qw,t−1+qξ,t−1〉] =: Clower.We are going to show that the dominant term in the lower bound of Et0 [‖wt0+t − wt0‖2] is Et0 [‖qv,t−1‖2], which is the critical component for ensuring that the lower bound is larger than the upper bound of the expected distance.Lemma 4. Denote θj := ∑j k=1 β j−k = ∑j k=1 βk−1 and λ := −λmin(H). Following the conditions and notations in Lemma 1 and Lemma 2, we have thatEt0 [‖qv,t−1‖2] ≥ ( Πt−1j=1(1 + ηθjλ) )2 r2γ. (8)Proof. We know that λmin(H) ≤ − < 0. Let v be the eigenvector of the Hessian H with unit norm that corresponds to λmin(H) so that Hv = λmin(H)v. We have (I − ηH)v = v − ηλmin(H)v =(1− ηλmin(H))v. Then,Et0 [‖qv,t−1‖2] (a) = Et0 [‖qv,t−1‖2‖v‖2](b) ≥ Et0 [〈qv,t−1, v〉2] (c) = Et0 [〈 ( Πt−1j=1Gj ) rmt0 , v〉2](d) = Et0 [〈 ( Πt−1j=1(I − ηθjH) ) rmt0 , v〉2] = Et0〈 ( Πt−1j=1(1− ηθjλmin(H)) ) rmt0 , v〉2](e) ≥ ( Πt−1j=1(1 + ηθjλ) )2 r2γ,(9)where (a) is because v is with unit norm, (b) is by Cauchy–Schwarz inequality, (c), (d) are by the definitions, and (e) is by the CNC assumption so that Et0 [〈mt0 , v〉2] ≥ γ.Observe that the lower bound in (8) is monotone increasing with t and the momentum parameter β. Moreover, it actually grows exponentially in t. To get the contradiction, we have to show that the lower bound is larger than the upper bound. By Lemma 1 and Lemma 3, it suffices to prove the following lemma. We provide its proof in Appendix E.Lemma 5. Let Fthred = O( 4) and η2Tthred ≤ r2. By following the conditions and notations in Theorem 1, Lemma 1 and Lemma 2, we conclude that if SGD with momentum (Algorithm 2) has the APCG property, then we have that Clower := Et0 [‖qv,Tthred−1‖2]+2ηEt0 [〈qv,Tthred−1, qm,Tthred−1 + qq,Tthred−1 + qw,Tthred−1 + qξ,Tthred−1〉] > Cupper.In this paper, we identify three properties that guarantee SGD with momentum in reaching a secondorder stationary point faster by a higher momentum, which justifies the practice of using a large value of momentum parameter β. We show that a greater momentum leads to escaping strict saddle points faster due to that SGD with momentum recursively enlarges the projection to an escape direction. However, how to make sure that SGD with momentum has the three properties is not very clear. It would be interesting to identify conditions that guarantee SGD with momentum to have the properties. Perhaps a good starting point is understanding why the properties hold in phase retrieval. We believe that our results shed light on understanding the recent success of SGD with momentum in non-convex optimization and deep learning.We gratefully acknowledge financial support from NSF IIS awards 1910077 and 1453304.Heavy ball method: The heavy ball method was originally proposed by Polyak (1964). It has been observed that this algorithm, even in the deterministic setting, provides no convergence speedup over standard gradient descent, except in some highly structure cases such as convex quadratic objectives where an “accelerated” rate is possible (Lessard et al. (2016); Goh (2017)). In recent years, some works make some efforts in analyzing heavy ball method for other classes of optimization problems besides the quadratic functions. For example, Ghadimi et al. (2015) prove an O(1/T ) ergodic convergence rate when the problem is smooth convex, while Sun et al. (2019) provide a non-ergodic convergence rate for certain classes of convex problems. Ochs et al. (2014) combine the technique of forward-backward splitting with heavy ball method for a specific class of nonconvex optimization problem. For stochastic heavy ball method, Loizou & Richtárik (2017) analyze a class of linear regression problems and shows a linear convergence rate of stochastic momentum, in which the linear regression problems actually belongs to the case of strongly convex quadratic functions. Other works includes (Gadat et al. (2016)), which shows almost sure convergence to the critical points by stochastic heavy ball for general non-convex coercive functions. Yet, the result does not show any advantage of stochastic heavy ball over other optimization algorithms like SGD. Can et al. (2019) show an accelerated linear convergence to a stationary distribution under Wasserstein distance for strongly convex quadratic functions by SGD with stochastic heavy ball momentum. Yang et al. (2018) provide a unified analysis of stochastic heavy ball momentum and Nesterov’s momentum for smooth non-convex objective functions. They show that the expected gradient norm converges at rate O(1/ √ t). Yet, the rate is not better than that of the standard SGD. We are also aware of the works (Ghadimi & Lan (2016; 2013)), which propose some variants of stochastic accelerated algorithms with first order stationary point guarantees. Yet, the framework in (Ghadimi & Lan (2016; 2013)) does not capture the stochastic heavy ball momentum used in practice. There is also a negative result about the heavy ball momentum. Kidambi et al. (2018) show that for a specific strongly convex andstrongly smooth problem, SGD with heavy ball momentum fails to achieving the best convergence rate while some algorithms can.Reaching a second order stationary point: As we mentioned earlier, there are many works aim at reaching a second order stationary point. We classify them into two categories: specialized algorithms and simple GD/SGD variants. Specialized algorithms are those designed to exploit the negative curvature explicitly and escape saddle points faster than the ones without the explicit exploitation (e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al. (2018)). Simple GD/SGD variants are those with minimal tweaks of standard GD/SGD or their variants (e.g. Ge et al. (2015); Levy (2016); Fang et al. (2019); Jin et al. (2017; 2018; 2019); Daneshmand et al. (2018); Staib et al. (2019)). Our work belongs to this category. In this category, perhaps the pioneer works are (Ge et al. (2015)) and (Jin et al. (2017)). Jin et al. (2017) show that explicitly adding isotropic noise in each iteration guarantees that GD escapes saddle points and finds a second order stationary point with high probability. Following (Jin et al. (2017)), Daneshmand et al. (2018) assume that stochastic gradient inherently has a component to escape. Specifically, they make assumption of the Correlated Negative Curvature (CNC) for stochastic gradient gt so that Et[〈gt, vt〉2] ≥ γ > 0. The assumption allows the algorithm to avoid the procedure of perturbing the updates by adding isotropic noise. Our work is motivated by (Daneshmand et al. (2018)) but assumes CNC for the stochastic momentum mt instead. Very recently, Jin et al. (2019) consider perturbing the update of SGD and provide a second order guarantee. Staib et al. (2019) consider a variant of RMSProp (Tieleman & Hinton (2012)), in which the gradient gt is multiplied by a preconditioning matrix Gt and the update is wt+1 = wt − G−1/2t gt. The work shows that the algorithm can help in escaping saddle points faster compared to the standard SGD under certain conditions. Fang et al. (2019) propose average-SGD, in which a suffix averaging scheme is conducted for the updates. They also assume an inherent property of stochastic gradients that allows SGD to escape saddle points.We summarize the iteration complexity results of the related works for simple SGD variants on Table 1. 6 The readers can see that the iteration complexity of (Fang et al. (2019)) and (Jin et al. (2019)) are better than (Daneshmand et al. (2018); Staib et al. (2019)) and our result. So, we want to explain the results and clarify the differences. First, we focus on explaining why the popular algorithm, SGD with heavy ball momentum, works well in practice, which is without the suffix averaging scheme used in (Fang et al. (2019)) and is without the explicit perturbation used in (Jin et al. (2019)). Specifically, we focus on studying the effect of stochastic heavy ball momentum and showing the advantage of using it. Furthermore, our analysis framework is built on the work of (Daneshmand et al. (2018)). We believe that, based on the insight in our work, one can also show the advantage of stochastic momentum by modifying the assumptions and algorithms in (Fang et al. (2019)) or (Jin et al. (2019)) and consequently get a better dependency on .6We follow the work (Daneshmand et al. (2018)) for reaching an ( , )-stationary point, while some works are for an ( , √ )-stationary point. We translate them into the complexity of getting an ( , )-stationary point.In the following, Lemma 7 says that under the APAG property, when the gradient norm is large, on expectation SGD with momentum decreases the function value by a constant and consequently makes progress. On the other hand, Lemma 8 upper-bounds the increase of function value of the next iterate (if happens) by leveraging the GrACE property. Lemma 6. If SGD with momentum has the APAG property, then, considering the update step wt+1 = wt − ηmt, we have that Et[f(wt+1)] ≤ f(wt)− η2‖∇f(wt)‖ 2 + Lη2c2m 2 .Proof. By the L-smoothness assumption,f(wt+1) ≤ f(wt)− η〈∇f(wt),mt〉+ Lη22 ‖mt‖2≤f(wt)− η〈∇f(wt), gt〉 − η〈∇f(wt),mt − gt〉+ Lη2c2m2 . (10)Taking the expectation on both sides. We haveEt[f(wt+1)] ≤ f(wt)− η‖∇f(wt)‖2 − ηEt[〈∇f(wt),mt − gt〉] + Lη2c2m2≤ f(wt)− η2 ‖∇f(wt)‖2 + Lη2c2m 2 . (11)where we use the APAG property in the last inequality.Lemma 7. Assume that the step size η satisfies η ≤ 28Lc2m . If SGD with momentum has theAPAG property, then, considering the update step wt+1 = wt − ηmt, we have that Et[f(wt+1)] ≤ f(wt)− η4 2 when ‖∇f(wt)‖ ≥ .Proof. Et[f(wt+1)−f(wt)] Lemma 6 ≤ −η2‖∇f(wt)‖ 2+ Lη2c2m 2 ‖∇f(wt)‖≥ ≤ −η2 2+ Lη2c2m 2 ≤ − η 42, where the last inequality is due to the constraint of η.Lemma 8. If SGD with momentum has the GrACE property, then, considering the update step wt+1 = wt − ηmt, we have that Et[f(wt+1)] ≤ f(wt) + η2ch + ρη 3 6 c 3 m.Proof. Consider the update rule wt+1 = wt − ηmt, where mt represents the stochastic momentum and η is the step size. By ρ-Lipschitzness of Hessian, we have f(wt+1) ≤ f(wt)− η〈∇f(wt), gt〉+ η〈∇f(wt), gt−mt〉+ η 2 2 m > t ∇2f(wt)mt+ ρη3 6 ‖mt‖ 3. Taking the conditional expectation, one hasEt[f(wt+1)] ≤ f(wt)− Et[η‖∇f(wt)‖2] + Et[η〈∇f(wt), gt −mt〉+ η22 m>t ∇2f(wt)mt] +ρη36 c3m.≤ f(wt) + 0 + η2ch + ρη36 c3m.(12)Lemma 1 Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that Et0 [f(wt0) − f(wt0+t)] ≤ Fthred for any 0 ≤ t ≤ Tthred. Then,Et0 [‖wt0+t − wt0‖2] ≤ Cupper,t := 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8η2tσ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(13)Proof. Recall that the update is wt0+1 = wt0 − rmt0 , and wt0+t = wt0+t−1 − ηmt0+t−1, for t > 1. We have that‖wt0+t − wt0‖2 ≤ 2(‖wt0+t − wt0+1‖2 + ‖wt0+1 − wt0‖2) ≤ 2‖wt0+t − wt0+1‖2 + 2r2c2m, (14) where the first inequality is by the triangle inequality and the second one is due to the assumption that ‖mt‖ ≤ cm for any t. Now let us denote• αs := ∑t−1−s j=0 β j• At−1 := ∑t−1 s=1 αsand let us rewrite gt = ∇f(wt) + ξt, where ξt is the zero-mean noise. We have thatEt0 [‖wt0+t − wt0+1‖2] = Et0 [‖ t−1∑ s=1 −ηmt0+s‖2] = Et0 [η2‖ t−1∑ s=1 ( ( s∑ j=1 βs−jgt0+j) + β smt0 ) ‖2]≤ Et0 [2η2‖ t−1∑ s=1 s∑ j=1 βs−jgt0+j‖2 + 2η2‖ t−1∑ s=1 βsmt0‖2] ≤ Et0 [2η2‖ t−1∑ s=1 s∑ j=1 βs−jgt0+j‖2] + 2η2 ( β 1− β )2 c2m = Et0 [2η2‖ t−1∑ s=1 αsgt0+s‖2] + 2η2 ( β 1− β )2 c2m= Et0 [2η2‖ t−1∑ s=1 αs ( ∇f(wt0+s) + ξt0+s ) ‖2] + 2η2 ( β 1− β )2 c2m≤ Et0 [4η2‖ t−1∑ s=1 αs∇f(wt0+s)‖2] + Et0 [4η2‖ t−1∑ s=1 αsξt0+s‖2] + 2η2 ( β 1− β )2 c2m.(15)To proceed, we need to upper bound Et0 [4η2‖ ∑t−1 s=1 αs∇f(wt0+s)‖2]. We have that Et0 [4η2‖ t−1∑ s=1 αs∇f(wt0+s)‖2] (a) ≤ Et0 [4η2A2t−1 t−1∑ s=1 αs At−1 ‖∇f(wt0+s)‖2](b) ≤ Et0 [4η2 At−1 1− β t−1∑ s=1 ‖∇f(wt0+s)‖2] (c) ≤ Et0 [4η2 t (1− β)2 t−1∑ s=1 ‖∇f(wt0+s)‖2].(16)where (a) is by Jensen’s inequality, (b) is by maxs αs ≤ 11−β , and (c) is by At−1 ≤ t 1−β . Now let us switch to bound the other term.Et0 [4η2‖ t−1∑ s=1 αsξt0+s‖2] = 4η2 ( Et0 [ t−1∑ i6=j αiαjξ > t0+iξt0+j ] + Et0 [ t−1∑ s=1 α2sξ > t0+sξt0+s] ) (a) = 4η2 ( 0 + Et0 [t−1∑ s=1 α2sξ > t0+sξt0+s] ) ,(b) ≤ 4η2 tσ 2(1− β)2 . (17)where (a) is because Et0 [ξ>t0+iξt0+j ] = 0 for i 6= j, (b) is by that ‖ξt‖ 2 ≤ σ2 and maxt αt ≤ 11−β . Combining (14), (15), (16), (17),Et0 [‖wt0+t − wt0‖2] ≤ Et0 [8η2 t (1− β)2 t−1∑ s=1 ‖∇f(wt0+s)‖2] + 8η2 tσ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(18) Now we need to bound Et0 [ ∑t−1 s=1 ‖∇f(wt0+s)‖2]. By using ρ-Lipschitzness of Hessian, we have thatf(wt0+s) ≤ f(wt0+s−1)− η〈∇f(wt0+s−1),mt0+s−1〉+ 12 η2m>t0+s−1∇2f(wt0+s−1)mt0+s−1 + ρ6 η3‖mt0+s−1‖3.(19)By adding η〈∇f(wt0+s−1), gt0+s−1〉 on both sides, we haveη〈∇f(wt0+s−1), gt0+s−1〉 ≤ f(wt0+s−1)− f(wt0+s) + η〈∇f(wt0+s−1), gt0+s−1 −mt0+s−1〉+ 12 η2m>t0+s−1∇2f(wt0+s−1)mt0+s−1 + ρ6 η3‖mt0+s−1‖3.(20)Taking conditional expectation on both sides leads toEt0+s−1[η‖∇f(wt0+s−1)‖2] ≤ Et0+s−1[f(wt0+s−1)− f(wt0+s)] + η2ch + ρ6 η3c3m, (21)where Et0+s−1[η〈∇f(wt0+s−1), gt0+s−1 −mt0+s−1〉 + 12η 2m>t0+s−1∇ 2f(wt0+s−1)mt0+s−1] ≤ η2ch by the GrACE property. We have that for t0 ≤ t0 + s− 1Et0 [η‖∇f(wt0+s−1)‖2] = Et0 [Et0+s−1[η‖∇f(wt0+s−1)‖2]] (21)≤ Et0 [Et0+s−1[f(wt0+s−1)− f(wt0+s)]] + η2ch + ρ6 η3c3m= Et0 [f(wt0+s−1)− f(wt0+s)] + η2ch + ρ6 η3c3m. (22)Summing the above inequality from s = 2, 3, . . . , t leads to Et0 [ t−1∑ s=1 η‖∇f(wt0+s)‖2] ≤ Et0 [f(wt0+1)− f(wt0+t)] + η2(t− 1)ch + ρ 6 η3(t− 1)c3m= Et0 [f(wt0+1)− f(wt0) + f(wt0)− f(wt0+t)] + η2(t− 1)ch + ρ6 η3(t− 1)c3m(a) ≤ Et0 [f(wt0+1)− f(wt0)] + Fthred + η2(t− 1)ch + ρ6 η3(t− 1)c3m,(23)where (a) is by the assumption (made for proving by contradiction) that Et0 [f(wt0)− f(wt0+s)] ≤ Fthred for any 0 ≤ s ≤ Tthred. By (21) with s = 1 and η = r, we haveEt0 [r‖∇f(wt0)‖2] ≤ Et0 [f(wt0)− f(wt0+1)] + r2ch + ρ6 r3c3m. (24)By (23) and (24), we know thatEt0 [ t−1∑ s=1 η‖∇f(wt0+s)‖2] ≤ Et0 [r‖f(wt0)‖2] + Et0 [ t−1∑ s=1 η‖∇f(wt0+s)‖2]≤ Fthred + r2ch + ρ6 r3c3m + η2tch + ρ6 η3tc3m(a) ≤ Fthred + 2r2ch + ρ6 r3c3m +ρ 6 r2ηc3m.(b) ≤ Fthred + 2r2ch + ρ3 r3c3m, (25)where (a) is by the constraint that η2t ≤ r2 for 0 ≤ t ≤ Tthred and (b) is by the constraint that r ≥ η. By combining (25) and (18)Et0 [‖wt0+t − wt0‖2] ≤ Et0 [8η2 t (1− β)2 t−1∑ s=1 ‖∇f(wt0+s)‖2] + 8η2 tσ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m≤ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8η2tσ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(26)Lemma 2 Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic approximation at wt0 , Q(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉 + 12 (w − wt0)>H(w − wt0), where H := ∇2f(wt0). Also, define Gs := (I − η ∑s k=1 β s−kH) and• qv,t−1 := ( Πt−1j=1Gj )( − rmt0 ) . • qm,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj ) βsmt0 . • qq,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj )∑s k=1 βs−k(∇f(wt0+k)−∇Q(wt0+s)). • qw,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj )∑s k=1 β s−k∇f(wt0). • qξ,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj )∑s k=1 β s−kξt0+k.Then, wt0+t − wt0 = qv,t−1 + ηqm,t−1 + ηqq,t−1 + ηqw,t−1 + ηqξ,t−1.Notations: Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic approximation at wt0 ,Q(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉+ 12 (w − wt0)>H(w − wt0), (27)where H := ∇2f(wt0). Also, we denoteGs := (I − η s∑k=1βs−kH)vm,s := β smt0vq,s := s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) vw,s :=s∑ k=1 βs−k∇f(wt0)vξ,s := s∑ k=1 βs−kξt0+kθs := s∑ k=1 βs−k.(28)Proof. First, we rewrite mt0+j for any j ≥ 1 as follows.mt0+j = β jmt0 + j∑ k=1 βj−kgt0+k= βjmt0 + j∑ k=1 βj−k ( ∇f(wt0+k) + ξt0+k ) .(29)We have thatwt0+t − wt0 = wt0+t−1 − wt0 − ηmt0+t−1(a) = wt0+t−1 − wt0 − η ( βt−1mt0 + t−1∑ k=1 βt−1−k ( ∇f(wt0+k) + ξt0+k )) (b) = wt0+t−1 − wt0 − ηt−1∑ k=1 βt−1−k∇Q(wt0+t−1)− η ( βt−1mt0 + t−1∑ k=1 βt−1−k ( ∇f(wt0+k)−∇Q(wt0+t−1) + ξt0+k )) (c) = wt0+t−1 − wt0 − ηt−1∑ k=1 βt−1−k ( H(wt0+t−1 − wt0) +∇f(wt0) ) − η ( βt−1mt0 +t−1∑ k=1 βt−1−k ( ∇f(wt0+k)−∇Q(wt0+t−1) + ξt0+k )) = (I − ηt−1∑ k=1 βt−1−kH) ( wt0+t−1 − wt0 ) − η ( βt−1mt0 +t−1∑ k=1 βt−1−k ( ∇f(wt0+k)−∇Q(wt0+t−1) +∇f(wt0) + ξt0+k )) ,(30) where (a) is by using (29) with j = t− 1, (b) is by subtracting and adding back the same term, and(c) is by ∇Q(wt0+t−1) = ∇f(wt0) +H(wt0+t−1 − wt0). To continue, by using the nations in (28), we can rewrite (30) aswt0+t − wt0 = Gt−1 ( wt0+t−1 − wt0 ) − η ( vm,t−1 + vq,t−1 + vw,t−1 + vξ,t−1 ) . (31)Recursively expanding (31) leads to wt0+t − wt0 = Gt−1 ( wt0+t−1 − wt0 ) − η ( vm,t−1 + vq,t−1 + vw,t−1 + vξ,t−1 ) = Gt−1 ( Gt−2 ( wt0+t−2 − wt0 ) − η ( vm,t−2 + vq,t−2 + vw,t−2 + vξ,t−2)) − η ( vm,t−1 + vq,t−1 + vw,t−1 + vξ,t−1) (a) = ( Πt−1j=1Gj )( wt0+1 − wt0)− ηt−1∑ s=1 ( Πt−1j=s+1Gj )( vm,s + vq,s + vw,s + vξ,s ) ,(b) = ( Πt−1j=1Gj )( − rmt0 ) − η t−1∑ s=1 ( Πt−1j=s+1Gj )( vm,s + vq,s + vw,s + vξ,s ) ,(32)where (a) we use the notation that Πt−1j=sGj := Gs × Gs+1 × . . . . . . Gt−1 and the notation that Πt−1j=tGj = 1 and (b) is by the update rule. By using the definitions of {q?,t−1} in the lemma statement, we complete the proof.Lemma 3 Following the notations of Lemma 2, we have thatEt0 [‖wt0+t − wt0‖2] ≥ Et0 [‖qv,t−1‖2] + 2ηEt0 [〈qv,t−1, qm,t−1 + qq,t−1 + qw,t−1 + qξ,t−1〉] := Clower (33)Proof. Following the proof of Lemma 2, we have wt0+t − wt0 = qv,t−1 + η ( qm,t−1 + qq,t−1 + qw,t−1 + qξ,t−1 ) . (34)Therefore, by using ‖a+ b‖2 ≥ ‖a‖2 + 2〈a, b〉,Et0 [‖wt0+t − wt0‖2] ≥ Et0 [‖qv,t−1‖2] + 2ηEt0 [〈qv,t−1, qm,t−1 + qq,t−1 + qw,t−1 + qξ,t−1〉]. (35)Lemma 5 Let Fthred = O( 4) and η2Tthred ≤ r2. By following the conditions and notations in Theorem 1, Lemma 1 and Lemma 2, we conclude that if SGD with momentum (Algorithm 2) has the APCG property, then we have that Clower := Et0 [‖qv,Tthred−1‖2]+2ηEt0 [〈qv,Tthred−1, qm,Tthred−1 + qq,Tthred−1 + qw,Tthred−1 + qξ,Tthred−1〉] > Cupper.W.l.o.g, we assume that cm, L, σ2, c′, ch, and ρ are not less than one and that ≤ 1.E.1 SOME CONSTRAINTS ON β .We require that parameter β is not too close to 1 so that the following holds,• 1) L(1− β)3 > 1. • 2) σ2(1− β)3 > 1. • 3) c′(1− β)2 > 1. • 4) η ≤ 1−βL . • 5) η ≤ 1−β .• 6) Tthred ≥ c(1−β)η log( Lcmσ 2ρc′ch (1−β)δγ ) ≥ 1 + 2β 1−β .The constraints upper-bound the value of β. That is, β cannot be too close to 1. We note that the β dependence on L, σ, and c′ are only artificial. We use these constraints in our proofs but they are mostly artefacts of the analysis. For example, if a function is L-smooth, and L < 1, then it is also 1-smooth, so we can assume without loss of generality that L > 1. Similarly, the dependence on σ is not highly relevant, since we can always increase the variance of the stochastic gradient, for example by adding an O(1) gaussian perturbation.To prove Lemma 5, we need a series of lemmas with the choices of parameters on Table 3.Upper bounding Et0 [‖qq,t−1‖]:Lemma 9. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [‖qq,t−1‖] ≤ ( Πt−1j=1(1 + ηθjλ) ) βLcm (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η .(36)Et0 [‖qq,t−1‖] = Et0 [‖ − t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](a) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](b) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2‖ s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](c) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k‖ ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](d) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ( ‖∇f(wt0+k)−∇f(wt0+s)‖+ ‖∇f(wt0+s)−∇Q(wt0+s)‖ ) ](37) where (a), (c), (d) is by triangle inequality, (b) is by the fact that ‖Ax‖2 ≤ ‖A‖2‖x‖2 for any matrix A and vector x. Now that we have an upper bound of ‖∇f(wt0+k)−∇f(wt0+s)‖,‖∇f(wt0+k)−∇f(wt0+s)‖ (a) ≤ L‖wt0+k − wt0+s‖ (b) ≤ Lη(s− k)cm. (38)where (a) is by the assumption of L-Lipschitz gradient and (b) is by applying the triangle inequality (s − k) times and that ‖wt − wt−1‖ ≤ η‖mt−1‖ ≤ ηcm, for any t. We can also derive an upper bound of Et0 [‖∇f(wt0+s)−∇Q(wt0+s)‖],Et0 [‖∇f(wt0+s)−∇Q(wt0+s)‖] (a)≤ Et0 [ ρ2 ‖wt0+s − wt0‖2](b) ≤ ρ 2 (8ηs(Fthred + 2r2ch + ρ3r3c3m) (1− β)2 + 8 r2σ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m. )(39) Above, (a) is by the fact that if a function f(·) has ρ Lipschitz Hessian, then‖∇f(y)−∇f(x)−∇2f(x)(y − x)‖ ≤ ρ 2 ‖y − x‖2 (40)(c.f. Lemma 1.2.4 in (Nesterov (2013))) and using the definition thatQ(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉+ 12 (w − wt0)>H(w − wt0),(b) is by Lemma 1 and η2t ≤ r2 for 0 ≤ t ≤ Tthred Et0 [‖wt0+t − wt0‖2] ≤ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8η2tσ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m≤ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(41) Combing (37), (38), (39), we have thatEt0 [‖qq,t−1‖] (37) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ( ‖∇f(wt0+k)−∇f(wt0+s)‖+ ‖∇f(wt0+s)−∇Q(wt0+s)‖ ) ](38),(39) ≤ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm+ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (8ηs(Fthred + 2r2ch + ρ3r3c3m) (1− β)2 + 8 r2σ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m ):= t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm + t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (νs + ν),(42) where on the last line we use the notation thatνs := 8ηs ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2ν := 8 r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(43)To continue, let us analyze ‖ ( Πt−1j=s+1Gj ) ‖2 first.‖ ( Πt−1j=s+1Gj ) ‖2 = ‖Πt−1j=s+1(I − η j∑ k=1 βj−kH)‖2(a) ≤ Πt−1j=s+1(1 + ηθjλ) = Πt−1j=1(1 + ηθjλ)Πsj=1(1 + ηθjλ)(b) ≤ Πt−1j=1(1 + ηθjλ)(1 + η )s . (44)Above, we use the notation that θj := ∑j k=1 βj−k. For (a), it is due to that λ := −λmin(H), λmax(H) ≤ L, and the choice of η so that 1 ≥ ηL1−β , or equivalently,η ≤ 1− β L . (45)For (b), it is due to that θj ≥ 1 for any j and λ ≥ . Therefore, we can upper-bound the first term on r.h.s of (42) ast−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm = t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s−1∑ k=1 βkkLηcm(a) ≤ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2β(1− β)2 Lηcm(b) ≤ ( Πt−1j=1(1 + ηθjλ) ) βLηcm (1− β)2 t−1∑ s=11(1 + η )s(c) ≤ ( Πt−1j=1(1 + ηθjλ) ) βLηcm (1− β)2 1 η = ( Πt−1j=1(1 + ηθjλ) ) βLcm (1− β)2 , (46)where (a) is by that fact that ∑∞ k=1 βkk ≤ β(1−β)2 for any 0 ≤ β < 1, (b) is by using (44), and (c) is by using that ∑∞ s=1( 1 1+η )s ≤ 1η . Now let us switch to bound∑t−1 s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 ∑s k=1 β s−k ρ 2 (νs + ν) on (42). We have that t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (νs + ν) (a) ≤ 1 1− β t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 ρ 2 (νs + ν)(b) ≤ ( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 νs +( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 ν(c) ≤ ( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 νs +( Πt−1j=1(1 + ηθjλ) ) 1− β ρν 2η=( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 νs +( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η(d) ≤ ( Πt−1j=1(1 + ηθjλ) ) 1− β ρ (η )2 8η ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m )2η (47) where (a) is by the fact that ∑s k=1 βs−k ≤ 1/(1 − β), (b) is by (44), (c) is by using that∑∞ s=1( 1 1+η ) s ≤ 1η , (d) is by ∑∞ k=1 z kk ≤ z(1−z)2 for any |z| ≤ 1 and substituting z = 1 1+η ,which leads to ∑∞ k=1 z kk ≤ z(1−z)2 = 1/(1+η ) (1−1/(1+η ))2 = 1+η (η )2 ≤ 2 (η )2 in which the last inequality is by chosen the step size η so that η ≤ 1. By combining (42), (46), and (47), we have thatEt0 [‖qq,t−1‖ (42) ≤ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm + t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (νs + ν)(46),(47) ≤ ( Πt−1j=1(1 + ηθjλ) ) βLcm (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η ,(48) which completes the proof.Lemma 10. Following the conditions in Lemma 1 and Lemma 2, we have‖qv,t−1‖ ≤ ( Πt−1j=1(1 + ηθjλ) ) rcm. (49)Proof. ‖qv,t−1‖ ≤ ‖ ( Πt−1j=1Gj )( − rmt0 ) ‖ ≤ ‖ ( Πt−1j=1Gj ) ‖2‖ − rmt0‖ ≤ ( Πt−1j=1(1 + ηθjλ) ) rcm, (50) where the last inequality is because η is chosen so that 1 ≥ ηL1−β and the fact that λmax(H) ≤ L.Lower bounding Et0 [2η〈qv,t−1, qq,t−1〉]: Lemma 11. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [2η〈qv,t−1, qq,t−1〉] ≥ −2η ( Πt−1j=1(1 + ηθjλ) )2 rcm×[ βLcm(1− β)2 +ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)3 + ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η (1− β) ].(51)Proof. By the results of Lemma 9 and Lemma 10Et0 [2η〈qv,t−1, qq,t−1〉] ≥ −Et0 [2η‖qv,t−1‖‖qq,t−1‖] Lemma 10 ≥ −Et0 [2η ( Πt−1j=1(1 + ηθjλ) ) rcm‖qq,t−1‖]Lemma 9 ≥ −2η ( Πt−1j=1(1 + ηθjλ) )2 rcm×[ βLcm(1− β)2 +ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)3 + ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η (1− β) ].(52)Lower bounding Et0 [2η〈qv,t−1, qξ,t−1〉]: Lemma 12. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [2η〈qv,t−1, qξ,t−1〉] = 0. (53)Et0 [2η〈qv,t−1, qξ,t−1〉] = Et0 [2η〈qv,t−1,− t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−kξt0+k〉](a) = Et0 [2η〈qv,t−1, s∑ k=1 αkξt0+k〉](b) = Et0 [2η s∑ k=1 Et0+k−1[〈qv,t−1, αkξt0+k〉]](c) = Et0 [2η s∑ k=1 〈qv,t−1,Et0+k−1[αkξt0+k]〉]= Et0 [2η s∑k=1αk〈qv,t−1,Et0+k−1[ξt0+k]〉](d) = 0,(54)where (a) holds for some coefficients αk, (b) is by the tower rule, (c) is because qv,t−1 is measureable with t0, and (d) is by the zero mean assumption of ξ’s.Lower bounding Et0 [2η〈qv,t−1, qm,t−1〉]: Lemma 13. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [2η〈qv,t−1, qm,t−1〉] ≥ 0. (55)Proof. Et0 [2η〈qv,t−1, qm,t−1〉]= 2ηrEt0 [〈 ( Πt−1j=1Gj ) mt0 , t−1∑ s=1 ( Πt−1j=s+1Gj ) βsmt0〉](a) = 2ηrEt0 [〈mt0 , Bmt0〉](b) ≥ 0,(56)where (a) is by defining the matrix B := ( Πt−1j=1Gj )>(∑t−1 s=1 ( Πt−1j=s+1Gj ) βs ) . For (b), notice that the matrix B is symmetric positive semidefinite. To see that the matrix B is symmetric positive semidefinite, observe that each Gj := (I − η ∑j k=1 βj−kH) can be written in the form of Gj = UDjU> for some orthonormal matrix U and a diagonal matrix Dj . Therefore, the matrix product( Πt−1j=1Gj )>( Πt−1j=s+1Gj ) = U(Πt−1j=1Dj)(Π t−1 j=s+1Dj)U> is symmetric positive semidefinite as long as each Gj is. So, (b) is by the property of a matrix being symmetric positive semidefinite.Lower bounding 2ηEt0 [〈qv,t−1, qw,t−1〉]: Lemma 14. Following the conditions in Lemma 1 and Lemma 2, if SGD with momentum has the APCG property, then2ηEt0 [〈qv,t−1, qw,t−1〉] ≥ − 2ηrc′(1− β) (Πt−1j=1(1 + ηθjλ)) 2 . (57)Proof. Define Ds := Πt−1j=1GjΠ t−1 j=s+1Gj .2ηEt0 [〈qv,t−1, qw,t−1〉] = 2ηEt0 [〈 ( Πt−1j=1Gj )( rmt0 ) , t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k∇f(wt0)〉]= 2ηEt0 [〈rmt0 , t−1∑ s=1 ( Πt−1j=1GjΠ t−1 j=s+1Gj ) s∑ k=1 βs−k∇f(wt0)〉]= 2ηr t−1∑ s=1 s∑ k=1 βs−kEt0 [〈mt0 , Ds∇f(wt0)〉](a) ≥ −2η2rc′ t−1∑ s=1 s∑ k=1 βs−k‖Ds‖2‖∇f(wt0)‖2≥ −2η 2rc′1− β t−1∑ s=1 ‖Ds‖2‖∇f(wt0)‖2, (58)where (a) is by the APCG property. We also have that ‖Ds‖2 = ‖Πt−1j=1GjΠ t−1 j=s+1Gj‖2 ≤ ‖Π t−1 j=1Gj‖2‖Π t−1 j=s+1Gj‖2(a) ≤ ‖Πt−1j=1Gj‖2 Πt−1j=1(1 + ηθjλ)(1 + η )s(b) ≤ ( Πt−1j=1(1 + ηθjλ) )2 (1 + η )s(59)where (a) and (b) is by (44). Substituting the result back to (58), we get2ηEt0 [〈qv,t−1, qw,t−1〉] ≥ − 2η2rc′1− β t−1∑ s=1 ‖Ds‖2‖∇f(wt0)‖2≥ −2η 2rc′1− β t−1∑ s=1( Πt−1j=1(1 + ηθjλ) )2 (1 + η )s ‖∇f(wt0)‖2 ≥ − 2η2rc′ (1− β)η ( Πt−1j=1(1 + ηθjλ) )2‖∇f(wt0)‖2 (60)Using the fact that ‖∇f(wt0)‖ ≤ completes the proof.Recall that the strategy is proving by contradiction. Assume that the function value does not decrease at least Fthred in Tthred iterations on expectation. Then, we can get an upper bound of the expected distance Et0 [‖wt0+Tthred − wt0‖2] ≤ Cupper but, by leveraging the negative curvature, we can also show a lower bound of the form Et0 [‖wt0+Tthred − wt0‖2] ≥ Clower. The strategy is showing that the lower bound is larger than the upper bound, which leads to the contradiction and concludes that the function value must decrease at least Fthred in Tthred iterations on expectation. To get the contradiction, according to Lemma 1 and Lemma 3, we need to show that Et0 [‖qv,Tthred−1‖2] + 2ηEt0 [〈qv,Tthred−1, qm,Tthred−1 + qq,Tthred−1 + qw,Tthred−1 + qξ,Tthred−1〉] > Cupper. (61) Yet, by Lemma 13 and Lemma 12, we have that ηEt0 [〈qv,Tthred−1, qm,Tthred−1〉] ≥ 0 and ηEt0 [〈qv,Tthred−1, qξ,Tthred−1〉] = 0. So, it suffices to prove that Et0 [‖qv,Tthred−1‖2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1 + qw,Tthred−1〉] > Cupper, (62) and it suffices to show that• 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1〉] ≥ 0.• 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qw,Tthred−1〉] ≥ 0.• 14Et0 [‖qv,Tthred−1‖ 2] ≥ Cupper.E.3.1 PROVING THAT 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1〉] ≥ 0:By Lemma 4 and Lemma 11, we have that1 4 Et0 [‖qv,Tthred−1‖2] + Et0 [2η〈qv,Tthred−1, qq,Tthred−1〉] ≥ 1 4 ( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ − 2η ( ΠTthred−1j=1 (1 + ηθjλ) )2 rcm × [ βLcm (1− β)2 + ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)3 + ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η (1− β) ].(63)To show that the above is nonnegative, it suffices to show thatr2γ ≥ 24ηrβLc 2 m(1− β)2 , (64)andr2γ ≥ 24ηrcmρ (1− β)η 28 ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 , (65)andr2γ ≥ 24ηrcm 1− βρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m )2η . (66)Now w.l.o.g, we assume that cm, L, σ2, c′, and ρ are not less than one and that ≤ 1. By using the values of parameters on Table 3, we have the following results; a sufficient condition of (64) is thatcr cη ≥ 24Lc2 m 2(1− β)2 . (67)A sufficient condition of (65) is thatcr cF ≥ 576cmρ (1− β)3 , (68)and 1 ≥ 1152cmρchcr(1− β)3 , (69)and1 ≥ 192c 4 mρ 2c2r (1− β)3 . (70)A sufficient condition of (66) is that1 ≥ 96cmρ(σ 2 + 3c2m)cr(1− β)3 , (71)and a sufficient condition for the above (71), by the assumption that both σ2 ≥ 1 and cm ≥ 1, is1 ≥ 576c 3 mρσ 2cr(1− β)3 . (72)Now let us verify if (67), (68), (69), (70), (72) are satisfied. For (67), using the constraint of cη on Table 3, we have that 1cη ≥ c5mρL 2σ2c′ch c1. Using this inequality, it suffices to let cr ≥ c0 2c3mρLσ 2c′ch(1−β)2 for getting (67), which holds by using the constraint that c′(1−β)2 > 1 and ≤ 1.For (68), using the constraint of cF on Table 3, we have that 1cF ≥ c4mρ 2Lσ4ch c2. Using this inequality, it suffices to let cr ≥ c0c3mρLσ4(1−β)3 , which holds by using the constraint that σ 2(1− β)3 > 1. For (69), it needs (1−β) 31152cmρch ≥ c0c3mρLσ2ch ≥ cr, which hold by using the constraint that σ 2(1− β)3 > 1.For (70), it suffices to let (1−β) 214c2mρ ≥ c0c3mρLσ2ch ≥ cr which holds by using the constraint thatσ2(1− β)3 > 1. For (72), it suffices to let (1−β) 3576c3mρσ 2 ≥ c0 c3mρLσ 2ch ≥ cr, which holds by using theconstraint that L(1− β)3 > 1 and ≤ 1. Therefore, by choosing the parameter values as Table 3, we can guarantee that 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1〉] ≥ 0.E.3.2 PROVING THAT 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qw,Tthred−1〉] ≥ 0:By Lemma 4 and Lemma 14, we have that1 4 Et0 [‖qv,Tthred−1‖2] + 2ηEt0 [〈qv,Tthred−1, qw,Tthred−1〉]≥ 1 4( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ − 2ηrc ′(1− β) (ΠTthred−1j=1 (1 + ηθjλ))2 . (73)To show that the above is nonnegative, it suffices to show thatr2γ ≥ 8ηrc ′(1− β) . (74)A sufficient condition is crcη ≥ 8 4c′ 1−β . Using the constraint of cη on Table 3, we have that 1 cη ≥ c5mρL 2σ2c′ch c1 . So, it suffices to let cr ≥ c0 4 3c5mρL 2σ2ch(1−β) , which holds by using the constraint that L(1− β)3 > 1 (so that L(1− β) > 1) and ≤ 1.E.3.3 PROVING THAT 14Et0 [‖qv,Tthred−1‖ 2] ≥ Cupper :From Lemma 4 and Lemma 1, we need to show that14( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ≥ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m. (75)We know that 14 ( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ ≥ 14 ( ΠTthred−1j=1 (1 + ηθj ) )2 r2γ. It suffices to show that 14( ΠTthred−1j=1 (1 + ηθj ) )2 r2γ≥ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m. (76)Note that the left hand side is exponentially growing in Tthred. We can choose the number of iterations Tthred large enough to get the desired result. Specifically, we claim that Tthred ≥ c(1−β) η log( Lcmσ 2ρc′ch(1−β)δγ ) for some constant c > 0. To see this, let us first apply log on both sides of (76),2 ( Tthred−1∑j=1log(1 + ηθj ) ) + log(r2γ) ≥ log(8aTthred + 8b) (77)where we denote a := 4η ( Fthred+2r2ch+ ρ3 r 3c3m ) (1−β)2 and b := 4 r2σ2 (1−β)2 + 2η 2 ( β 1−β )2 c2m + r2c2m. To proceed, we are going to use the inequality log(1 + x) ≥ x2 , for x ∈ [0,∼ 2.51]. We have that1 ≥ η (1− β)(78)as guaranteed by the constraint of η. So,2 ( Tthred−1∑j=1 log(1 + ηθj ) ) (a) ≥ Tthred−1∑ j=1 ηθj = Tthred−1∑ j=1 j−1∑ k=0 βkη= Tthred−1∑ j=1 1− βj 1− β η ≥ 1 1− β (Tthred − 1− β 1− β )η .(b) ≥ Tthred − 1 2(1− β) η , (79)where (a) is by using the inequality log(1 + x) ≥ x2 with x = ηθj ≤ 1 and (b) is by making Tthred−1 2(1−β) ≥ β (1−β)2 , which is equivalent to the condition thatTthred ≥ 1 + 2β1− β (80)Now let us substitute the result of (79) back to (77). We have thatTthred ≥ 1 + 2(1− β)η log( 8aTthred + 8b γr2 ), (81)which is what we need to show. By choosing Tthred large enough,Tthred ≥ c(1− β)η log(Lcmσ 2ρc′ch(1− β)δγ ) = O((1− β) log( 1 (1− β) ) −6) (82)for some constant c > 0, we can guarantee that the above inequality (81) holds.Lemma 15 (Daneshmand et al. (2018)) Let us define the event Υk := {‖∇f(wkTthred)‖ ≥ or λmin(∇2f(wkTthred)) ≤ − }. The complement is Υck := {‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≥ − }, which suggests that wkTthred is an ( , )-second order stationary points. Suppose thatE[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ −∆E[f(w(k+1)Tthred)− f(wkTthred)|Υ c k] ≤ δ∆ 2 .(83)Set T = 2Tthred ( f(w0) − minw f(w) ) /(δ∆). We return w uniformly randomly from w0, wTthred , w2Tthred , . . . , wkTthred , . . . , wKTthred , where K := bT/Tthredc. Then, with probability at least 1− δ, we will have chosen a wk where Υk did not occur.Proof. Let Pk be the probability that Υk occurs. E[f(w(k+1)Tthred)− f(wkTthred)] = E[f(w(k+1)Tthred)− f(wkTthred)|Υk]Pk + E[f(w(k+1)Tthred)− f(wkTthred)|Υ c k](1− Pk)≤ −∆Pk + δ∆/2(1− Pk) = δ∆/2− (1 + δ/2)∆Pk ≤ δ∆/2−∆Pk.(84) Summing over all K, we have1K + 1 K∑ k=0 E[f(w(k+1)Tthred)− f(wkTthred)] ≤ ∆ 1 K + 1 K∑ k=0 (δ/2− Pk)⇒ 1 K + 1 K∑ k=0 Pk ≤ δ/2 + f(w0)−minw f(w) (K + 1)∆ ≤ δ⇒ 1 K + 1 K∑ k=0 (1− Pk) ≥ 1− δ.(85)Theorem 1 Assume that the stochastic momentum satisfies CNC. Set r = O( 2), η = O( 5), and Tthred = c(1−β)η log( Lcmσ 2ρc′ch (1−β)δγ ) = O((1 − β) log( Lcmσ 2ρc′ch (1−β)δγ )−6) for some constant c > 0. If SGD with momentum (Algorithm 2) has APAG property when gradient is large (‖∇f(w)‖ ≥ ), APCGTthred property when it enters a region of saddle points that exhibits a negative curvature (‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − ), and GrACE property throughout the iterations, then it reaches an ( , ) second order stationary point in T = 2Tthred(f(w0)−minw f(w))/(δFthred) = O((1− β) log(Lcmσ2ρc′ch (1−β)δγ ) −10) iterations with high probability 1− δ, where Fthred = O( 4).In this subsection, we provide a sketch of the proof of Theorem 1. The complete proof is available in Appendix G. Our proof uses a lemma in (Daneshmand et al. (2018)), which is Lemma 15 below. The lemma guarantees that uniformly sampling a w from {wkTthred}, k = 0, 1, 2, . . . , bT/Tthredc gives an ( , )-second order stationary point with high probability. We replicate the proof of Lemma 15 in Appendix F.Lemma 15. (Daneshmand et al. (2018)) Let us define the event Υk := {‖∇f(wkTthred)‖ ≥ or λmin(∇2f(wkTthred)) ≤ − }. The complement is Υck := {‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≥ − }, which suggests that wkTthred is an ( , )-second order stationary points. Suppose thatE[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ −∆ & E[f(w(k+1)Tthred)− f(wkTthred)|Υ c k] ≤ δ∆ 2 . (86) Set T = 2Tthred ( f(w0) − minw f(w) ) /(δ∆). 8 We return w uniformly randomly from w0, wTthred , w2Tthred , . . . , wkTthred , . . . , wKTthred , where K := bT/Tthredc. Then, with probability at least 1− δ, we will have chosen a wk where Υk did not occur.To use the result of Lemma 15, we need to let the conditions in (86) be satisfied. We can bound E[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ −Fthred, based on the analysis of the large gradient norm regime (Lemma 7) and the analysis for the scenario when the update is with small gradient norm but a large negative curvature is available (Subsection 3.2.1). For the other condition, E[f(w(k+1)Tthred)− f(wkTthred)|Υck] ≤ δ Fthred 2 , it requires that the expected amortized increase of function value due to taking the large step size r is limited (i.e. bounded by δFthred2 ) when wkTthred is a second order stationary point. By having the conditions satisfied, we can apply Lemma 15 and finish the proof of the theorem.Proof. Our proof is based on Lemma 15. So, let us consider the events in Lemma 15, Υk := {‖∇f(wkTthred)‖ ≥ or λmin(∇2f(wkTthred)) ≤ − }. We first show that E[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ Fthred.8One can use any upper bound of f(w0)−minw f(w) as f(w0)−minw f(w) in the expression of T .Consider that Υk is the case that ‖∇f(wkTthred)‖ ≥ . Denote t0 := kTthred in the following. We have thatEt0 [f(wt0+Tthred)− f(wt0)] = Tthred−1∑ t=0 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]]= Et0 [f(wt0+1)− f(wt0)] + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](a) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m 2 + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](b) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m 2 + Tthred−1∑ t=1 ( η2ch + ρ 6 η3c3m ) (c) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m 2 + r2ch + ρ 6 r3c3m(d) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m + r2ch(e) ≤ −r 2 2 + Lr2c2m + r 2ch (f) ≤ −r 4 2 (g) ≤ −Fthred,(87)where (a) is by using Lemma 6 with step size r, (b) is by using Lemma 8, (c) is due to the constraint that η2Tthred ≤ r2, (d) is by the choice of r, (e) is by ‖∇f(wt)‖ ≥ , (f) is by the choice of r so that r ≤ 24(Lc2m+ch) , and (g) is byr 4 2 ≥ Fthred. (88)The scenario that Υk is the case that ‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≤ − has been analyzed in Appendix E, which guarantees that E[f(wt0+Tthred) − f(wt0)] ≤ −Fthred under the setting.Now let us switch to show that E[f(w(k+1)Tthred) − f(wkTthred)|Υck] ≤ δ Fthred 2 . Recall that Υ c k means that ‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≥ − . Denote t0 := kTthred in the following. We have thatEt0 [f(wt0+Tthred)− f(wt0)] = Tthred−1∑ t=0 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]]= Et0 [f(wt0+1)− f(wt0)] + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](a) ≤ r2ch + ρ6 r3c3m + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](b) ≤ r2ch + ρ6 r3c3m + Tthred−1∑ t=1 ( η2ch + ρ 6 η3c3m ) (c) ≤ 2r2ch + ρ3 r3c3m ≤ 4r2ch(d) ≤ δFthred 2 . (89)where (a) is by using Lemma 8 with step size r, (b) is by using Lemma 8 with step step size η, (c) is by setting η2Tthred ≤ r2 and η ≤ r, (d) is by the choice of r so that 8r2ch ≤ δFthred.Now we are ready to use Lemma 15, since both the conditions are satisfied. According to the lemma and the choices of parameters value on Table 3, we can set T = 2Tthred ( f(w0) −minw f(w) ) /(δFthred) = O((1− β) log(Lcmσ 2ρc′ch (1−β)δγ )−10), which will return a w that is an ( , ) second order stationary point. Thus, we have completed the proof.Theorem 2 in Daneshmand et al. (2018) states that, for CNC-SGD to find an ( , ρ1/2 ) stationary point, the total number of iterations is T = O( ` 10L3δ4γ4 log 2( `L δγ ) −10), where ` is the bound of the stochastic gradient norm ‖gt‖ ≤ `which can be viewed as the counterpart of cm in our paper. By translating their result for finding an ( , ) stationary point, it is T = O( ` 10L3ρ5δ4γ4 log 2(ρ`L δγ ) −10). On the other hand, using the parameters value on Table 3, we have that T = 2Tthred ( f(w0)−minw f(w) ) /(δFthred) = O( (1−β)c9mL 3ρ3(σ2)3c2hc ′δ4γ4 log( Lcmσ 2c′ch (1−β)δγ ) −10) for Algorithm 2.Before making a comparison, we note that their result does not have a dependency on the variance of stochastic gradient (i.e. σ2), which is because they assume that the variance is also bounded by the constant ` (can be seen from (86) in the supplementary of their paper where the variance terms ‖ζi‖ are bounded by `). Following their treatment, if we assume that σ2 ≤ cm, then on (71) we can instead replace (σ2 + 3c2m) with 4c 2 m and on (72) it becomes 1 ≥ 576c3mρcr (1−β)3 . This will remove all the parameters’ dependency on σ2. Now by comparing Õ((1− β)c9mc2hc′ · ρ3L3 δ4γ4 −10) of ours and T = Õ(ρ2`10 · ρ 3L3δ4γ4 −10) of Daneshmand et al. (2018), we see that in the high momentum regimewhere (1−β) << ρ 2`10c9mc 2 hc ′ , Algorithm 2 is strictly better than that of Daneshmand et al. (2018), whichmeans that a higher momentum can help to find a second order stationary point faster.
industrial or research application depend on computerized segmentation of different parts of images such as stagnant and flowing zones which is the toughest task. X-ray Computed Tomography (CT) is one of a powerful non-destructive technique for cross-sectional images of a 3D object based on X-ray absorption. CT is the most proficient for investigating different granular flow phenomena and segmentation of the stagnant zone as compared to other imaging techniques. In any case, manual segmentation is tiresome and erroneous for further investigations. Hence, automatic and precise strategies are required. In the present work, a U-net architecture is used for segmenting the stagnant zone during silo discharging process. This proposed image segmentation method provides fast and effective outcomes by exploiting a convolutional neural networks technique with an accuracy of 97 percent.Keywords—U-net segmentation; deep neural networks; stagnantzone; X-ray tomographyI. INTRODUCTIONIn many industries, like in mining, agriculture, civil engineering, and pharmaceutical manufacturing; silo is used for protecting, storing and loading granular materials into process machinery [1]. During the silo discharging process, there are two major types of flow: namely, “funnel” and “mass” flows [2]. In the case of mass flow, all granulates discharge with a uniform downward velocity across the entire cross-section area. Whereas, in the case of funnel flow, in which this paper focuses on and characterized by granular is flowing only in the center of the silo and creating a stagnant zone at the walls of the container [2,3].The hopper geometry, internal friction between particles and wall all have a direct impact on the flow type [4]. In order to understand and describe the flow behavior and evaluating the silo wall pressures, the knowledge of the density distribution within the bulk solid is very important aspect [5]. Furthermore, during funnel flow, the shape and size of the stagnant zone depend on different factors including the granular material, the bin wall roughness, the initial packing density and filling level [6].X-ray Computed tomography (CT) is one of the most powerful 3D imaging techniques among available tomographic techniques due to its high-resolution capability. For the last two decades CT has been used as a non-destructive method to characterize objects, visualize flows, analyze concentration changes of the bulk solid during silo discharging process [5 ,7,8]. Particle Image Velocimetry (PIV) [9,10] and Electrical Capacitance Tomography (ECT) [11–14] are also some of the techniques used to visualize concentration changes during flows of granular materials.Even though convolutional neural network (CNN) has recently become popular and has increasingly been used as an alternative to many traditional pattern recognition problems, its application for segmenting stagnant zones of X-ray CT images is not common. For processing and analyzing process tomography data, artificial neural network algorithms were applied in electrical impedance tomography images [15–17]. This paper proposed a deep neural networks technique for segmenting the stagnant zone automatically. The main advantage of the proposed approach is an effective segmentation for acquiring the desired characteristics of flow parameters without prior image processing or expert guidance.II. EXPERIMENTAL SET-UP USING X-RAY TOMOGRAPHYAn especially designed model silo, with rectangular bin, allowed carrying out in situ experiments. The bin part is 10 cm wide, 5 cm deep and 20 cm high. The left and right hopper angles can be independently set to generate different types of flows, i.e. mass and funnel flows, with concentric/eccentric discharging modes. The silo model design was easily customizable to shift the outlet position and the angle between the hopper and the silo with respect to the vertical. The silo material is polycarbonate with 5 mm thickness. The outlet width is manually set, which allows controlling the discharge velocity. During concentric flow, both hoppers’ angles are set to the same value. In order to observe the eccentric flow, these angles are set to a different value. The non-symmetric silo construction causes shift silo outlet to the left or right side, in the direction of the larger angle.The time-lapse studies were performed at INSA-Lyon (France) using a GE Phoenix v|tome|x device (see Fig. 1). The device is equipped with a high energy X-ray microfocus source (up to 160 kV) with a 4 μm spot size. The detector is equipped with a 1500X1920 pixels (with a pixel size of 127X127 μm 2 ). Fig. 2 presented a picture of the silo model inside X-ray tomography hutch. During measurements, the distance between the X-ray source and the detector was equal to 577 mm and voxel size 160 um. Sorghum and rice have been used as a granular material during the experimental campaign of this study.III. THREE-DIMENSIONAL SEGMENTATIONCross-sectional views of the X-ray tomography of initially loosely packed rice with smooth silo bin walls are presented in Fig. 2.The main aim of this study was to find an effective way of segmenting the stagnant zone during eccentric discharging modes. After the initial packing density scan was performed, the outlet of the silo was opened for about 2 second and the next scan was carried out till all granulate are discharged out from the silo. Thus, to analyze the concentration changes, theabsolute difference between two successive scans has been computed and it shows the formation of the stagnant zone in the case of funnel flow. Fig. 3c presents concentration changes during eccentric discharging mode (angle between the hopper and the silo with respect to the vertical axis was 30 º and 20 º ) and it reveals the evolution of the stagnant zone at the right side of the image with a good contrast in which the hopper angle was 20 º .The last step consists in segmenting the stagnant zones. By using a three-dimensional Otsu threshold method it was possible to compute the stagnant zone mask as presented in Fig.4.Although the Otsu method was able to segment the stagnant zone, it is hard to get 100% accurate segmentation out of it. Since some particles in the flow zone may shift in the position of another particle, giving an impression of no movement based on the absolute difference. Thus, such kind of effect could hinder the segmentation result. As a result, a deep neural networks approach has been applied for effective segmentation and explained hereafter.IV. U-NET BASED DEEP CONVOLUTIONAL NETWORKSThe main advantage of the proposed approach is an effective segmentation for acquiring the desired characteristicsof flow parameters without prior image processing or expert guidance. From several deep neural networks, u-net is one of the successful architecture which is used in different image segmentation tasks. Originally u-net neural network architecture was built for performing semantic segmentation on a small biomedical data-set [18]. The architecture is computationally efficient and trainable with a small dataset, which is a core advantage for datasets like in material science where the little amount of labelled data is available. Despite it might be good and well-fitted in bio-medical tasks, it needs to be tuned again for fitting the granular material X-ray CT image segmentation task investigated in this study. Thus, the u-net neural network architecture and hyper-parameters values are adjusted in order to get good segmentation results.The network has been trained on 2D segmentation and then it can find the whole 3D volume segmentation of a new scan in which the network has never seen before. As each of the CT images already contain repetitive structures with the corresponding variation, only very few images are required to train a network that generalizes reasonably well. One of the major modifications is that the original u-net used stochastic gradient descent optimizer [18], but this modified u-net architecture used Adam optimizer [19] to minimize the categorical cross-entropy objective.During training, 40 images were selected for the training and test dataset. The dataset was first divided into two subsets, train, and test. The first subset contains 30 images in which 80% were used for training and 20% for validation. The trained model was next tested on the second subset which contains the remaining 10 images. During training, 30 manually annotated ground truth segmentations were used to train the network to recognize the stagnant zone borders. Since the available dataset is small, an extensive amount of data augmentations has been applied to improve the performance of the network. The testing datasets were used for the evaluation of the network performance.Fig. 5 illustrates the comparison between the ground truth, segmentation by using Otsu and predicted segmentation by the modified u-net trained model for the same 2D image. As the result shows, Otsu segmentation is too sensitive for gradient changes and the trained model predicted a smooth segmentation like the ground truth.Intersection over Union (IoU) is used as an accuracy measure to compare dropped out ground truth slices to the predicted results. The IoU score is a standard performance measure for the object category segmentation problem. Given a set of images, the IoU measure gives the similarity between the predicted region and the ground-truth region for an object present in the set of images and is defined as true positives/(truepositives + false negatives + false positives). Table 1 presents the result of the experiment for segmenting stagnant zone.Once after having trained model using the CNN method, the end-to-end 3D automatic segmentation offers an effective and fast segmentation of stagnant zone during silo discharging process. The key advantage of this method could be used for deep investigation of flow characteristics without utilizing any prior image processing methods or expert guidance. For both the Otsu and the trained model execution time (on CPU) were compared for generating one segmented image. The Otsu segmentation took around 4 sec per one 2D image. Where else the trained model took less than a second for generating its prediction for a given input image.In order to prove that the trained model (which used the sorghum grains flow as a training dataset) could generate the stagnant zone segmentation for completely different scan and grain material, it was tested by pre-processing two 3D scans of rice grains flow. The result shows that the trained model was able to generate predicted segmentation successfully. Fig. 6 presents two major steps for generating the stagnant zone prediction of new scans having similar flow property (in this case rice grains flow). The first step was computing the absolute difference of two successive scans and then this new scan was given as an input to the trained model. Finally, the trained model generates the stagnant zone segmentation as shown in Fig. 6d. Fig. 7 illustrates 3D dense segmentation of rice grains flow by superimposing the predicted mask into the original scan.side view .V. CONCLUSIONIn order to analyse the X-ray CT data that has been acquired from different experimental campaigns and complex structure of granular material flows, it can be tedious and extremely timeconsuming for manual analysis. This paper presents a new approach for automatic segmentation of the stagnant zone in an effective way by exploiting the CNN technique. The main advantages of the proposed approach are the speed and effective segmentation for acquiring the desired characteristic flow parameters. Once having the trained model, it was tested that the model could generate a predicted segmentation in less than a second for a given completely new granular material flow image with an accuracy of 97%. The accuracy of the CNN approach could also probably be further improved if the delineations of the ground truth were acquired from different experts and more number of dataset was used. Moreover, the architecture of the model could be modified to accommodate 3D volumes of images as an input for processing them with corresponding 3D operations.The work is funded by the National Science Centre in Poland (grant number: 2015/19/B/ST8/02773).[18] Ronneberger, O., Fischer, P., Brox, T.: U-Net: ConvolutionalNetworks for Biomedical Image Segmentation. In: Medical ImageComputing and Computer-Assisted Intervention (MICCAI). pp.234–241. Springer (2015)[19] Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization.Int. Conf. Learn. Represent. (2014)View publication stats
In recent years, deep reinforcement learning has achieved many impressive results, including playing Atari games from raw pixels (Guo et al., 2014; Mnih et al., 2015; Schulman et al., 2015), and acquiring advanced manipulation and locomotion skills (Levine et al., 2016; Lillicrap et al., 2015; Watter et al., 2015; Heess et al., 2015b; Schulman et al., 2015; 2016). However, many of the successes come at the expense of high sample complexity. For example, the state-of-the-art Atari results require tens of thousands of episodes of experience (Mnih et al., 2015) per game. To master a game, one would need to spend nearly 40 days playing it with no rest. In contrast, humans and animals are capable of learning a new task in a very small number of trials. Continuing the previous example, the human player in Mnih et al. (2015) only needed 2 hours of experience before mastering a game. We argue that the reason for this sharp contrast is largely due to the lack of a good prior, which results in these deep RL agents needing to rebuild their knowledge about the world from scratch.Although Bayesian reinforcement learning provides a solid framework for incorporating prior knowledge into the learning process (Strens, 2000; Ghavamzadeh et al., 2015; Kolter & Ng, 2009), exact computation of the Bayesian update is intractable in all but the simplest cases. Thus, practical reinforcement learning algorithms often incorporate a mixture of Bayesian and domain-specific ideas to bring down sample complexity and computational burden. Notable examples include guided policy search with unknown dynamics (Levine & Abbeel, 2014) and PILCO (Deisenroth & Rasmussen, 2011). These methods can learn a task using a few minutes to a few hours of real experience, compared to days or even weeks required by previous methods (Schulman et al., 2015; 2016; Lillicrap et al., 2015). However, these methods tend to make assumptions about the environment (e.g., instrumentation for access to the state at learning time), or become computationally intractable in high-dimensional settings (Wahlström et al., 2015).Rather than hand-designing domain-specific reinforcement learning algorithms, we take a different approach in this paper: we view the learning process of the agent itself as an objective, which can be optimized using standard reinforcement learning algorithms. The objective is averaged across all possible MDPs according to a specific distribution, which reflects the prior that we would like to distill into the agent. We structure the agent as a recurrent neural network, which receives past rewards, actions, and termination flags as inputs in addition to the normally received observations. Furthermore, its internal state is preserved across episodes, so that it has the capacity to perform learning in its own hidden activations. The learned agent thus also acts as the learning algorithm, and can adapt to the task at hand when deployed.We evaluate this approach on two sets of classical problems, multi-armed bandits and tabular MDPs. These problems have been extensively studied, and there exist algorithms that achieve asymptotically optimal performance. We demonstrate that our method, named RL2, can achieve performance comparable with these theoretically justified algorithms. Next, we evaluate RL2 on a vision-based navigation task implemented using the ViZDoom environment (Kempka et al., 2016), showing that RL2 can also scale to high-dimensional problems.We define a discrete-time finite-horizon discounted Markov decision process (MDP) by a tupleM = (S,A,P, r, ρ0, γ, T ), in which S is a state set, A an action set, P : S × A × S → R+ a transition probability distribution, r : S ×A → [−Rmax, Rmax] a bounded reward function, ρ0 : S → R+ an initial state distribution, γ ∈ [0, 1] a discount factor, and T the horizon. In policy search methods, we typically optimize a stochastic policy πθ : S × A → R+ parametrized by θ. The objective is to maximize its expected discounted return, η(πθ) = Eτ [ ∑T t=0 γtr(st, at)], where τ = (s0, a0, . . .) denotes the whole trajectory, s0 ∼ ρ0(s0), at ∼ πθ(at|st), and st+1 ∼ P(st+1|st, at).We now describe our formulation, which casts learning an RL algorithm as a reinforcement learning problem, and hence the name RL2.We assume knowledge of a set of MDPs, denoted byM, and a distribution over them: ρM :M→ R+. We only need to sample from this distribution. We use n to denote the total number of episodes allowed to spend with a specific MDP. We define a trial to be such a series of episodes of interaction with a fixed MDP.This process of interaction between an agent and the environment is illustrated in Figure 1. Here, each trial happens to consist of two episodes, hence n = 2. For each trial, a separate MDP is drawn from ρM, and for each episode, a fresh s0 is drawn from the initial state distribution specific to the corresponding MDP. Upon receiving an action at produced by the agent, the environment computes reward rt, steps forward, and computes the next state st+1. If the episode has terminated, it sets termination flag dt to 1, which otherwise defaults to 0. Together, the next state st+1, actionat, reward rt, and termination flag dt, are concatenated to form the input to the policy1, which, conditioned on the hidden state ht+1, generates the next hidden state ht+2 and action at+1. At the end of an episode, the hidden state of the policy is preserved to the next episode, but not preserved between trials.The objective under this formulation is to maximize the expected total discounted reward accumulated during a single trial rather than a single episode. Maximizing this objective is equivalent to minimizing the cumulative pseudo-regret (Bubeck & Cesa-Bianchi, 2012). Since the underlying MDP changes across trials, as long as different strategies are required for different MDPs, the agent must act differently according to its belief over which MDP it is currently in. Hence, the agent is forced to integrate all the information it has received, including past actions, rewards, and termination flags, and adapt its strategy continually. Hence, we have set up an end-to-end optimization process, where the agent is encouraged to learn a “fast” reinforcement learning algorithm.For clarity of exposition, we have defined the “inner” problem (of which the agent sees n each trials) to be an MDP rather than a POMDP. However, the method can also be applied in the partiallyobserved setting without any conceptual changes. In the partially observed setting, the agent is faced with a sequence of POMDPs, and it receives an observation ot instead of state st at time t. The visual navigation experiment in Section 3.3, is actually an instance of the this POMDP setting.We represent the policy as a general recurrent neural network. Each timestep, it receives the tuple (s, a, r, d) as input, which is embedded using a function φ(s, a, r, d) and provided as input to an RNN. To alleviate the difficulty of training RNNs due to vanishing and exploding gradients (Bengio et al., 1994), we use Gated Recurrent Units (GRUs) (Cho et al., 2014) which have been demonstrated to have good empirical performance (Chung et al., 2014; Józefowicz et al., 2015). The output of the GRU is fed to a fully connected layer followed by a softmax function, which forms the distribution over actions.We have also experimented with alternative architectures which explicitly reset part of the hidden state each episode of the sampled MDP, but we did not find any improvement over the simple architecture described above.After formulating the task as a reinforcement learning problem, we can readily use standard off-theshelf RL algorithms to optimize the policy. We use a first-order implementation of Trust Region Policy Optimization (TRPO) (Schulman et al., 2015), because of its excellent empirical performance, and because it does not require excessive hyperparameter tuning. For more details, we refer the reader to the original paper. To reduce variance in the stochastic gradient estimation, we use a baseline which is also represented as an RNN using GRUs as building blocks. We optionally apply Generalized Advantage Estimation (GAE) (Schulman et al., 2016) to further reduce the variance.We designed experiments to answer the following questions:• Can RL2 learn algorithms that achieve good performance on MDP classes with special structure, relative to existing algorithms tailored to this structure that have been proposed in the literature?• Can RL2 scale to high-dimensional tasks?For the first question, we evaluate RL2 on two sets of tasks, multi-armed bandits (MAB) and tabular MDPs. These problems have been studied extensively in the reinforcement learning literature, and this body of work includes algorithms with guarantees of asymptotic optimality. We demonstrate that our approach achieves comparable performance to these theoretically justified algorithms.1To make sure that the inputs have a consistent dimension, we use placeholder values for the initial input to the policy.For the second question, we evaluate RL2 on a vision-based navigation task. Our experiments show that the learned policy makes effective use of the learned visual information and also short-term information acquired from previous episodes.Multi-armed bandit problems are a subset of MDPs where the agent’s environment is stateless. Specifically, there are k arms (actions), and at every time step, the agent pulls one of the arms, say i, and receives a reward drawn from an unknown distribution: our experiments take each arm to be a Bernoulli distribution with parameter pi. The goal is to maximize the total reward obtained over a fixed number of time steps. The key challenge is balancing exploration and exploitation— “exploring” each arm enough times to estimate its distribution (pi), but eventually switching over to “exploitation” of the best arm. Despite the simplicity of multi-arm bandit problems, their study has led to a rich theory and a collection of algorithms with optimality guarantees.Using RL2, we can train an RNN policy to solve bandit problems by training it on a given distribution ρM. If the learning is successful, the resulting policy should be able to perform competitively with the theoretically optimal algorithms. We randomly generated bandit problems by sampling each parameter pi from the uniform distribution on [0, 1]. After training the RNN policy with RL2, we compared it against the following strategies:• Random: this is a baseline strategy, where the agent pulls a random arm each time.• Gittins index (Gittins, 1979): this method gives the Bayes optimal solution in the discounted infinite-horizon case, by computing an index separately for each arm, and taking the arm with the largest index. While this work shows it is sufficient to independently compute an index for each arm (hence avoiding combinatorial explosion with the number of arms), it doesn’t show how to tractably compute these individual indices exactly. We follow the practical approximations described in Gittins et al. (2011), Chakravorty & Mahajan (2013), and Whittle (1982), and choose the best-performing approximation for each setup.• UCB1 (Auer, 2002): this method estimates an upper-confidence bound, and pulls the arm with the largest value of ucbi(t) = µ̂i(t−1)+c √ 2 log t Ti(t−1) , where µ̂i(t−1) is the estimatedmean parameter for the ith arm, Ti(t−1) is the number of times the ith arm has been pulled, and c is a tunable hyperparameter (Audibert & Munos, 2011). We initialize the statistics with exactly one success and one failure, which corresponds to a Beta(1, 1) prior.• Thompson sampling (TS) (Thompson, 1933): this is a simple method which, at each time step, samples a list of arm means from the posterior distribution, and choose the best arm according to this sample. It has been demonstrated to compare favorably to UCB1 empirically (Chapelle & Li, 2011). We also experiment with an optimistic variant (OTS) (May et al., 2012), which samples N times from the posterior, and takes the one with the highest probability.• -Greedy: in this strategy, the agent chooses the arm with the best empirical mean with probability 1 − , and chooses a random arm with probability . We use the same initialization as UCB1.• Greedy: this is a special case of -Greedy with = 0.The Bayesian methods, Gittins index and Thompson sampling, take advantage of the distribution ρM; and we provide these methods with the true distribution. For each method with hyperparameters, we maximize the score with a separate grid search for each of the experimental settings. The hyperparameters used for TRPO are shown in the appendix.The results are summarized in Table 1. Learning curves for various settings are shown in Figure 2. We observe that our approach achieves performance that is almost as good as the the reference methods, which were (human) designed specifically to perform well on multi-armed bandit problems. It is worth noting that the published algorithms are mostly designed to minimize asymptotic regret (rather than finite horizon regret), hence there tends to be a little bit of room to outperform them in the finite horizon settings.We observe that there is a noticeable gap between Gittins index and RL2 in the most challenging scenario, with 50 arms and 500 episodes. This raises the question whether better architectures or better (slow) RL algorithms should be explored. To determine the bottleneck, we trained the same policy architecture using supervised learning, using the trajectories generated by the Gittins index approach as training data. We found that the learned policy, when executed in test domains, achieved the same level of performance as the Gittins index approach, suggesting that there is room for improvement by using better RL algorithms.The bandit problem provides a natural and simple setting to investigate whether the policy learns to trade off between exploration and exploitation. However, the problem itself involves no sequential decision making, and does not fully characterize the challenges in solving MDPs. Hence, we perform further experiments using randomly generated tabular MDPs, where there is a finite number of possible states and actions—small enough that the transition probability distribution can be explicitly given as a table. We compare our approach with the following methods:• Random: the agent chooses an action uniformly at random for each time step; • PSRL (Strens, 2000; Osband et al., 2013): this is a direct generalization of Thompson sam-pling to MDPs, where at the beginning of each episode, we sample an MDP from the posterior distribution, and take actions according to the optimal policy for the entire episode. Similarly, we include an optimistic variant (OPSRL), which has also been explored in Osband & Van Roy (2016). • BEB (Kolter & Ng, 2009): this is a model-based optimistic algorithm that adds an explo-ration bonus to (thus far) infrequently visited states and actions.Setup Random PSRL OPSRL UCRL2 BEB -Greedy Greedy RL2n = 10 100.1 138.1 144.1 146.6 150.2 132.8 134.8 156.2 n = 25 250.2 408.8 425.2 424.1 427.8 377.3 368.8 445.7 n = 50 499.7 904.4 930.7 918.9 917.8 823.3 769.3 936.1 n = 75 749.9 1417.1 1449.2 1427.6 1422.6 1293.9 1172.9 1428.8 n = 100 999.4 1939.5 1973.9 1942.1 1935.1 1778.2 1578.5 1913.7The distribution over MDPs is constructed with |S| = 10, |A| = 5. The rewards follow a Gaussian distribution with unit variance, and the mean parameters are sampled independently from Normal(1, 1). The transitions are sampled from a flat Dirichlet distribution. This construction matches the commonly used prior in Bayesian RL methods. We set the horizon for each episode to be T = 10, and an episode always starts on the first state.The results are summarized in Table 2, and the learning curves are shown in Figure 3. We follow the same evaluation procedure as in the bandit case. We experiment with n ∈ {10, 25, 50, 75, 100}. For fewer episodes, our approach surprisingly outperforms existing methods by a large margin. The advantage is reversed as n increases, suggesting that the reinforcement learning problem in the outer loop becomes more challenging to solve. We think that the advantage for small n comes from the need for more aggressive exploitation: since there are 140 degrees of freedom to estimate in order to characterize the MDP, and by the 10th episode, we will not have enough samples to form a good estimate of the entire dynamics. By directly optimizing the RNN in this setting, our approach should be able to cope with this shortage of samples, and decides to exploit sooner compared to the reference algorithms.The previous two tasks both only involve very low-dimensional state spaces. To evaluate the feasibility of scaling up RL2, we further experiment with a challenging vision-based task, where theagent is asked to navigate a randomly generated maze to find a randomly placed target2. The agent receives a +1 reward when it reaches the target, −0.001 when it hits the wall, and −0.04 per time step to encourage it to reach targets faster. It can interact with the maze for multiple episodes, during which the maze structure and target position are held fixed. The optimal strategy is to explore the maze efficiently during the first episode, and after locating the target, act optimally against the current maze and target based on the collected information. An illustration of the task is given in Figure 4.Visual navigation alone is a challenging task for reinforcement learning. The agent only receives very sparse rewards during training, and does not have the primitives for efficient exploration at the beginning of training. It also needs to make efficient use of memory to decide how it should explore the space, without forgetting about where it has already explored. Previously, Oh et al. (2016) have studied similar vision-based navigation tasks in Minecraft. However, they use higher-level actions for efficient navigation. Similar high-level actions in our task would each require around 5 low-level actions combined in the right way. In contrast, our RL2 agent needs to learn these higher-level actions from scratch.We use a simple training setup, where we use small mazes of size 5× 5, with 2 episodes of interaction, each with horizon up to 250. Here the size of the maze is measured by the number of grid cells along each wall in a discrete representation of the maze. During each trial, we sample 1 out of 1000 randomly generated configurations of map layout and target positions. During testing, we evaluate on 1000 separately generated configurations. In addition, we also study its extrapolation behavior along two axes, by (1) testing on large mazes of size 9× 9 (see Figure 4c) and (2) running the agent for up to 5 episodes in both small and large mazes. For the large maze, we also increase the horizon per episode by 4x due to the increased size of the maze.The results are summarized in Table 3, and the learning curves are shown in Figure 5. We observe that there is a significant reduction in trajectory lengths between the first two episodes in both the smaller and larger mazes, suggesting that the agent has learned how to use information from past episodes. It also achieves reasonable extrapolation behavior in further episodes by maintaining its performance, although there is a small drop in the rate of success in the larger mazes. We also observe that on larger mazes, the ratio of improved trajectories is lower, likely because the agent has not learned how to act optimally in the larger mazes.Still, even on the small mazes, the agent does not learn to perfectly reuse prior information. An illustration of the agent’s behavior is shown in Figure 6. The intended behavior, which occurs most frequently, as shown in 6a and 6b, is that the agent should remember the target’s location, and utilize it to act optimally in the second episode. However, occasionally the agent forgets about where the target was, and continues to explore in the second episode, as shown in 6c and 6d. We believe that better reinforcement learning techniques used as the outer-loop algorithm will improve these results in the future.The concept of using prior experience to speed up reinforcement learning algorithms has been explored in the past in various forms. Earlier studies have investigated automatic tuning of hyperparameters, such as learning rate and temperature (Ishii et al., 2002; Schweighofer & Doya, 2003), as a form of meta-learning. Wilson et al. (2007) use hierarchical Bayesian methods to maintain a posterior over possible models of dynamics, and apply optimistic Thompson sampling according to the posterior. Many works in hierarchical reinforcement learning propose to extract reusable skills from previous tasks to speed up exploration in new tasks (Singh, 1992; Perkins et al., 1999). Werefer the reader to Taylor & Stone (2009) for a more thorough survey on the multi-task and transfer learning aspects.The formulation of searching for a best-performing algorithm, whose performance is averaged over a given distribution over MDPs, have been investigated in the past in more limited forms (Maes et al., 2011; Castronovo et al., 2012). There, they propose to learn an algorithm to solve multiarmed bandits using program search, where the search space consists of simple formulas composed from hand-specified primitives, which needs to be tuned for each specific distribution over MDPs. In comparison, our approach allows for entirely end-to-end training without requiring such domain knowledge.More recently, Fu et al. (2015) propose a model-based approach on top of iLQG with unknown dynamics (Levine & Abbeel, 2014), which uses samples collected from previous tasks to build a neural network prior for the dynamics, and can perform one-shot learning on new, but related tasks thanks to reduced sample complexity. There has been a growing interest in using deep neural networks for multi-task learning and transfer learning (Parisotto et al., 2015; Rusu et al., 2015; 2016a; Devin et al., 2016; Rusu et al., 2016b).In the broader context of machine learning, there has been a lot of interest in one-shot learning for object classification (Vilalta & Drissi, 2002; Fei-Fei et al., 2006; Larochelle et al., 2008; Lake et al., 2011; Koch, 2015). Our work draws inspiration from a particular line of work (Younger et al., 2001; Santoro et al., 2016; Vinyals et al., 2016), which formulates meta-learning as an optimization problem, and can thus be optimized end-to-end via gradient descent. While these work applies to the supervised learning setting, our work applies in the more general reinforcement learning setting. Although the reinforcement learning setting is more challenging, the resulting behavior is far richer: our agent must not only learn to exploit existing information, but also learn to explore, a problem that is usually not a factor in supervised learning. Another line of work (Hochreiter et al., 2001; Younger et al., 2001; Andrychowicz et al., 2016; Li & Malik, 2016) studies meta-learning over the optimization process. There, the meta-learner makes explicit updates to a parametrized model. In comparison, we do not use a directly parametrized policy; instead, the recurrent neural network agent acts as the meta-learner and the resulting policy simultaneously.Our formulation essentially constructs a partially observable MDP (POMDP) which is solved in the outer loop, where the underlying MDP is unobserved by the agent. This reduction of an unknown MDP to a POMDP can be traced back to dual control theory (Feldbaum, 1960), where “dual” refers to the fact that one is controlling both the state and the state estimate. Feldbaum pointed out that the solution can in principle be computed with dynamic programming, but doing so is usually impractical. POMDPs with such structure have also been studied under the name “mixed observability MDPs” (Ong et al., 2010). However, the method proposed there suffers from the usual challenges of solving POMDPs in high dimensions.Apart from the various multiple-episode tasks we investigate in this work, previous literature on training RNN policies have used similar tasks that require memory to test if long-term dependency can be learned. Recent examples include the Labyrinth experiment in the A3C paper (Mnih et al., 2016), and the water maze experiment in the Recurrent DDPG paper (Heess et al., 2015a). Although these tasks can be reformulated under the RL2 framework, the key difference is that they focus on the memory aspect instead of the fast RL aspect.This paper suggests a different approach for designing better reinforcement learning algorithms: instead of acting as the designers ourselves, learn the algorithm end-to-end using standard reinforcement learning techniques. That is, the “fast” RL algorithm is a computation whose state is stored in the RNN activations, and the RNN’s weights are learned by a general-purpose “slow” reinforcement learning algorithm. Our method, RL2, has demonstrated competence comparable with theoretically optimal algorithms in small-scale settings. We have further shown its potential to scale to high-dimensional tasks.In the experiments, we have identified opportunities to improve upon RL2: the outer-loop reinforcement learning algorithm was shown to be an immediate bottleneck, and we believe that for settings with extremely long horizons, better architecture may also be required for the policy. Although wehave used generic methods and architectures for the outer-loop algorithm and the policy, doing this also ignores the underlying episodic structure. We expect algorithms and policy architectures that exploit the problem structure to significantly boost the performance.We would like to thank our colleagues at Berkeley and OpenAI for insightful discussions. This research was funded in part by ONR through a PECASE award. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship. Xi Chen was also supported by a Berkeley AI Research lab Fellowship. We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the ARC through a Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers.Common to all experiments: as mentioned in Section 2.2, we use placeholder values when necessary. For example, at t = 0 there is no previous action, reward, or termination flag. Since all of our experiments use discrete actions, we use the embedding of the action 0 as a placeholder for actions, and 0 for both the rewards and termination flags. To form the input to the GRU, we use the values for the rewards and termination flags as-is, and embed the states and actions as described separately below for each experiments. These values are then concatenated together to form the joint embedding.For the neural network architecture, We use rectified linear units throughout the experiments as the hidden activation, and we apply weight normalization without data-dependent initialization (Salimans & Kingma, 2016) to all weight matrices. The hidden-to-hidden weight matrix uses an orthogonal initialization (Saxe et al., 2013), and all other weight matrices use Xavier initialization (Glorot & Bengio, 2010). We initialize all bias vectors to 0. Unless otherwise mentioned, the policy and the baseline uses separate neural networks with the same architecture until the final layer, where the number of outputs differ.All experiments are implemented using TensorFlow (Abadi et al., 2016) and rllab (Duan et al., 2016). We use the implementations of classic algorithms provided by the TabulaRL package (Osband, 2016).A.1 MULTI-ARMED BANDITSThe parameters for TRPO are shown in Table 1. Since the environment is stateless, we use a constant embedding 0 as a placeholder in place of the states, and a one-hot embedding for the actions.A.2 TABULAR MDPSThe parameters for TRPO are shown in Table 2. We use a one-hot embedding for the states and actions separately, which are then concatenated together.A.3 VISUAL NAVIGATIONThe parameters for TRPO are shown in Table 3. For this task, we use a neural network to form the joint embedding. We rescale the images to have width 40 and height 30 with RGB channels preserved, and we recenter the RGB values to lie within range [−1, 1]. Then, this preprocessedimage is passed through 2 convolution layers, each with 16 filters of size 5 × 5 and stride 2. The action is first embedded into a 256-dimensional vector where the embedding is learned, and then concatenated with the flattened output of the final convolution layer. The joint vector is then fed to a fully connected layer with 256 hidden units.Unlike previous experiments, we let the policy and the baseline share the same neural network. We found this to improve the stability of training baselines and also the end performance of the policy, possibly due to regularization effects and better learned features imposed by weight sharing. Similar weight-sharing techniques have also been explored in Mnih et al. (2016).B.1 MULTI-ARMED BANDITSThere are 3 algorithms with hyperparameters: UCB1, Optimistic Thompson Sampling (OTS), and -Greedy. We perform a coarse grid search to find the best hyperparameter for each of them. More specifically:• UCB1: We test c ∈ {0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}. The best found parameter for each setting is given in Table 4.B.2 TABULAR MDPSThere are 4 algorithms with hyperparameters: Optimistic PSRL (OPSRL), BEB, -Greedy, UCRL2. Details are given below.• Optimistic PSRL (OPSRL): The hyperparameter is the number of posterior samples. We use up to 20 samples. The best found parameter for each setting is given in Table 7.In this section, we provide further analysis of the behavior of RL2 agent in comparison with the baseline algorithms, on the multi-armed bandit task. Certain algorithms such as UCB1 are designed not in the Bayesian context; instead they are tailored to be robust in adversarial cases. To highlight this aspect, we evaluate the algorithms on a different metric, namely the percentage of trials where the best arm is recovered. We treat the best arm chosen by the policy to be the arm that has been pulled most often, and the ground truth best arm is the arm with the highest mean parameter. In addition, we split the set of all possible bandit tasks into simpler and harder tasks, where the difficulty is measured by the -gap between the mean parameter of the best arm and the second best arm. We compare the percentage of recovering the best arm separately according to the gap, as shown in Table 11.Note that there are two columns associated with the UCB1 algorithm, where UCB1 (without “∗”) is evaluated with c = 0.2, the parameter that gives the best performance as evaluated by the average total reward, and UCB1∗ uses c = 1.0. Surprisingly, although using c = 1.0 performs the best in terms of recovering the best arm, its performance is significantly worse than using c = 0.2 when evaluated under the average total reward (369.2 ± 2.2 vs. 405.8 ± 2.2). This also explains that although RL2 does not perform the best according to this metric (which is totally expected, since it is not optimized under this metric), it achieves comparable average total reward as other bestperforming methods.
SGD with stochastic momentum has been a de facto algorithm in nonconvex optimization and deep learning. It has been widely adopted for training machine learning models in various applications. Modern techniques in computer vision (e.g.Krizhevsky et al. (2012); He et al. (2016); Cubuk et al. (2018); Gastaldi (2017)), speech recognition (e.g. Amodei et al. (2016)), natural language processing (e.g. Vaswani et al. (2017)), and reinforcement learning (e.g. Silver et al. (2017)) use SGD with stochastic momentum to train models. The advantage of SGD with stochastic momentum has been widely observed (Hoffer et al. (2017); Loshchilov & Hutter (2019); Wilson et al. (2017)). Sutskever et al. (2013) demonstrate that training deep neural nets by SGD with stochastic momentum helps achieving in faster convergence compared with the standard SGD (i.e. without momentum). The success of momentum makes it a necessary tool for designing new optimization algorithms in optimization and deep learning. For example, all the popular variants of adaptive stochastic gradient methods like Adam (Kingma & Ba (2015)) or AMSGrad (Reddi et al. (2018b)) include the use of momentum.Despite the wide use of stochastic momentum (Algorithm 1) in practice, 1 justification for the clear empirical improvements has remained elusive, as has any mathematical guidelines for actually setting the momentum parameter—it has been observed that large values (e.g. β = 0.9) work well in practice. It should be noted that Algorithm 1 is the default momentum-method in popular software1Heavy ball momentum is the default choice of momentum method in PyTorch and Tensorflow, instead of Nesterov’s momentum. See the manual pages https://pytorch.org/docs/stable/_modules/ torch/optim/sgd.html and https://www.tensorflow.org/api_docs/python/tf/ keras/optimizers/SGD.Algorithm 1: SGD with stochastic heavy ball momentum 1: Required: Step size parameter η and momentum parameter β. 2: Init: w0 ∈ Rd and m−1 = 0 ∈ Rd. 3: for t = 0 to T do 4: Given current iterate wt, obtain stochastic gradient gt := ∇f(wt; ξt). 5: Update stochastic momentum mt := βmt−1 + gt. 6: Update iterate wt+1 := wt − ηmt. 7: end forpackages such as PyTorch and Tensorflow. In this paper we provide a theoretical analysis for SGD with momentum. We identify some mild conditions that guarantees SGD with stochastic momentum will provably escape saddle points faster than the standard SGD, which provides clear evidence for the benefit of using stochastic momentum. For stochastic heavy ball momentum, a weighted average of stochastic gradients at the visited points is maintained. The new update is computed as the current update minus a step in the direction of the momentum. Our analysis shows that these updates can amplify a component in an escape direction of the saddle points.In this paper, we focus on finding a second-order stationary point for smooth non-convex optimization by SGD with stochastic heavy ball momentum. Specifically, we consider the stochastic nonconvex optimization problem, minw∈Rd f(w) := Eξ∼D[f(w; ξ)], where we overload the notation so that f(w; ξ) represents a stochastic function induced by the randomness ξ while f(w) is the expectation of the stochastic functions. An ( , )-second-order stationary point w satisfies‖∇f(w)‖ ≤ and ∇2f(w) − I. (1)Obtaining a second order guarantee has emerged as a desired goal in the nonconvex optimization community. Since finding a global minimum or even a local minimum in general nonconvex optimization can be NP hard (Anandkumar & Ge (2016); Nie (2015); Murty & Kabadi (1987); Nesterov (2000)), most of the papers in nonconvex optimization target at reaching an approximate second-order stationary point with additional assumptions like Lipschitzness in the gradients and the Hessian (e.g. Allen-Zhu & Li (2018); Carmon & Duchi (2018); Curtis et al. (2017); Daneshmand et al. (2018); Du et al. (2017); Fang et al. (2018; 2019); Ge et al. (2015); Jin et al. (2017; 2019); Kohler & Lucchi (2017); Lei et al. (2017); Lee et al. (2019); Levy (2016); Mokhtari et al. (2018); Nesterov & Polyak (2006); Reddi et al. (2018a); Staib et al. (2019); Tripuraneni et al. (2018); Xu et al. (2018)). 2 We follow these related works for the goal and aim at showing the benefit of the use of the momentum in reaching an ( , )-second-order stationary point.We introduce a required condition, akin to a model assumption made in (Daneshmand et al. (2018)), that ensures the dynamic procedure in Algorithm 2 produces updates with suitable correlation with the negative curvature directions of the function f .Definition 1. Assume, at some time t, that the Hessian Ht = ∇2f(wt) has some eigenvalue smaller than − and ‖∇f(wt)‖ ≤ . Let vt be the eigenvector corresponding to the smallest eigenvalue of ∇2f(wt). The stochastic momentum mt satisfies Correlated Negative Curvature (CNC) at t with parameter γ > 0 ifEt[〈mt, vt〉2] ≥ γ. (2)As we will show, the recursive dynamics of SGD with heavy ball momentum helps in amplifying the escape signal γ, which allows it to escape saddle points faster.Contribution: We show that, under CNC assumption and some minor constraints that upper-bound parameter β, if SGD with momentum has properties called Almost Positively Aligned with Gradient (APAG), Almost Positively Correlated with Gradient (APCG), and Gradient Alignment or Curvature Exploitation (GrACE), defined in the later section, then it takes T = O((1−β) log(1/(1−β) ) −10) iterations to return an ( , ) second order stationary point. Alternatively, one can obtain an ( , √ ) second order stationary point in T = O((1 − β) log(1/(1 − β) ) −5) iterations. Our theoretical result demonstrates that a larger momentum parameter β can help in escaping saddle points faster. As saddle points are pervasive in the loss landscape of optimization and deep learning (Dauphin et al.2We apologize that the list is far from exhaustive.(2014); Choromanska et al. (2015)), the result sheds light on explaining why SGD with momentum enables training faster in optimization and deep learning.Notation: In this paper we use Et[·] to represent conditional expectation E[·|w1, w2, . . . , wt], which is about fixing the randomness upto but not including t and notice that wt was determined at t− 1.2.1 A THOUGHT EXPERIMENT.Let us provide some high-level intuition about the benefit of stochastic momentum with respect to escaping saddle points. In an iterative update scheme, at some time t0 the parameters wt0 can enter a saddle point region, that is a place where Hessian∇2f(wt0) has a non-trivial negative eigenvalue, say λmin(∇2f(wt0)) ≤ − , and the gradient ∇f(wt0) is small in norm, say ‖∇f(wt0)‖ ≤ . The challenge here is that gradient updates may drift only very slowly away from the saddle point, and may not escape this region; see (Du et al. (2017); Lee et al. (2019)) for additional details. On the other hand, if the iterates were to move in one particular direction, namely along vt0 the direction of the smallest eigenvector of ∇2f(wt0), then a fast escape is guaranteed under certain constraints on thestep size η; see e.g. (Carmon et al. (2018)). While the negative eigenvector could be computed directly, this 2nd-order method is prohibitively expensive and hence we typically aim to rely on gradient methods. With this in mind, Daneshmand et al. (2018), who study non-momentum SGD, make an assumption akin to our CNC property described above that each stochastic gradient gt0 is strongly non-orthogonal to vt0 the direction of large negative curvature. This suffices to drive the updates out of the saddle point region.In the present paper we study stochastic momentum, and our CNC property requires that the update direction mt0 is strongly non-orthogonal to vt0 ; more precisely, Et0 [〈mt0 , vt0〉2] ≥ γ > 0. We are able to take advantage of the analysis of (Daneshmand et al. (2018)) to establish that updates begin to escape a saddle point region for similar reasons. Further, this effect is amplified in successive iterations through the momentum update when β is close to 1. Assume that at some wt0 we have mt0 which possesses significant correlation with the negative curvature direction vt0 , then on successive rounds mt0+1 is quite close to βmt0 , mt0+2 is quite close to β2mt0 , and so forth; see Figure 1 for an example. This provides an intuitive perspective on how momentum might help accelerate the escape process. Yet one might ask does this procedure provably contribute to the escape process and, if so, what is the aggregate performance improvement of the momentum? We answer the first question in the affirmative, and we answer the second question essentially by showing that momentum can help speed up saddle-point escape by a multiplicative factor of 1− β. On the negative side, we also show that β is constrained and may not be chosen arbitrarily close to 1.Let us now establish, empirically, the clear benefit of stochastic momentum on the problem of saddle-point escape. We construct two stochastic optimization tasks, and each exhibits at least one significant saddle point. The two objectives are as follows.min w f(w) :=1n n∑ i=1 (1 2 w>Hw + b>i w + ‖w‖1010 ) , (3)min w f(w) :=1n n∑ i=1 ( (a>i w) 2 − y )2 . (4)Problem (3) of these was considered by (Staib et al. (2019); Reddi et al. (2018a)) and represents a very straightforward non-convex optimization challenge, with an embedded saddle given by the matrixH := diag([1,−0.1]), and stochastic gaussian perturbations given by bi ∼ N (0, diag([0.1, 0.001])); the small variance in the second component provides lower noise in the escape direction. Here we have set n = 10. Observe that the origin is in the neighborhood of saddle points and has objective value zero. SGD and SGD with momentum are initialized at the origin in the experiment so that they have to escape saddle points before the convergence. The second objective (4) appears in the phase retrieval problem, that has real applications in physical sciences (Candés et al. (2013); Shechtman et al. (2015)). In phase retrieval3, one wants to find an unknown w∗ ∈ Rd with access to but a few samples yi = (a>i w∗)2; the design vector ai is known a priori. Here we have sampled w∗ ∼ N (0, Id/d) and ai ∼ N (0, Id) with d = 10 and n = 200. The empirical findings, displayed in Figure 2, are quite stark: for both objectives, convergence is significantly accelerated by larger choices of β. In the first objective (Figure 4a), we see each optimization trajectory entering a saddle point region, apparent from the “flat” progress, yet we observe that large-momentum trajectories escape the saddle much more quickly than those with smaller momentum. A similar affect appears in Figure 4b. To the best of our knowledge, this is the first reported empirical finding that establishes the dramatic speed up of stochastic momentum for finding an optimal solution in phase retrieval.Heavy ball method: The heavy ball method was originally proposed by Polyak (1964). It has been observed that this algorithm, even in the deterministic setting, provides no convergence speedup over standard gradient descent, except in some highly structure cases such as convex quadratic objectives where an “accelerated” rate is possible (Lessard et al. (2016); Goh (2017); Ghadimi et al. (2015); Sun et al. (2019); Loizou & Richtárik (2017; 2018); Gadat et al. (2016); Yang et al. (2018); Kidambi et al. (2018); Can et al. (2019)). We provide a comprehensive survey of the related works about heavy ball method in Appendix A.Reaching a second order stationary point: As we mentioned earlier, there are many works aim at reaching a second order stationary point. We classify them into two categories: specialized algorithms and simple GD/SGD variants. Specialized algorithms are those designed to exploit the negative curvature explicitly and escape saddle points faster than the ones without the explicit exploitation (e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al. (2018)). Simple GD/SGD variants are those with minimal tweaks of standard GD/SGD or their variants (e.g. Ge et al. (2015); Levy (2016); Fang et al. (2019); Jin et al. (2017; 2018; 2019); Daneshmand et al. (2018);3It is known that phase retrieval is nonconvex and has the so-called strict saddle property: (1) every local minimizer {w∗,−w∗} is global up to phase, (2) each saddle exhibits negative curvature (see e.g. (Sun et al. (2015; 2016); Chen et al. (2018)))Staib et al. (2019)). Our work belongs to this category. In this category, perhaps the pioneer works are (Ge et al. (2015)) and (Jin et al. (2017)). Jin et al. (2017) show that explicitly adding isotropic noise in each iteration guarantees that GD escapes saddle points and finds a second order stationary point with high probability. Following (Jin et al. (2017)), Daneshmand et al. (2018) assume that stochastic gradient inherently has a component to escape. Specifically, they make assumption of the Correlated Negative Curvature (CNC) for stochastic gradient gt so that Et[〈gt, vt〉2] ≥ γ > 0. The assumption allows the algorithm to avoid the procedure of perturbing the updates by adding isotropic noise. Our work is motivated by (Daneshmand et al. (2018)) but assumes CNC for the stochastic momentum mt instead. In Appendix A, we compare the results of our work with the related works.We assume that the gradient∇f is L-Lipschitz; that is, f is L-smooth. Further, we assume that the Hessian∇2f is ρ-Lipschitz. These two properties ensure that ‖∇f(w)−∇f(w′)‖ ≤ L‖w−w′‖ and that ‖∇2f(w)−∇2f(w′)‖ ≤ ρ‖w−w′‖, ∀w,w′. The L-Lipschitz gradient assumption implies that |f(w′)−f(w)−〈∇f(w), w′−w〉| ≤ L2 ‖w−w′‖2,∀w,w′, while the ρ-Lipschitz Hessian assumption implies that |f(w′)−f(w)−〈∇f(w), w′−w〉−(w′−w)>∇2f(w)(w′−w)| ≤ ρ6‖w−w′‖3, ∀w,w′. Furthermore, we assume that the stochastic gradient has bounded noise ‖∇f(w)−∇f(w; ξ)‖2 ≤ σ2 and that the norm of stochastic momentum is bounded so that ‖mt‖ ≤ cm. We denote ΠiMi as the matrix product of matrices {Mi} and we use σmax(M) = ‖M‖2 := supx6=0 〈x,Mx〉 〈x,x〉 to denote the spectral norm of the matrix M .Our analysis of stochastic momentum relies on three properties of the stochastic momentum dynamic. These properties are somewhat unusual, but we argue they should hold in natural settings, and later we aim to demonstrate that they hold empirically in a couple of standard problems of interest. Definition 2. We say that SGD with stochastic momentum satisfies Almost Positively Aligned with Gradient (APAG) 4 if we haveEt[〈∇f(wt),mt − gt〉] ≥ − 12 ‖∇f(wt)‖2. (5)We say that SGD with stochastic momentum satisfies Almost Positively Correlated with Gradient (APCG) with parameter τ if ∃c′ > 0 such that,Et[〈∇f(wt),Mtmt〉] ≥ −c′ησmax(Mt)‖∇f(wt)‖2, (6)where the PSD matrix Mt is defined asMt = (Π τ−1 s=1Gs,t)(Π τ−1 s=kGs,t) with Gs,t := I−η ∑s j=1 β s−j∇2f(wt) = I−η(1−β s) 1−β ∇ 2f(wt)for any integer 1 ≤ k ≤ τ − 1, and η is any step size chosen that guarantees each Gs,t is PSD. Definition 3. We say that the SGD with momentum exhibits Gradient Alignment or Curvature Exploitation (GrACE) if ∃ch ≥ 0 such thatEt[η〈∇f(wt), gt −mt〉+ η 2 2 m > t ∇2f(wt)mt] ≤ η2ch. (7)APAG requires that the momentum term mt must, in expectation, not be significantly misaligned with the gradient ∇f(wt). This is a very natural condition when one sees that the momentum term is acting as a biased estimate of the gradient of the deterministic f . APAG demands that the bias can not be too large relative to the size of ∇f(wt). Indeed this property is only needed in our analysis when the gradient is large (i.e. ‖∇f(wt)‖ ≥ ) as it guarantees that the algorithm makes progress; our analysis does not require APAG holds when gradient is small.APCG is a related property, but requires that the current momentum term mt is almost positively correlated with the the gradient∇f(wt), but measured in the Mahalanobis norm induced by Mt. It4Note that our analysis still go through if one replaces 1 2 on r.h.s. of (5) with any larger number c < 1; the resulted iteration complexity would be only a constant multiple worse.s=1 Gs,t)(Π3×105s=1 Gs,t) and we onlyreport the values when the update is in the region of saddle points. For (f), we let Mt = (Π500s=1Gs,t)(Π500s=1Gs,t) and we observe that the value is almost always nonnegative. The figures implies that SGD with momentum has APAG and APCG properties in the experiments. Furthermore, an interesting observation is that, for the phase retrieval problem, the expected values might actually be nonnegative.may appear to be an unusual object, but one can view the PSD matrix Mt as measuring something about the local curvature of the function with respect to the trajectory of the SGD with momentum dynamic. We will show that this property holds empirically on two natural problems for a reasonable constant c′. APCG is only needed in our analysis when the update is in a saddle region with significant negative curvature, ‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − . Our analysis does not require APCG holds when the gradient is large or the update is at an ( , )-second order stationary point.For GrACE, the first term on l.h.s of (7) measures the alignment between stochastic momentum mt and the gradient ∇f(wt), while the second term on l.h.s measures the curvature exploitation. The first term is small (or even negative) when the stochastic momentum mt is aligned with the gradient∇f(wt), while the second term is small (or even negative) when the stochastic momentum mt can exploit a negative curvature (i.e. the subspace of eigenvectors that corresponds to the negative eigenvalues of the Hessian∇2f(wt) if exists). Overall, a small sum of the two terms (and, consequently, a small ch) allows one to bound the function value of the next iterate (see Lemma 8).On Figure 3, we report some quantities related to APAG and APCG as well as the gradient norm when solving the previously discussed problems (3) and (4) using SGD with momentum. We also report a quantity regarding GrACE on Figure 4 in the appendix.The high level idea of our analysis follows as a similar template to (Jin et al. (2017); Daneshmand et al. (2018); Staib et al. (2019)). Our proof is structured into three cases: either (a) ‖∇f(w)‖ ≥ , or (b) ‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − , or otherwise (c) ‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≥ − ,Algorithm 2: SGD with stochastic heavy ball momentum 1: Required: Step size parameters r and η, momentum parameter β, and period parameter Tthred. 2: Init: w0 ∈ Rd and m−1 = 0 ∈ Rd. 3: for t = 0 to T do 4: Get stochastic gradient gt at wt, and set stochastic momentum mt := βmt−1 + gt. 5: Set learning rate: η̂ := η unless (t mod Tthred) = 0 in which case η̂ := r 6: wt+1 = wt − η̂mt. 7: end formeaning we have arrived in a second-order stationary region. The precise algorithm we analyze is Algorithm 2, which identical to Algorithm 1 except that we boost the step size to a larger value r on occasion. We will show that the algorithm makes progress in cases (a) and (b). In case (c), when the goal has already been met, further execution of the algorithm only weakly hurts progress. Ultimately, we prove that a second order stationary point is arrived at with high probability. While our proof borrows tools from (Daneshmand et al. (2018); Staib et al. (2019)), much of the momentum analysis is entirely novel to our knowledge. Theorem 1. Assume that the stochastic momentum satisfies CNC. Set 5 r = O( 2), η = O( 5), and Tthred = c(1−β)η log( Lcmσ 2ρc′ch (1−β)δγ ) = O((1 − β) log( Lcmσ 2ρc′ch (1−β)δγ )−6) for some constant c > 0. If SGD with momentum (Algorithm 2) has APAG property when gradient is large (‖∇f(w)‖ ≥ ), APCGTthred property when it enters a region of saddle points that exhibits a negative curvature (‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − ), and GrACE property throughout the iterations, then it reaches an ( , ) second order stationary point in T = 2Tthred(f(w0)−minw f(w))/(δFthred) = O((1− β) log(Lcmσ2ρc′ch (1−β)δγ ) −10) iterations with high probability 1− δ, where Fthred = O( 4).The theorem implies the advantage of using stochastic momentum for SGD. Higher β leads to reaching a second order stationary point faster. As we will show in the following, this is due to that higher β enables escaping the saddle points faster. In Subsection 3.2.1, we provide some key details of the proof of Theorem 1. The interested reader can read a high-level sketch of the proof, as well as the detailed version, in Appendix G.Remark 1: (constraints on β) We also need some minor constraints on β so that β cannot be too close to 1. They are 1) L(1 − β)3 > 1, 2) σ2(1 − β)3 > 1, 3) c′(1 − β)2 > 1, 4) η ≤ 1−βL , 5) η ≤ 1−β , and 6) Tthred ≥ 1 + 2β 1−β . Please see Appendix E.1 for the details and discussions.Remark 2: (escaping saddle points) Note that Algorithm 2 reduces to CNC-SGD of Daneshmand et al. (2018) when β = 0 (i.e. without momentum). Therefore, let us compare the results. We show that the escape time of Algorithm 2 is Tthred := Õ ( (1−β) η ) (see Appendix E.3.3, especially (81-82)).On the other hand, for CNC-SGD, based on Table 3 in their paper, is Tthred = Õ ( 1 η ) . One can clearly see that Tthred of our result has a dependency 1 − β, which makes it smaller than that of Daneshmand et al. (2018) for any same η and consequently demonstrates escaping saddle point faster with momentum.Remark 3: (finding a second order stationary point) Denote ` a number such that ∀t, ‖gt‖ ≤ `. In Appendix G.3, we show that in the high momentum regime where (1− β) << ρ 2`10c9mc 2 hc ′ , Algorithm 2is strictly better than CNC-SGD of Daneshmand et al. (2018), which means that a higher momentum can help find a second order stationary point faster. Empirically, we find out that c′ ≈ 0 (Figure 3) and ch ≈ 0 (Figure 4) in the phase retrieval problem, so the condition is easily satisfied for a wide range of β.In this subsection, we analyze the process of escaping saddle points by SGD with momentum. Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that it enters the region exhibiting a small5See Table 3 in Appendix E for the precise expressions of the parameters. Here, we hide the parameters’ dependencies on γ, L, cm, c′, σ2, ρ, ch, and δ. W.l.o.g, we also assume that cm, L, σ2, c′, ch, and ρ are not less than one and ≤ 1.gradient but a large negative eigenvalue of the Hessian (i.e. ‖∇f(wt0)‖ ≤ and λmin(∇2f(wt0)) ≤ − ). We want to show that it takes at most Tthred iterations to escape the region and whenever it escapes, the function value decreases at least by Fthred = O( 4) on expectation, where the precise expression of Fthred will be determined later in Appendix E. The technique that we use is proving by contradiction. Assume that the function value on expectation does not decrease at least Fthred in Tthred iterations. Then, we get an upper bound of the expected distance Et0 [‖wt0+Tthred − wt0‖2] ≤ Cupper. Yet, by leveraging the negative curvature, we also show a lower bound of the form Et0 [‖wt0+Tthred − wt0‖2] ≥ Clower. The analysis will show that the lower bound is larger than the upper bound (namely, Clower > Cupper), which leads to the contradiction and concludes that the function value must decrease at least Fthred in Tthred iterations on expectation. Since Tthred = O((1− β) log( 1(1−β) )6), the dependency on β suggests that larger β can leads to smaller Tthred, which implies that larger momentum helps in escaping saddle points faster. Lemma 1 below provides an upper bound of the expected distance. The proof is in Appendix C.Lemma 1. Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that Et0 [f(wt0) − f(wt0+t)] ≤ Fthred for any 0 ≤ t ≤ Tthred. Then, Et0 [‖wt0+t − wt0‖2] ≤ Cupper,t := 8ηt ( Fthred+2r2ch+ ρ3 r 3c3m ) (1−β)2 + 8η 2 tσ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m.We see thatCupper,t in Lemma 1 is monotone increasing with t, so we can defineCupper := Cupper,Tthred . Now let us switch to obtaining the lower bound of Et0 [‖wt0+Tthred − wt0‖2]. The key to get the lower bound comes from the recursive dynamics of SGD with momentum.Lemma 2. Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic approximation at wt0 , Q(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉 + 12 (w − wt0)>H(w − wt0), where H := ∇2f(wt0). Also, define Gs := (I − η ∑s k=1 βs−kH). Then we can write wt0+t −wt0 exactly using the following decomposition.qv,t−1︷ ︸︸ ︷( Πt−1j=1Gj )( − rmt0 ) +η qm,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) βsmt0+ η qq,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) )+ η qw,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k∇f(wt0) +ηqξ,t−1︷ ︸︸ ︷ (−1)t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−kξt0+k .The proof of Lemma 2 is in Appendix D. Furthermore, we will use the quantities qv,t−1, qm,t−1, qq,t−1, qw,t−1, qξ,t−1 as defined above throughout the analysis.Lemma 3. Following the notations of Lemma 2, we have thatEt0 [‖wt0+t−wt0‖2] ≥ Et0 [‖qv,t−1‖2]+2ηEt0 [〈qv,t−1, qm,t−1+qq,t−1+qw,t−1+qξ,t−1〉] =: Clower.We are going to show that the dominant term in the lower bound of Et0 [‖wt0+t − wt0‖2] is Et0 [‖qv,t−1‖2], which is the critical component for ensuring that the lower bound is larger than the upper bound of the expected distance.Lemma 4. Denote θj := ∑j k=1 β j−k = ∑j k=1 βk−1 and λ := −λmin(H). Following the conditions and notations in Lemma 1 and Lemma 2, we have thatEt0 [‖qv,t−1‖2] ≥ ( Πt−1j=1(1 + ηθjλ) )2 r2γ. (8)Proof. We know that λmin(H) ≤ − < 0. Let v be the eigenvector of the Hessian H with unit norm that corresponds to λmin(H) so that Hv = λmin(H)v. We have (I − ηH)v = v − ηλmin(H)v =(1− ηλmin(H))v. Then,Et0 [‖qv,t−1‖2] (a) = Et0 [‖qv,t−1‖2‖v‖2](b) ≥ Et0 [〈qv,t−1, v〉2] (c) = Et0 [〈 ( Πt−1j=1Gj ) rmt0 , v〉2](d) = Et0 [〈 ( Πt−1j=1(I − ηθjH) ) rmt0 , v〉2] = Et0〈 ( Πt−1j=1(1− ηθjλmin(H)) ) rmt0 , v〉2](e) ≥ ( Πt−1j=1(1 + ηθjλ) )2 r2γ,(9)where (a) is because v is with unit norm, (b) is by Cauchy–Schwarz inequality, (c), (d) are by the definitions, and (e) is by the CNC assumption so that Et0 [〈mt0 , v〉2] ≥ γ.Observe that the lower bound in (8) is monotone increasing with t and the momentum parameter β. Moreover, it actually grows exponentially in t. To get the contradiction, we have to show that the lower bound is larger than the upper bound. By Lemma 1 and Lemma 3, it suffices to prove the following lemma. We provide its proof in Appendix E.Lemma 5. Let Fthred = O( 4) and η2Tthred ≤ r2. By following the conditions and notations in Theorem 1, Lemma 1 and Lemma 2, we conclude that if SGD with momentum (Algorithm 2) has the APCG property, then we have that Clower := Et0 [‖qv,Tthred−1‖2]+2ηEt0 [〈qv,Tthred−1, qm,Tthred−1 + qq,Tthred−1 + qw,Tthred−1 + qξ,Tthred−1〉] > Cupper.In this paper, we identify three properties that guarantee SGD with momentum in reaching a secondorder stationary point faster by a higher momentum, which justifies the practice of using a large value of momentum parameter β. We show that a greater momentum leads to escaping strict saddle points faster due to that SGD with momentum recursively enlarges the projection to an escape direction. However, how to make sure that SGD with momentum has the three properties is not very clear. It would be interesting to identify conditions that guarantee SGD with momentum to have the properties. Perhaps a good starting point is understanding why the properties hold in phase retrieval. We believe that our results shed light on understanding the recent success of SGD with momentum in non-convex optimization and deep learning.We gratefully acknowledge financial support from NSF IIS awards 1910077 and 1453304.Heavy ball method: The heavy ball method was originally proposed by Polyak (1964). It has been observed that this algorithm, even in the deterministic setting, provides no convergence speedup over standard gradient descent, except in some highly structure cases such as convex quadratic objectives where an “accelerated” rate is possible (Lessard et al. (2016); Goh (2017)). In recent years, some works make some efforts in analyzing heavy ball method for other classes of optimization problems besides the quadratic functions. For example, Ghadimi et al. (2015) prove an O(1/T ) ergodic convergence rate when the problem is smooth convex, while Sun et al. (2019) provide a non-ergodic convergence rate for certain classes of convex problems. Ochs et al. (2014) combine the technique of forward-backward splitting with heavy ball method for a specific class of nonconvex optimization problem. For stochastic heavy ball method, Loizou & Richtárik (2017) analyze a class of linear regression problems and shows a linear convergence rate of stochastic momentum, in which the linear regression problems actually belongs to the case of strongly convex quadratic functions. Other works includes (Gadat et al. (2016)), which shows almost sure convergence to the critical points by stochastic heavy ball for general non-convex coercive functions. Yet, the result does not show any advantage of stochastic heavy ball over other optimization algorithms like SGD. Can et al. (2019) show an accelerated linear convergence to a stationary distribution under Wasserstein distance for strongly convex quadratic functions by SGD with stochastic heavy ball momentum. Yang et al. (2018) provide a unified analysis of stochastic heavy ball momentum and Nesterov’s momentum for smooth non-convex objective functions. They show that the expected gradient norm converges at rate O(1/ √ t). Yet, the rate is not better than that of the standard SGD. We are also aware of the works (Ghadimi & Lan (2016; 2013)), which propose some variants of stochastic accelerated algorithms with first order stationary point guarantees. Yet, the framework in (Ghadimi & Lan (2016; 2013)) does not capture the stochastic heavy ball momentum used in practice. There is also a negative result about the heavy ball momentum. Kidambi et al. (2018) show that for a specific strongly convex andstrongly smooth problem, SGD with heavy ball momentum fails to achieving the best convergence rate while some algorithms can.Reaching a second order stationary point: As we mentioned earlier, there are many works aim at reaching a second order stationary point. We classify them into two categories: specialized algorithms and simple GD/SGD variants. Specialized algorithms are those designed to exploit the negative curvature explicitly and escape saddle points faster than the ones without the explicit exploitation (e.g. Carmon et al. (2018); Agarwal et al. (2017); Allen-Zhu & Li (2018); Xu et al. (2018)). Simple GD/SGD variants are those with minimal tweaks of standard GD/SGD or their variants (e.g. Ge et al. (2015); Levy (2016); Fang et al. (2019); Jin et al. (2017; 2018; 2019); Daneshmand et al. (2018); Staib et al. (2019)). Our work belongs to this category. In this category, perhaps the pioneer works are (Ge et al. (2015)) and (Jin et al. (2017)). Jin et al. (2017) show that explicitly adding isotropic noise in each iteration guarantees that GD escapes saddle points and finds a second order stationary point with high probability. Following (Jin et al. (2017)), Daneshmand et al. (2018) assume that stochastic gradient inherently has a component to escape. Specifically, they make assumption of the Correlated Negative Curvature (CNC) for stochastic gradient gt so that Et[〈gt, vt〉2] ≥ γ > 0. The assumption allows the algorithm to avoid the procedure of perturbing the updates by adding isotropic noise. Our work is motivated by (Daneshmand et al. (2018)) but assumes CNC for the stochastic momentum mt instead. Very recently, Jin et al. (2019) consider perturbing the update of SGD and provide a second order guarantee. Staib et al. (2019) consider a variant of RMSProp (Tieleman & Hinton (2012)), in which the gradient gt is multiplied by a preconditioning matrix Gt and the update is wt+1 = wt − G−1/2t gt. The work shows that the algorithm can help in escaping saddle points faster compared to the standard SGD under certain conditions. Fang et al. (2019) propose average-SGD, in which a suffix averaging scheme is conducted for the updates. They also assume an inherent property of stochastic gradients that allows SGD to escape saddle points.We summarize the iteration complexity results of the related works for simple SGD variants on Table 1. 6 The readers can see that the iteration complexity of (Fang et al. (2019)) and (Jin et al. (2019)) are better than (Daneshmand et al. (2018); Staib et al. (2019)) and our result. So, we want to explain the results and clarify the differences. First, we focus on explaining why the popular algorithm, SGD with heavy ball momentum, works well in practice, which is without the suffix averaging scheme used in (Fang et al. (2019)) and is without the explicit perturbation used in (Jin et al. (2019)). Specifically, we focus on studying the effect of stochastic heavy ball momentum and showing the advantage of using it. Furthermore, our analysis framework is built on the work of (Daneshmand et al. (2018)). We believe that, based on the insight in our work, one can also show the advantage of stochastic momentum by modifying the assumptions and algorithms in (Fang et al. (2019)) or (Jin et al. (2019)) and consequently get a better dependency on .6We follow the work (Daneshmand et al. (2018)) for reaching an ( , )-stationary point, while some works are for an ( , √ )-stationary point. We translate them into the complexity of getting an ( , )-stationary point.In the following, Lemma 7 says that under the APAG property, when the gradient norm is large, on expectation SGD with momentum decreases the function value by a constant and consequently makes progress. On the other hand, Lemma 8 upper-bounds the increase of function value of the next iterate (if happens) by leveraging the GrACE property. Lemma 6. If SGD with momentum has the APAG property, then, considering the update step wt+1 = wt − ηmt, we have that Et[f(wt+1)] ≤ f(wt)− η2‖∇f(wt)‖ 2 + Lη2c2m 2 .Proof. By the L-smoothness assumption,f(wt+1) ≤ f(wt)− η〈∇f(wt),mt〉+ Lη22 ‖mt‖2≤f(wt)− η〈∇f(wt), gt〉 − η〈∇f(wt),mt − gt〉+ Lη2c2m2 . (10)Taking the expectation on both sides. We haveEt[f(wt+1)] ≤ f(wt)− η‖∇f(wt)‖2 − ηEt[〈∇f(wt),mt − gt〉] + Lη2c2m2≤ f(wt)− η2 ‖∇f(wt)‖2 + Lη2c2m 2 . (11)where we use the APAG property in the last inequality.Lemma 7. Assume that the step size η satisfies η ≤ 28Lc2m . If SGD with momentum has theAPAG property, then, considering the update step wt+1 = wt − ηmt, we have that Et[f(wt+1)] ≤ f(wt)− η4 2 when ‖∇f(wt)‖ ≥ .Proof. Et[f(wt+1)−f(wt)] Lemma 6 ≤ −η2‖∇f(wt)‖ 2+ Lη2c2m 2 ‖∇f(wt)‖≥ ≤ −η2 2+ Lη2c2m 2 ≤ − η 42, where the last inequality is due to the constraint of η.Lemma 8. If SGD with momentum has the GrACE property, then, considering the update step wt+1 = wt − ηmt, we have that Et[f(wt+1)] ≤ f(wt) + η2ch + ρη 3 6 c 3 m.Proof. Consider the update rule wt+1 = wt − ηmt, where mt represents the stochastic momentum and η is the step size. By ρ-Lipschitzness of Hessian, we have f(wt+1) ≤ f(wt)− η〈∇f(wt), gt〉+ η〈∇f(wt), gt−mt〉+ η 2 2 m > t ∇2f(wt)mt+ ρη3 6 ‖mt‖ 3. Taking the conditional expectation, one hasEt[f(wt+1)] ≤ f(wt)− Et[η‖∇f(wt)‖2] + Et[η〈∇f(wt), gt −mt〉+ η22 m>t ∇2f(wt)mt] +ρη36 c3m.≤ f(wt) + 0 + η2ch + ρη36 c3m.(12)Lemma 1 Denote t0 any time such that (t0 mod Tthred) = 0. Suppose that Et0 [f(wt0) − f(wt0+t)] ≤ Fthred for any 0 ≤ t ≤ Tthred. Then,Et0 [‖wt0+t − wt0‖2] ≤ Cupper,t := 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8η2tσ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(13)Proof. Recall that the update is wt0+1 = wt0 − rmt0 , and wt0+t = wt0+t−1 − ηmt0+t−1, for t > 1. We have that‖wt0+t − wt0‖2 ≤ 2(‖wt0+t − wt0+1‖2 + ‖wt0+1 − wt0‖2) ≤ 2‖wt0+t − wt0+1‖2 + 2r2c2m, (14) where the first inequality is by the triangle inequality and the second one is due to the assumption that ‖mt‖ ≤ cm for any t. Now let us denote• αs := ∑t−1−s j=0 β j• At−1 := ∑t−1 s=1 αsand let us rewrite gt = ∇f(wt) + ξt, where ξt is the zero-mean noise. We have thatEt0 [‖wt0+t − wt0+1‖2] = Et0 [‖ t−1∑ s=1 −ηmt0+s‖2] = Et0 [η2‖ t−1∑ s=1 ( ( s∑ j=1 βs−jgt0+j) + β smt0 ) ‖2]≤ Et0 [2η2‖ t−1∑ s=1 s∑ j=1 βs−jgt0+j‖2 + 2η2‖ t−1∑ s=1 βsmt0‖2] ≤ Et0 [2η2‖ t−1∑ s=1 s∑ j=1 βs−jgt0+j‖2] + 2η2 ( β 1− β )2 c2m = Et0 [2η2‖ t−1∑ s=1 αsgt0+s‖2] + 2η2 ( β 1− β )2 c2m= Et0 [2η2‖ t−1∑ s=1 αs ( ∇f(wt0+s) + ξt0+s ) ‖2] + 2η2 ( β 1− β )2 c2m≤ Et0 [4η2‖ t−1∑ s=1 αs∇f(wt0+s)‖2] + Et0 [4η2‖ t−1∑ s=1 αsξt0+s‖2] + 2η2 ( β 1− β )2 c2m.(15)To proceed, we need to upper bound Et0 [4η2‖ ∑t−1 s=1 αs∇f(wt0+s)‖2]. We have that Et0 [4η2‖ t−1∑ s=1 αs∇f(wt0+s)‖2] (a) ≤ Et0 [4η2A2t−1 t−1∑ s=1 αs At−1 ‖∇f(wt0+s)‖2](b) ≤ Et0 [4η2 At−1 1− β t−1∑ s=1 ‖∇f(wt0+s)‖2] (c) ≤ Et0 [4η2 t (1− β)2 t−1∑ s=1 ‖∇f(wt0+s)‖2].(16)where (a) is by Jensen’s inequality, (b) is by maxs αs ≤ 11−β , and (c) is by At−1 ≤ t 1−β . Now let us switch to bound the other term.Et0 [4η2‖ t−1∑ s=1 αsξt0+s‖2] = 4η2 ( Et0 [ t−1∑ i6=j αiαjξ > t0+iξt0+j ] + Et0 [ t−1∑ s=1 α2sξ > t0+sξt0+s] ) (a) = 4η2 ( 0 + Et0 [t−1∑ s=1 α2sξ > t0+sξt0+s] ) ,(b) ≤ 4η2 tσ 2(1− β)2 . (17)where (a) is because Et0 [ξ>t0+iξt0+j ] = 0 for i 6= j, (b) is by that ‖ξt‖ 2 ≤ σ2 and maxt αt ≤ 11−β . Combining (14), (15), (16), (17),Et0 [‖wt0+t − wt0‖2] ≤ Et0 [8η2 t (1− β)2 t−1∑ s=1 ‖∇f(wt0+s)‖2] + 8η2 tσ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(18) Now we need to bound Et0 [ ∑t−1 s=1 ‖∇f(wt0+s)‖2]. By using ρ-Lipschitzness of Hessian, we have thatf(wt0+s) ≤ f(wt0+s−1)− η〈∇f(wt0+s−1),mt0+s−1〉+ 12 η2m>t0+s−1∇2f(wt0+s−1)mt0+s−1 + ρ6 η3‖mt0+s−1‖3.(19)By adding η〈∇f(wt0+s−1), gt0+s−1〉 on both sides, we haveη〈∇f(wt0+s−1), gt0+s−1〉 ≤ f(wt0+s−1)− f(wt0+s) + η〈∇f(wt0+s−1), gt0+s−1 −mt0+s−1〉+ 12 η2m>t0+s−1∇2f(wt0+s−1)mt0+s−1 + ρ6 η3‖mt0+s−1‖3.(20)Taking conditional expectation on both sides leads toEt0+s−1[η‖∇f(wt0+s−1)‖2] ≤ Et0+s−1[f(wt0+s−1)− f(wt0+s)] + η2ch + ρ6 η3c3m, (21)where Et0+s−1[η〈∇f(wt0+s−1), gt0+s−1 −mt0+s−1〉 + 12η 2m>t0+s−1∇ 2f(wt0+s−1)mt0+s−1] ≤ η2ch by the GrACE property. We have that for t0 ≤ t0 + s− 1Et0 [η‖∇f(wt0+s−1)‖2] = Et0 [Et0+s−1[η‖∇f(wt0+s−1)‖2]] (21)≤ Et0 [Et0+s−1[f(wt0+s−1)− f(wt0+s)]] + η2ch + ρ6 η3c3m= Et0 [f(wt0+s−1)− f(wt0+s)] + η2ch + ρ6 η3c3m. (22)Summing the above inequality from s = 2, 3, . . . , t leads to Et0 [ t−1∑ s=1 η‖∇f(wt0+s)‖2] ≤ Et0 [f(wt0+1)− f(wt0+t)] + η2(t− 1)ch + ρ 6 η3(t− 1)c3m= Et0 [f(wt0+1)− f(wt0) + f(wt0)− f(wt0+t)] + η2(t− 1)ch + ρ6 η3(t− 1)c3m(a) ≤ Et0 [f(wt0+1)− f(wt0)] + Fthred + η2(t− 1)ch + ρ6 η3(t− 1)c3m,(23)where (a) is by the assumption (made for proving by contradiction) that Et0 [f(wt0)− f(wt0+s)] ≤ Fthred for any 0 ≤ s ≤ Tthred. By (21) with s = 1 and η = r, we haveEt0 [r‖∇f(wt0)‖2] ≤ Et0 [f(wt0)− f(wt0+1)] + r2ch + ρ6 r3c3m. (24)By (23) and (24), we know thatEt0 [ t−1∑ s=1 η‖∇f(wt0+s)‖2] ≤ Et0 [r‖f(wt0)‖2] + Et0 [ t−1∑ s=1 η‖∇f(wt0+s)‖2]≤ Fthred + r2ch + ρ6 r3c3m + η2tch + ρ6 η3tc3m(a) ≤ Fthred + 2r2ch + ρ6 r3c3m +ρ 6 r2ηc3m.(b) ≤ Fthred + 2r2ch + ρ3 r3c3m, (25)where (a) is by the constraint that η2t ≤ r2 for 0 ≤ t ≤ Tthred and (b) is by the constraint that r ≥ η. By combining (25) and (18)Et0 [‖wt0+t − wt0‖2] ≤ Et0 [8η2 t (1− β)2 t−1∑ s=1 ‖∇f(wt0+s)‖2] + 8η2 tσ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m≤ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8η2tσ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(26)Lemma 2 Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic approximation at wt0 , Q(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉 + 12 (w − wt0)>H(w − wt0), where H := ∇2f(wt0). Also, define Gs := (I − η ∑s k=1 β s−kH) and• qv,t−1 := ( Πt−1j=1Gj )( − rmt0 ) . • qm,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj ) βsmt0 . • qq,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj )∑s k=1 βs−k(∇f(wt0+k)−∇Q(wt0+s)). • qw,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj )∑s k=1 β s−k∇f(wt0). • qξ,t−1 := − ∑t−1 s=1 ( Πt−1j=s+1Gj )∑s k=1 β s−kξt0+k.Then, wt0+t − wt0 = qv,t−1 + ηqm,t−1 + ηqq,t−1 + ηqw,t−1 + ηqξ,t−1.Notations: Denote t0 any time such that (t0 mod Tthred) = 0. Let us define a quadratic approximation at wt0 ,Q(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉+ 12 (w − wt0)>H(w − wt0), (27)where H := ∇2f(wt0). Also, we denoteGs := (I − η s∑k=1βs−kH)vm,s := β smt0vq,s := s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) vw,s :=s∑ k=1 βs−k∇f(wt0)vξ,s := s∑ k=1 βs−kξt0+kθs := s∑ k=1 βs−k.(28)Proof. First, we rewrite mt0+j for any j ≥ 1 as follows.mt0+j = β jmt0 + j∑ k=1 βj−kgt0+k= βjmt0 + j∑ k=1 βj−k ( ∇f(wt0+k) + ξt0+k ) .(29)We have thatwt0+t − wt0 = wt0+t−1 − wt0 − ηmt0+t−1(a) = wt0+t−1 − wt0 − η ( βt−1mt0 + t−1∑ k=1 βt−1−k ( ∇f(wt0+k) + ξt0+k )) (b) = wt0+t−1 − wt0 − ηt−1∑ k=1 βt−1−k∇Q(wt0+t−1)− η ( βt−1mt0 + t−1∑ k=1 βt−1−k ( ∇f(wt0+k)−∇Q(wt0+t−1) + ξt0+k )) (c) = wt0+t−1 − wt0 − ηt−1∑ k=1 βt−1−k ( H(wt0+t−1 − wt0) +∇f(wt0) ) − η ( βt−1mt0 +t−1∑ k=1 βt−1−k ( ∇f(wt0+k)−∇Q(wt0+t−1) + ξt0+k )) = (I − ηt−1∑ k=1 βt−1−kH) ( wt0+t−1 − wt0 ) − η ( βt−1mt0 +t−1∑ k=1 βt−1−k ( ∇f(wt0+k)−∇Q(wt0+t−1) +∇f(wt0) + ξt0+k )) ,(30) where (a) is by using (29) with j = t− 1, (b) is by subtracting and adding back the same term, and(c) is by ∇Q(wt0+t−1) = ∇f(wt0) +H(wt0+t−1 − wt0). To continue, by using the nations in (28), we can rewrite (30) aswt0+t − wt0 = Gt−1 ( wt0+t−1 − wt0 ) − η ( vm,t−1 + vq,t−1 + vw,t−1 + vξ,t−1 ) . (31)Recursively expanding (31) leads to wt0+t − wt0 = Gt−1 ( wt0+t−1 − wt0 ) − η ( vm,t−1 + vq,t−1 + vw,t−1 + vξ,t−1 ) = Gt−1 ( Gt−2 ( wt0+t−2 − wt0 ) − η ( vm,t−2 + vq,t−2 + vw,t−2 + vξ,t−2)) − η ( vm,t−1 + vq,t−1 + vw,t−1 + vξ,t−1) (a) = ( Πt−1j=1Gj )( wt0+1 − wt0)− ηt−1∑ s=1 ( Πt−1j=s+1Gj )( vm,s + vq,s + vw,s + vξ,s ) ,(b) = ( Πt−1j=1Gj )( − rmt0 ) − η t−1∑ s=1 ( Πt−1j=s+1Gj )( vm,s + vq,s + vw,s + vξ,s ) ,(32)where (a) we use the notation that Πt−1j=sGj := Gs × Gs+1 × . . . . . . Gt−1 and the notation that Πt−1j=tGj = 1 and (b) is by the update rule. By using the definitions of {q?,t−1} in the lemma statement, we complete the proof.Lemma 3 Following the notations of Lemma 2, we have thatEt0 [‖wt0+t − wt0‖2] ≥ Et0 [‖qv,t−1‖2] + 2ηEt0 [〈qv,t−1, qm,t−1 + qq,t−1 + qw,t−1 + qξ,t−1〉] := Clower (33)Proof. Following the proof of Lemma 2, we have wt0+t − wt0 = qv,t−1 + η ( qm,t−1 + qq,t−1 + qw,t−1 + qξ,t−1 ) . (34)Therefore, by using ‖a+ b‖2 ≥ ‖a‖2 + 2〈a, b〉,Et0 [‖wt0+t − wt0‖2] ≥ Et0 [‖qv,t−1‖2] + 2ηEt0 [〈qv,t−1, qm,t−1 + qq,t−1 + qw,t−1 + qξ,t−1〉]. (35)Lemma 5 Let Fthred = O( 4) and η2Tthred ≤ r2. By following the conditions and notations in Theorem 1, Lemma 1 and Lemma 2, we conclude that if SGD with momentum (Algorithm 2) has the APCG property, then we have that Clower := Et0 [‖qv,Tthred−1‖2]+2ηEt0 [〈qv,Tthred−1, qm,Tthred−1 + qq,Tthred−1 + qw,Tthred−1 + qξ,Tthred−1〉] > Cupper.W.l.o.g, we assume that cm, L, σ2, c′, ch, and ρ are not less than one and that ≤ 1.E.1 SOME CONSTRAINTS ON β .We require that parameter β is not too close to 1 so that the following holds,• 1) L(1− β)3 > 1. • 2) σ2(1− β)3 > 1. • 3) c′(1− β)2 > 1. • 4) η ≤ 1−βL . • 5) η ≤ 1−β .• 6) Tthred ≥ c(1−β)η log( Lcmσ 2ρc′ch (1−β)δγ ) ≥ 1 + 2β 1−β .The constraints upper-bound the value of β. That is, β cannot be too close to 1. We note that the β dependence on L, σ, and c′ are only artificial. We use these constraints in our proofs but they are mostly artefacts of the analysis. For example, if a function is L-smooth, and L < 1, then it is also 1-smooth, so we can assume without loss of generality that L > 1. Similarly, the dependence on σ is not highly relevant, since we can always increase the variance of the stochastic gradient, for example by adding an O(1) gaussian perturbation.To prove Lemma 5, we need a series of lemmas with the choices of parameters on Table 3.Upper bounding Et0 [‖qq,t−1‖]:Lemma 9. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [‖qq,t−1‖] ≤ ( Πt−1j=1(1 + ηθjλ) ) βLcm (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η .(36)Et0 [‖qq,t−1‖] = Et0 [‖ − t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](a) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](b) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2‖ s∑ k=1 βs−k ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](c) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k‖ ( ∇f(wt0+k)−∇Q(wt0+s) ) ‖](d) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ( ‖∇f(wt0+k)−∇f(wt0+s)‖+ ‖∇f(wt0+s)−∇Q(wt0+s)‖ ) ](37) where (a), (c), (d) is by triangle inequality, (b) is by the fact that ‖Ax‖2 ≤ ‖A‖2‖x‖2 for any matrix A and vector x. Now that we have an upper bound of ‖∇f(wt0+k)−∇f(wt0+s)‖,‖∇f(wt0+k)−∇f(wt0+s)‖ (a) ≤ L‖wt0+k − wt0+s‖ (b) ≤ Lη(s− k)cm. (38)where (a) is by the assumption of L-Lipschitz gradient and (b) is by applying the triangle inequality (s − k) times and that ‖wt − wt−1‖ ≤ η‖mt−1‖ ≤ ηcm, for any t. We can also derive an upper bound of Et0 [‖∇f(wt0+s)−∇Q(wt0+s)‖],Et0 [‖∇f(wt0+s)−∇Q(wt0+s)‖] (a)≤ Et0 [ ρ2 ‖wt0+s − wt0‖2](b) ≤ ρ 2 (8ηs(Fthred + 2r2ch + ρ3r3c3m) (1− β)2 + 8 r2σ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m. )(39) Above, (a) is by the fact that if a function f(·) has ρ Lipschitz Hessian, then‖∇f(y)−∇f(x)−∇2f(x)(y − x)‖ ≤ ρ 2 ‖y − x‖2 (40)(c.f. Lemma 1.2.4 in (Nesterov (2013))) and using the definition thatQ(w) := f(wt0) + 〈w − wt0 ,∇f(wt0)〉+ 12 (w − wt0)>H(w − wt0),(b) is by Lemma 1 and η2t ≤ r2 for 0 ≤ t ≤ Tthred Et0 [‖wt0+t − wt0‖2] ≤ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8η2tσ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m≤ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(41) Combing (37), (38), (39), we have thatEt0 [‖qq,t−1‖] (37) ≤ Et0 [ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ( ‖∇f(wt0+k)−∇f(wt0+s)‖+ ‖∇f(wt0+s)−∇Q(wt0+s)‖ ) ](38),(39) ≤ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm+ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (8ηs(Fthred + 2r2ch + ρ3r3c3m) (1− β)2 + 8 r2σ2 (1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m ):= t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm + t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (νs + ν),(42) where on the last line we use the notation thatνs := 8ηs ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2ν := 8 r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m.(43)To continue, let us analyze ‖ ( Πt−1j=s+1Gj ) ‖2 first.‖ ( Πt−1j=s+1Gj ) ‖2 = ‖Πt−1j=s+1(I − η j∑ k=1 βj−kH)‖2(a) ≤ Πt−1j=s+1(1 + ηθjλ) = Πt−1j=1(1 + ηθjλ)Πsj=1(1 + ηθjλ)(b) ≤ Πt−1j=1(1 + ηθjλ)(1 + η )s . (44)Above, we use the notation that θj := ∑j k=1 βj−k. For (a), it is due to that λ := −λmin(H), λmax(H) ≤ L, and the choice of η so that 1 ≥ ηL1−β , or equivalently,η ≤ 1− β L . (45)For (b), it is due to that θj ≥ 1 for any j and λ ≥ . Therefore, we can upper-bound the first term on r.h.s of (42) ast−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm = t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s−1∑ k=1 βkkLηcm(a) ≤ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2β(1− β)2 Lηcm(b) ≤ ( Πt−1j=1(1 + ηθjλ) ) βLηcm (1− β)2 t−1∑ s=11(1 + η )s(c) ≤ ( Πt−1j=1(1 + ηθjλ) ) βLηcm (1− β)2 1 η = ( Πt−1j=1(1 + ηθjλ) ) βLcm (1− β)2 , (46)where (a) is by that fact that ∑∞ k=1 βkk ≤ β(1−β)2 for any 0 ≤ β < 1, (b) is by using (44), and (c) is by using that ∑∞ s=1( 1 1+η )s ≤ 1η . Now let us switch to bound∑t−1 s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 ∑s k=1 β s−k ρ 2 (νs + ν) on (42). We have that t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (νs + ν) (a) ≤ 1 1− β t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 ρ 2 (νs + ν)(b) ≤ ( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 νs +( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 ν(c) ≤ ( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 νs +( Πt−1j=1(1 + ηθjλ) ) 1− β ρν 2η=( Πt−1j=1(1 + ηθjλ) ) 1− β t−1∑ s=11 (1 + η )s ρ 2 νs +( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η(d) ≤ ( Πt−1j=1(1 + ηθjλ) ) 1− β ρ (η )2 8η ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m )2η (47) where (a) is by the fact that ∑s k=1 βs−k ≤ 1/(1 − β), (b) is by (44), (c) is by using that∑∞ s=1( 1 1+η ) s ≤ 1η , (d) is by ∑∞ k=1 z kk ≤ z(1−z)2 for any |z| ≤ 1 and substituting z = 1 1+η ,which leads to ∑∞ k=1 z kk ≤ z(1−z)2 = 1/(1+η ) (1−1/(1+η ))2 = 1+η (η )2 ≤ 2 (η )2 in which the last inequality is by chosen the step size η so that η ≤ 1. By combining (42), (46), and (47), we have thatEt0 [‖qq,t−1‖ (42) ≤ t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−kLη(s− k)cm + t−1∑ s=1 ‖ ( Πt−1j=s+1Gj ) ‖2 s∑ k=1 βs−k ρ 2 (νs + ν)(46),(47) ≤ ( Πt−1j=1(1 + ηθjλ) ) βLcm (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)2+( Πt−1j=1(1 + ηθjλ) ) 1− β ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η ,(48) which completes the proof.Lemma 10. Following the conditions in Lemma 1 and Lemma 2, we have‖qv,t−1‖ ≤ ( Πt−1j=1(1 + ηθjλ) ) rcm. (49)Proof. ‖qv,t−1‖ ≤ ‖ ( Πt−1j=1Gj )( − rmt0 ) ‖ ≤ ‖ ( Πt−1j=1Gj ) ‖2‖ − rmt0‖ ≤ ( Πt−1j=1(1 + ηθjλ) ) rcm, (50) where the last inequality is because η is chosen so that 1 ≥ ηL1−β and the fact that λmax(H) ≤ L.Lower bounding Et0 [2η〈qv,t−1, qq,t−1〉]: Lemma 11. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [2η〈qv,t−1, qq,t−1〉] ≥ −2η ( Πt−1j=1(1 + ηθjλ) )2 rcm×[ βLcm(1− β)2 +ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)3 + ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η (1− β) ].(51)Proof. By the results of Lemma 9 and Lemma 10Et0 [2η〈qv,t−1, qq,t−1〉] ≥ −Et0 [2η‖qv,t−1‖‖qq,t−1‖] Lemma 10 ≥ −Et0 [2η ( Πt−1j=1(1 + ηθjλ) ) rcm‖qq,t−1‖]Lemma 9 ≥ −2η ( Πt−1j=1(1 + ηθjλ) )2 rcm×[ βLcm(1− β)2 +ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)3 + ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η (1− β) ].(52)Lower bounding Et0 [2η〈qv,t−1, qξ,t−1〉]: Lemma 12. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [2η〈qv,t−1, qξ,t−1〉] = 0. (53)Et0 [2η〈qv,t−1, qξ,t−1〉] = Et0 [2η〈qv,t−1,− t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−kξt0+k〉](a) = Et0 [2η〈qv,t−1, s∑ k=1 αkξt0+k〉](b) = Et0 [2η s∑ k=1 Et0+k−1[〈qv,t−1, αkξt0+k〉]](c) = Et0 [2η s∑ k=1 〈qv,t−1,Et0+k−1[αkξt0+k]〉]= Et0 [2η s∑k=1αk〈qv,t−1,Et0+k−1[ξt0+k]〉](d) = 0,(54)where (a) holds for some coefficients αk, (b) is by the tower rule, (c) is because qv,t−1 is measureable with t0, and (d) is by the zero mean assumption of ξ’s.Lower bounding Et0 [2η〈qv,t−1, qm,t−1〉]: Lemma 13. Following the conditions in Lemma 1 and Lemma 2, we haveEt0 [2η〈qv,t−1, qm,t−1〉] ≥ 0. (55)Proof. Et0 [2η〈qv,t−1, qm,t−1〉]= 2ηrEt0 [〈 ( Πt−1j=1Gj ) mt0 , t−1∑ s=1 ( Πt−1j=s+1Gj ) βsmt0〉](a) = 2ηrEt0 [〈mt0 , Bmt0〉](b) ≥ 0,(56)where (a) is by defining the matrix B := ( Πt−1j=1Gj )>(∑t−1 s=1 ( Πt−1j=s+1Gj ) βs ) . For (b), notice that the matrix B is symmetric positive semidefinite. To see that the matrix B is symmetric positive semidefinite, observe that each Gj := (I − η ∑j k=1 βj−kH) can be written in the form of Gj = UDjU> for some orthonormal matrix U and a diagonal matrix Dj . Therefore, the matrix product( Πt−1j=1Gj )>( Πt−1j=s+1Gj ) = U(Πt−1j=1Dj)(Π t−1 j=s+1Dj)U> is symmetric positive semidefinite as long as each Gj is. So, (b) is by the property of a matrix being symmetric positive semidefinite.Lower bounding 2ηEt0 [〈qv,t−1, qw,t−1〉]: Lemma 14. Following the conditions in Lemma 1 and Lemma 2, if SGD with momentum has the APCG property, then2ηEt0 [〈qv,t−1, qw,t−1〉] ≥ − 2ηrc′(1− β) (Πt−1j=1(1 + ηθjλ)) 2 . (57)Proof. Define Ds := Πt−1j=1GjΠ t−1 j=s+1Gj .2ηEt0 [〈qv,t−1, qw,t−1〉] = 2ηEt0 [〈 ( Πt−1j=1Gj )( rmt0 ) , t−1∑ s=1 ( Πt−1j=s+1Gj ) s∑ k=1 βs−k∇f(wt0)〉]= 2ηEt0 [〈rmt0 , t−1∑ s=1 ( Πt−1j=1GjΠ t−1 j=s+1Gj ) s∑ k=1 βs−k∇f(wt0)〉]= 2ηr t−1∑ s=1 s∑ k=1 βs−kEt0 [〈mt0 , Ds∇f(wt0)〉](a) ≥ −2η2rc′ t−1∑ s=1 s∑ k=1 βs−k‖Ds‖2‖∇f(wt0)‖2≥ −2η 2rc′1− β t−1∑ s=1 ‖Ds‖2‖∇f(wt0)‖2, (58)where (a) is by the APCG property. We also have that ‖Ds‖2 = ‖Πt−1j=1GjΠ t−1 j=s+1Gj‖2 ≤ ‖Π t−1 j=1Gj‖2‖Π t−1 j=s+1Gj‖2(a) ≤ ‖Πt−1j=1Gj‖2 Πt−1j=1(1 + ηθjλ)(1 + η )s(b) ≤ ( Πt−1j=1(1 + ηθjλ) )2 (1 + η )s(59)where (a) and (b) is by (44). Substituting the result back to (58), we get2ηEt0 [〈qv,t−1, qw,t−1〉] ≥ − 2η2rc′1− β t−1∑ s=1 ‖Ds‖2‖∇f(wt0)‖2≥ −2η 2rc′1− β t−1∑ s=1( Πt−1j=1(1 + ηθjλ) )2 (1 + η )s ‖∇f(wt0)‖2 ≥ − 2η2rc′ (1− β)η ( Πt−1j=1(1 + ηθjλ) )2‖∇f(wt0)‖2 (60)Using the fact that ‖∇f(wt0)‖ ≤ completes the proof.Recall that the strategy is proving by contradiction. Assume that the function value does not decrease at least Fthred in Tthred iterations on expectation. Then, we can get an upper bound of the expected distance Et0 [‖wt0+Tthred − wt0‖2] ≤ Cupper but, by leveraging the negative curvature, we can also show a lower bound of the form Et0 [‖wt0+Tthred − wt0‖2] ≥ Clower. The strategy is showing that the lower bound is larger than the upper bound, which leads to the contradiction and concludes that the function value must decrease at least Fthred in Tthred iterations on expectation. To get the contradiction, according to Lemma 1 and Lemma 3, we need to show that Et0 [‖qv,Tthred−1‖2] + 2ηEt0 [〈qv,Tthred−1, qm,Tthred−1 + qq,Tthred−1 + qw,Tthred−1 + qξ,Tthred−1〉] > Cupper. (61) Yet, by Lemma 13 and Lemma 12, we have that ηEt0 [〈qv,Tthred−1, qm,Tthred−1〉] ≥ 0 and ηEt0 [〈qv,Tthred−1, qξ,Tthred−1〉] = 0. So, it suffices to prove that Et0 [‖qv,Tthred−1‖2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1 + qw,Tthred−1〉] > Cupper, (62) and it suffices to show that• 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1〉] ≥ 0.• 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qw,Tthred−1〉] ≥ 0.• 14Et0 [‖qv,Tthred−1‖ 2] ≥ Cupper.E.3.1 PROVING THAT 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1〉] ≥ 0:By Lemma 4 and Lemma 11, we have that1 4 Et0 [‖qv,Tthred−1‖2] + Et0 [2η〈qv,Tthred−1, qq,Tthred−1〉] ≥ 1 4 ( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ − 2η ( ΠTthred−1j=1 (1 + ηθjλ) )2 rcm × [ βLcm (1− β)2 + ρ η 2 8 ( Fthred + 2r2ch + ρ3r 3c3m ) (1− β)3 + ρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m ) 2η (1− β) ].(63)To show that the above is nonnegative, it suffices to show thatr2γ ≥ 24ηrβLc 2 m(1− β)2 , (64)andr2γ ≥ 24ηrcmρ (1− β)η 28 ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 , (65)andr2γ ≥ 24ηrcm 1− βρ ( 8 r 2σ2 (1−β)2 + 4η 2 ( β 1−β )2 c2m + 2r 2c2m )2η . (66)Now w.l.o.g, we assume that cm, L, σ2, c′, and ρ are not less than one and that ≤ 1. By using the values of parameters on Table 3, we have the following results; a sufficient condition of (64) is thatcr cη ≥ 24Lc2 m 2(1− β)2 . (67)A sufficient condition of (65) is thatcr cF ≥ 576cmρ (1− β)3 , (68)and 1 ≥ 1152cmρchcr(1− β)3 , (69)and1 ≥ 192c 4 mρ 2c2r (1− β)3 . (70)A sufficient condition of (66) is that1 ≥ 96cmρ(σ 2 + 3c2m)cr(1− β)3 , (71)and a sufficient condition for the above (71), by the assumption that both σ2 ≥ 1 and cm ≥ 1, is1 ≥ 576c 3 mρσ 2cr(1− β)3 . (72)Now let us verify if (67), (68), (69), (70), (72) are satisfied. For (67), using the constraint of cη on Table 3, we have that 1cη ≥ c5mρL 2σ2c′ch c1. Using this inequality, it suffices to let cr ≥ c0 2c3mρLσ 2c′ch(1−β)2 for getting (67), which holds by using the constraint that c′(1−β)2 > 1 and ≤ 1.For (68), using the constraint of cF on Table 3, we have that 1cF ≥ c4mρ 2Lσ4ch c2. Using this inequality, it suffices to let cr ≥ c0c3mρLσ4(1−β)3 , which holds by using the constraint that σ 2(1− β)3 > 1. For (69), it needs (1−β) 31152cmρch ≥ c0c3mρLσ2ch ≥ cr, which hold by using the constraint that σ 2(1− β)3 > 1.For (70), it suffices to let (1−β) 214c2mρ ≥ c0c3mρLσ2ch ≥ cr which holds by using the constraint thatσ2(1− β)3 > 1. For (72), it suffices to let (1−β) 3576c3mρσ 2 ≥ c0 c3mρLσ 2ch ≥ cr, which holds by using theconstraint that L(1− β)3 > 1 and ≤ 1. Therefore, by choosing the parameter values as Table 3, we can guarantee that 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qq,Tthred−1〉] ≥ 0.E.3.2 PROVING THAT 14Et0 [‖qv,Tthred−1‖ 2] + 2ηEt0 [〈qv,Tthred−1, qw,Tthred−1〉] ≥ 0:By Lemma 4 and Lemma 14, we have that1 4 Et0 [‖qv,Tthred−1‖2] + 2ηEt0 [〈qv,Tthred−1, qw,Tthred−1〉]≥ 1 4( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ − 2ηrc ′(1− β) (ΠTthred−1j=1 (1 + ηθjλ))2 . (73)To show that the above is nonnegative, it suffices to show thatr2γ ≥ 8ηrc ′(1− β) . (74)A sufficient condition is crcη ≥ 8 4c′ 1−β . Using the constraint of cη on Table 3, we have that 1 cη ≥ c5mρL 2σ2c′ch c1 . So, it suffices to let cr ≥ c0 4 3c5mρL 2σ2ch(1−β) , which holds by using the constraint that L(1− β)3 > 1 (so that L(1− β) > 1) and ≤ 1.E.3.3 PROVING THAT 14Et0 [‖qv,Tthred−1‖ 2] ≥ Cupper :From Lemma 4 and Lemma 1, we need to show that14( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ≥ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m. (75)We know that 14 ( ΠTthred−1j=1 (1 + ηθjλ) )2 r2γ ≥ 14 ( ΠTthred−1j=1 (1 + ηθj ) )2 r2γ. It suffices to show that 14( ΠTthred−1j=1 (1 + ηθj ) )2 r2γ≥ 8ηt ( Fthred + 2r2ch + ρ3r 3c3m )(1− β)2 + 8r2σ2(1− β)2 + 4η2 ( β 1− β )2 c2m + 2r 2c2m. (76)Note that the left hand side is exponentially growing in Tthred. We can choose the number of iterations Tthred large enough to get the desired result. Specifically, we claim that Tthred ≥ c(1−β) η log( Lcmσ 2ρc′ch(1−β)δγ ) for some constant c > 0. To see this, let us first apply log on both sides of (76),2 ( Tthred−1∑j=1log(1 + ηθj ) ) + log(r2γ) ≥ log(8aTthred + 8b) (77)where we denote a := 4η ( Fthred+2r2ch+ ρ3 r 3c3m ) (1−β)2 and b := 4 r2σ2 (1−β)2 + 2η 2 ( β 1−β )2 c2m + r2c2m. To proceed, we are going to use the inequality log(1 + x) ≥ x2 , for x ∈ [0,∼ 2.51]. We have that1 ≥ η (1− β)(78)as guaranteed by the constraint of η. So,2 ( Tthred−1∑j=1 log(1 + ηθj ) ) (a) ≥ Tthred−1∑ j=1 ηθj = Tthred−1∑ j=1 j−1∑ k=0 βkη= Tthred−1∑ j=1 1− βj 1− β η ≥ 1 1− β (Tthred − 1− β 1− β )η .(b) ≥ Tthred − 1 2(1− β) η , (79)where (a) is by using the inequality log(1 + x) ≥ x2 with x = ηθj ≤ 1 and (b) is by making Tthred−1 2(1−β) ≥ β (1−β)2 , which is equivalent to the condition thatTthred ≥ 1 + 2β1− β (80)Now let us substitute the result of (79) back to (77). We have thatTthred ≥ 1 + 2(1− β)η log( 8aTthred + 8b γr2 ), (81)which is what we need to show. By choosing Tthred large enough,Tthred ≥ c(1− β)η log(Lcmσ 2ρc′ch(1− β)δγ ) = O((1− β) log( 1 (1− β) ) −6) (82)for some constant c > 0, we can guarantee that the above inequality (81) holds.Lemma 15 (Daneshmand et al. (2018)) Let us define the event Υk := {‖∇f(wkTthred)‖ ≥ or λmin(∇2f(wkTthred)) ≤ − }. The complement is Υck := {‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≥ − }, which suggests that wkTthred is an ( , )-second order stationary points. Suppose thatE[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ −∆E[f(w(k+1)Tthred)− f(wkTthred)|Υ c k] ≤ δ∆ 2 .(83)Set T = 2Tthred ( f(w0) − minw f(w) ) /(δ∆). We return w uniformly randomly from w0, wTthred , w2Tthred , . . . , wkTthred , . . . , wKTthred , where K := bT/Tthredc. Then, with probability at least 1− δ, we will have chosen a wk where Υk did not occur.Proof. Let Pk be the probability that Υk occurs. E[f(w(k+1)Tthred)− f(wkTthred)] = E[f(w(k+1)Tthred)− f(wkTthred)|Υk]Pk + E[f(w(k+1)Tthred)− f(wkTthred)|Υ c k](1− Pk)≤ −∆Pk + δ∆/2(1− Pk) = δ∆/2− (1 + δ/2)∆Pk ≤ δ∆/2−∆Pk.(84) Summing over all K, we have1K + 1 K∑ k=0 E[f(w(k+1)Tthred)− f(wkTthred)] ≤ ∆ 1 K + 1 K∑ k=0 (δ/2− Pk)⇒ 1 K + 1 K∑ k=0 Pk ≤ δ/2 + f(w0)−minw f(w) (K + 1)∆ ≤ δ⇒ 1 K + 1 K∑ k=0 (1− Pk) ≥ 1− δ.(85)Theorem 1 Assume that the stochastic momentum satisfies CNC. Set r = O( 2), η = O( 5), and Tthred = c(1−β)η log( Lcmσ 2ρc′ch (1−β)δγ ) = O((1 − β) log( Lcmσ 2ρc′ch (1−β)δγ )−6) for some constant c > 0. If SGD with momentum (Algorithm 2) has APAG property when gradient is large (‖∇f(w)‖ ≥ ), APCGTthred property when it enters a region of saddle points that exhibits a negative curvature (‖∇f(w)‖ ≤ and λmin(∇2f(w)) ≤ − ), and GrACE property throughout the iterations, then it reaches an ( , ) second order stationary point in T = 2Tthred(f(w0)−minw f(w))/(δFthred) = O((1− β) log(Lcmσ2ρc′ch (1−β)δγ ) −10) iterations with high probability 1− δ, where Fthred = O( 4).In this subsection, we provide a sketch of the proof of Theorem 1. The complete proof is available in Appendix G. Our proof uses a lemma in (Daneshmand et al. (2018)), which is Lemma 15 below. The lemma guarantees that uniformly sampling a w from {wkTthred}, k = 0, 1, 2, . . . , bT/Tthredc gives an ( , )-second order stationary point with high probability. We replicate the proof of Lemma 15 in Appendix F.Lemma 15. (Daneshmand et al. (2018)) Let us define the event Υk := {‖∇f(wkTthred)‖ ≥ or λmin(∇2f(wkTthred)) ≤ − }. The complement is Υck := {‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≥ − }, which suggests that wkTthred is an ( , )-second order stationary points. Suppose thatE[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ −∆ & E[f(w(k+1)Tthred)− f(wkTthred)|Υ c k] ≤ δ∆ 2 . (86) Set T = 2Tthred ( f(w0) − minw f(w) ) /(δ∆). 8 We return w uniformly randomly from w0, wTthred , w2Tthred , . . . , wkTthred , . . . , wKTthred , where K := bT/Tthredc. Then, with probability at least 1− δ, we will have chosen a wk where Υk did not occur.To use the result of Lemma 15, we need to let the conditions in (86) be satisfied. We can bound E[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ −Fthred, based on the analysis of the large gradient norm regime (Lemma 7) and the analysis for the scenario when the update is with small gradient norm but a large negative curvature is available (Subsection 3.2.1). For the other condition, E[f(w(k+1)Tthred)− f(wkTthred)|Υck] ≤ δ Fthred 2 , it requires that the expected amortized increase of function value due to taking the large step size r is limited (i.e. bounded by δFthred2 ) when wkTthred is a second order stationary point. By having the conditions satisfied, we can apply Lemma 15 and finish the proof of the theorem.Proof. Our proof is based on Lemma 15. So, let us consider the events in Lemma 15, Υk := {‖∇f(wkTthred)‖ ≥ or λmin(∇2f(wkTthred)) ≤ − }. We first show that E[f(w(k+1)Tthred)− f(wkTthred)|Υk] ≤ Fthred.8One can use any upper bound of f(w0)−minw f(w) as f(w0)−minw f(w) in the expression of T .Consider that Υk is the case that ‖∇f(wkTthred)‖ ≥ . Denote t0 := kTthred in the following. We have thatEt0 [f(wt0+Tthred)− f(wt0)] = Tthred−1∑ t=0 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]]= Et0 [f(wt0+1)− f(wt0)] + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](a) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m 2 + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](b) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m 2 + Tthred−1∑ t=1 ( η2ch + ρ 6 η3c3m ) (c) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m 2 + r2ch + ρ 6 r3c3m(d) ≤ −r 2 ‖∇f(wt0)‖2 + Lr2c2m + r2ch(e) ≤ −r 2 2 + Lr2c2m + r 2ch (f) ≤ −r 4 2 (g) ≤ −Fthred,(87)where (a) is by using Lemma 6 with step size r, (b) is by using Lemma 8, (c) is due to the constraint that η2Tthred ≤ r2, (d) is by the choice of r, (e) is by ‖∇f(wt)‖ ≥ , (f) is by the choice of r so that r ≤ 24(Lc2m+ch) , and (g) is byr 4 2 ≥ Fthred. (88)The scenario that Υk is the case that ‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≤ − has been analyzed in Appendix E, which guarantees that E[f(wt0+Tthred) − f(wt0)] ≤ −Fthred under the setting.Now let us switch to show that E[f(w(k+1)Tthred) − f(wkTthred)|Υck] ≤ δ Fthred 2 . Recall that Υ c k means that ‖∇f(wkTthred)‖ ≤ and λmin(∇2f(wkTthred)) ≥ − . Denote t0 := kTthred in the following. We have thatEt0 [f(wt0+Tthred)− f(wt0)] = Tthred−1∑ t=0 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]]= Et0 [f(wt0+1)− f(wt0)] + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](a) ≤ r2ch + ρ6 r3c3m + Tthred−1∑ t=1 Et0 [E[f(wt0+t+1)− f(wt0+t)|w0:t0+t]](b) ≤ r2ch + ρ6 r3c3m + Tthred−1∑ t=1 ( η2ch + ρ 6 η3c3m ) (c) ≤ 2r2ch + ρ3 r3c3m ≤ 4r2ch(d) ≤ δFthred 2 . (89)where (a) is by using Lemma 8 with step size r, (b) is by using Lemma 8 with step step size η, (c) is by setting η2Tthred ≤ r2 and η ≤ r, (d) is by the choice of r so that 8r2ch ≤ δFthred.Now we are ready to use Lemma 15, since both the conditions are satisfied. According to the lemma and the choices of parameters value on Table 3, we can set T = 2Tthred ( f(w0) −minw f(w) ) /(δFthred) = O((1− β) log(Lcmσ 2ρc′ch (1−β)δγ )−10), which will return a w that is an ( , ) second order stationary point. Thus, we have completed the proof.Theorem 2 in Daneshmand et al. (2018) states that, for CNC-SGD to find an ( , ρ1/2 ) stationary point, the total number of iterations is T = O( ` 10L3δ4γ4 log 2( `L δγ ) −10), where ` is the bound of the stochastic gradient norm ‖gt‖ ≤ `which can be viewed as the counterpart of cm in our paper. By translating their result for finding an ( , ) stationary point, it is T = O( ` 10L3ρ5δ4γ4 log 2(ρ`L δγ ) −10). On the other hand, using the parameters value on Table 3, we have that T = 2Tthred ( f(w0)−minw f(w) ) /(δFthred) = O( (1−β)c9mL 3ρ3(σ2)3c2hc ′δ4γ4 log( Lcmσ 2c′ch (1−β)δγ ) −10) for Algorithm 2.Before making a comparison, we note that their result does not have a dependency on the variance of stochastic gradient (i.e. σ2), which is because they assume that the variance is also bounded by the constant ` (can be seen from (86) in the supplementary of their paper where the variance terms ‖ζi‖ are bounded by `). Following their treatment, if we assume that σ2 ≤ cm, then on (71) we can instead replace (σ2 + 3c2m) with 4c 2 m and on (72) it becomes 1 ≥ 576c3mρcr (1−β)3 . This will remove all the parameters’ dependency on σ2. Now by comparing Õ((1− β)c9mc2hc′ · ρ3L3 δ4γ4 −10) of ours and T = Õ(ρ2`10 · ρ 3L3δ4γ4 −10) of Daneshmand et al. (2018), we see that in the high momentum regimewhere (1−β) << ρ 2`10c9mc 2 hc ′ , Algorithm 2 is strictly better than that of Daneshmand et al. (2018), whichmeans that a higher momentum can help to find a second order stationary point faster.
industrial or research application depend on computerized segmentation of different parts of images such as stagnant and flowing zones which is the toughest task. X-ray Computed Tomography (CT) is one of a powerful non-destructive technique for cross-sectional images of a 3D object based on X-ray absorption. CT is the most proficient for investigating different granular flow phenomena and segmentation of the stagnant zone as compared to other imaging techniques. In any case, manual segmentation is tiresome and erroneous for further investigations. Hence, automatic and precise strategies are required. In the present work, a U-net architecture is used for segmenting the stagnant zone during silo discharging process. This proposed image segmentation method provides fast and effective outcomes by exploiting a convolutional neural networks technique with an accuracy of 97 percent.Keywords—U-net segmentation; deep neural networks; stagnantzone; X-ray tomographyI. INTRODUCTIONIn many industries, like in mining, agriculture, civil engineering, and pharmaceutical manufacturing; silo is used for protecting, storing and loading granular materials into process machinery [1]. During the silo discharging process, there are two major types of flow: namely, “funnel” and “mass” flows [2]. In the case of mass flow, all granulates discharge with a uniform downward velocity across the entire cross-section area. Whereas, in the case of funnel flow, in which this paper focuses on and characterized by granular is flowing only in the center of the silo and creating a stagnant zone at the walls of the container [2,3].The hopper geometry, internal friction between particles and wall all have a direct impact on the flow type [4]. In order to understand and describe the flow behavior and evaluating the silo wall pressures, the knowledge of the density distribution within the bulk solid is very important aspect [5]. Furthermore, during funnel flow, the shape and size of the stagnant zone depend on different factors including the granular material, the bin wall roughness, the initial packing density and filling level [6].X-ray Computed tomography (CT) is one of the most powerful 3D imaging techniques among available tomographic techniques due to its high-resolution capability. For the last two decades CT has been used as a non-destructive method to characterize objects, visualize flows, analyze concentration changes of the bulk solid during silo discharging process [5 ,7,8]. Particle Image Velocimetry (PIV) [9,10] and Electrical Capacitance Tomography (ECT) [11–14] are also some of the techniques used to visualize concentration changes during flows of granular materials.Even though convolutional neural network (CNN) has recently become popular and has increasingly been used as an alternative to many traditional pattern recognition problems, its application for segmenting stagnant zones of X-ray CT images is not common. For processing and analyzing process tomography data, artificial neural network algorithms were applied in electrical impedance tomography images [15–17]. This paper proposed a deep neural networks technique for segmenting the stagnant zone automatically. The main advantage of the proposed approach is an effective segmentation for acquiring the desired characteristics of flow parameters without prior image processing or expert guidance.II. EXPERIMENTAL SET-UP USING X-RAY TOMOGRAPHYAn especially designed model silo, with rectangular bin, allowed carrying out in situ experiments. The bin part is 10 cm wide, 5 cm deep and 20 cm high. The left and right hopper angles can be independently set to generate different types of flows, i.e. mass and funnel flows, with concentric/eccentric discharging modes. The silo model design was easily customizable to shift the outlet position and the angle between the hopper and the silo with respect to the vertical. The silo material is polycarbonate with 5 mm thickness. The outlet width is manually set, which allows controlling the discharge velocity. During concentric flow, both hoppers’ angles are set to the same value. In order to observe the eccentric flow, these angles are set to a different value. The non-symmetric silo construction causes shift silo outlet to the left or right side, in the direction of the larger angle.The time-lapse studies were performed at INSA-Lyon (France) using a GE Phoenix v|tome|x device (see Fig. 1). The device is equipped with a high energy X-ray microfocus source (up to 160 kV) with a 4 μm spot size. The detector is equipped with a 1500X1920 pixels (with a pixel size of 127X127 μm 2 ). Fig. 2 presented a picture of the silo model inside X-ray tomography hutch. During measurements, the distance between the X-ray source and the detector was equal to 577 mm and voxel size 160 um. Sorghum and rice have been used as a granular material during the experimental campaign of this study.III. THREE-DIMENSIONAL SEGMENTATIONCross-sectional views of the X-ray tomography of initially loosely packed rice with smooth silo bin walls are presented in Fig. 2.The main aim of this study was to find an effective way of segmenting the stagnant zone during eccentric discharging modes. After the initial packing density scan was performed, the outlet of the silo was opened for about 2 second and the next scan was carried out till all granulate are discharged out from the silo. Thus, to analyze the concentration changes, theabsolute difference between two successive scans has been computed and it shows the formation of the stagnant zone in the case of funnel flow. Fig. 3c presents concentration changes during eccentric discharging mode (angle between the hopper and the silo with respect to the vertical axis was 30 º and 20 º ) and it reveals the evolution of the stagnant zone at the right side of the image with a good contrast in which the hopper angle was 20 º .The last step consists in segmenting the stagnant zones. By using a three-dimensional Otsu threshold method it was possible to compute the stagnant zone mask as presented in Fig.4.Although the Otsu method was able to segment the stagnant zone, it is hard to get 100% accurate segmentation out of it. Since some particles in the flow zone may shift in the position of another particle, giving an impression of no movement based on the absolute difference. Thus, such kind of effect could hinder the segmentation result. As a result, a deep neural networks approach has been applied for effective segmentation and explained hereafter.IV. U-NET BASED DEEP CONVOLUTIONAL NETWORKSThe main advantage of the proposed approach is an effective segmentation for acquiring the desired characteristicsof flow parameters without prior image processing or expert guidance. From several deep neural networks, u-net is one of the successful architecture which is used in different image segmentation tasks. Originally u-net neural network architecture was built for performing semantic segmentation on a small biomedical data-set [18]. The architecture is computationally efficient and trainable with a small dataset, which is a core advantage for datasets like in material science where the little amount of labelled data is available. Despite it might be good and well-fitted in bio-medical tasks, it needs to be tuned again for fitting the granular material X-ray CT image segmentation task investigated in this study. Thus, the u-net neural network architecture and hyper-parameters values are adjusted in order to get good segmentation results.The network has been trained on 2D segmentation and then it can find the whole 3D volume segmentation of a new scan in which the network has never seen before. As each of the CT images already contain repetitive structures with the corresponding variation, only very few images are required to train a network that generalizes reasonably well. One of the major modifications is that the original u-net used stochastic gradient descent optimizer [18], but this modified u-net architecture used Adam optimizer [19] to minimize the categorical cross-entropy objective.During training, 40 images were selected for the training and test dataset. The dataset was first divided into two subsets, train, and test. The first subset contains 30 images in which 80% were used for training and 20% for validation. The trained model was next tested on the second subset which contains the remaining 10 images. During training, 30 manually annotated ground truth segmentations were used to train the network to recognize the stagnant zone borders. Since the available dataset is small, an extensive amount of data augmentations has been applied to improve the performance of the network. The testing datasets were used for the evaluation of the network performance.Fig. 5 illustrates the comparison between the ground truth, segmentation by using Otsu and predicted segmentation by the modified u-net trained model for the same 2D image. As the result shows, Otsu segmentation is too sensitive for gradient changes and the trained model predicted a smooth segmentation like the ground truth.Intersection over Union (IoU) is used as an accuracy measure to compare dropped out ground truth slices to the predicted results. The IoU score is a standard performance measure for the object category segmentation problem. Given a set of images, the IoU measure gives the similarity between the predicted region and the ground-truth region for an object present in the set of images and is defined as true positives/(truepositives + false negatives + false positives). Table 1 presents the result of the experiment for segmenting stagnant zone.Once after having trained model using the CNN method, the end-to-end 3D automatic segmentation offers an effective and fast segmentation of stagnant zone during silo discharging process. The key advantage of this method could be used for deep investigation of flow characteristics without utilizing any prior image processing methods or expert guidance. For both the Otsu and the trained model execution time (on CPU) were compared for generating one segmented image. The Otsu segmentation took around 4 sec per one 2D image. Where else the trained model took less than a second for generating its prediction for a given input image.In order to prove that the trained model (which used the sorghum grains flow as a training dataset) could generate the stagnant zone segmentation for completely different scan and grain material, it was tested by pre-processing two 3D scans of rice grains flow. The result shows that the trained model was able to generate predicted segmentation successfully. Fig. 6 presents two major steps for generating the stagnant zone prediction of new scans having similar flow property (in this case rice grains flow). The first step was computing the absolute difference of two successive scans and then this new scan was given as an input to the trained model. Finally, the trained model generates the stagnant zone segmentation as shown in Fig. 6d. Fig. 7 illustrates 3D dense segmentation of rice grains flow by superimposing the predicted mask into the original scan.side view .V. CONCLUSIONIn order to analyse the X-ray CT data that has been acquired from different experimental campaigns and complex structure of granular material flows, it can be tedious and extremely timeconsuming for manual analysis. This paper presents a new approach for automatic segmentation of the stagnant zone in an effective way by exploiting the CNN technique. The main advantages of the proposed approach are the speed and effective segmentation for acquiring the desired characteristic flow parameters. Once having the trained model, it was tested that the model could generate a predicted segmentation in less than a second for a given completely new granular material flow image with an accuracy of 97%. The accuracy of the CNN approach could also probably be further improved if the delineations of the ground truth were acquired from different experts and more number of dataset was used. Moreover, the architecture of the model could be modified to accommodate 3D volumes of images as an input for processing them with corresponding 3D operations.The work is funded by the National Science Centre in Poland (grant number: 2015/19/B/ST8/02773).[18] Ronneberger, O., Fischer, P., Brox, T.: U-Net: ConvolutionalNetworks for Biomedical Image Segmentation. In: Medical ImageComputing and Computer-Assisted Intervention (MICCAI). pp.234–241. Springer (2015)[19] Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization.Int. Conf. Learn. Represent. (2014)View publication stats
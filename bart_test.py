# %%
from models.bart_utils import BARTMultiGPUWrapper

bart = BARTMultiGPUWrapper(device='cpu')
bart.set_mode('train')
src_tokens = bart.encode("this is a very very long document, it has so many tokens.").unsqueeze(0)
tgt_tokens = bart.encode("this is its summary.").unsqueeze(0)
lm_logits = bart(src_tokens, tgt_tokens)
# %%
fulltext = "Recent work [21] shows that it is often possible to construct an input mislabeled by a neural net by " \
           "perturbing a correctly labeled input by a tiny amount in a carefully chosen direction. Lack of robustness " \
           "can be problematic in a variety of settings, such as changing camera lens or lighting conditions, " \
           "successive frames in a video, or adversarial attacks in security-critical applications [18]. A number of " \
           "approaches have since been proposed to improve robustness [6, 5, 1, 7, 20]. However, work in this " \
           "direction has been handicapped by the lack of objective measures of robustness. A typical approach to " \
           "improving the robustness of a neural net f is to use an algorithm A to find adversarial examples, " \
           "augment the training set with these examples, and train a new neural net f ′ [5]. Robustness is then " \
           "evaluated by using the same algorithm A to find adversarial examples for f ′—if A discovers fewer " \
           "adversarial examples for f ′ than for f , then f ′ is concluded to be more robust than f . However, " \
           "f ′ may have overfit to adversarial examples generated by A—in particular, a different algorithm A′ may " \
           "find as many adversarial examples for f ′ as for f . Having an objective robustness measure is vital not " \
           "only to reliably compare different algorithms, but also to understand robustness of production neural " \
           "nets—e.g., when deploying a login system based on face recognition, a security team may need to evaluate " \
           "the risk of an attack using adversarial examples. In this paper, we study the problem of measuring " \
           "robustness. We propose to use two statistics of the robustness ρ(f,x∗) of f at point x∗ (i.e., " \
           "the L∞ distance from x∗ to the nearest adversarial example) [21]. The first one measures the frequency " \
           "with which adversarial examples occur; the other measures the severity of such adversarial examples. Both " \
           "statistics depend on a parameter , which intuitively specifies the threshold below which adversarial " \
           "examples should not exist (i.e., points x with L∞ distance to x∗ less than should be assigned the same " \
           "label as x∗). 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, " \
           "Spain. The key challenge is efficiently computing ρ(f,x∗). We give an exact formulation of this problem " \
           "as an intractable optimization problem. To recover tractability, we approximate this optimization problem " \
           "by constraining the search to a convex region Z(x∗) around x∗. Furthermore, we devise an iterative " \
           "approach to solving the resulting linear program that produces an order of magnitude speed-up. Common " \
           "neural nets (specifically, those using rectified linear units as activation functions) are in fact " \
           "piecewise linear functions [15]; we choose Z(x∗) to be the region around x∗ on which f is linear. Since " \
           "the linear nature of neural nets is often the cause of adversarial examples [5], our choice of Z(x∗) " \
           "focuses the search where adversarial examples are most likely to exist. We evaluate our approach on a " \
           "deep convolutional neural network f for MNIST. We estimate ρ(f,x∗) using both our algorithm ALP and (as a " \
           "baseline) the algorithm AL-BFGS introduced by [21]. We show that ALP produces a substantially more " \
           "accurate estimate of ρ(f,x∗) than AL-BFGS. We then use data augmentation with each algorithm to improve " \
           "the robustness of f , resulting in fine-tuned neural nets fLP and fL-BFGS. According to AL-BFGS, " \
           "fL-BFGS is more robust than f , but not according to ALP. In other words, fL-BFGS overfits to adversarial " \
           "examples computed using AL-BFGS. In contrast, fLP is more robust according to both AL-BFGS and ALP. " \
           "Furthermore, to demonstrate scalability, we apply our approach to evaluate the robustness of the 23-layer " \
           "network-in-network (NiN) neural net [13] for CIFAR-10, and reveal a surprising lack of robustness. We " \
           "fine-tune NiN and show that robustness improves, albeit only by a small amount. In summary, " \
           "our contributions are: • We formalize the notion of pointwise robustness studied in previous work [5, 21, " \
           "6] and propose two statistics for measuring robustness based on this notion (§2). • We show how computing " \
           "pointwise robustness can be encoded as a constraint system (§3). We approximate this constraint system " \
           "with a tractable linear program and devise an optimization for solving this linear program an order of " \
           "magnitude faster (§4). • We demonstrate experimentally that our algorithm produces substantially more " \
           "accurate measures of robustness compared to algorithms based on previous work, and show evidence that " \
           "neural nets fine-tuned to improve robustness (§5) can overfit to adversarial examples identified by a " \
           "specific algorithm (§6).The susceptibility of neural nets to adversarial examples was discovered by [21]. " \
           "Given a test point x∗ with predicted label `∗, an adversarial example is an input x∗ + r with predicted " \
           "label ` 6= `∗ where the adversarial perturbation r is small (in L∞ norm). Then, [21] devises an " \
           "approximate algorithm for finding the smallest possible adversarial perturbation r. Their approach is to " \
           "minimize the combined objective loss(f(x∗ + r), `) + c‖r‖∞, which is an instance of box-constrained " \
           "convex optimization that can be solved using L-BFGS-B. The constant c is optimized using line search. Our " \
           "formalization of the robustness ρ(f,x∗) of f at x∗ corresponds to the notion in [21] of finding the " \
           "minimal ‖r‖∞. We propose an exact algorithm for computing ρ(f,x∗) as well as a tractable approximation. " \
           "The algorithm in [21] can also be used to approximate ρ(f,x∗); we show experimentally that our algorithm " \
           "is substantially more accurate than [21]. There has been a range of subsequent work studying robustness; " \
           "[17] devises an algorithm for finding purely synthetic adversarial examples (i.e., no initial image x∗), " \
           "[22] searches for adversarial examples using random perturbations, showing that adversarial examples in " \
           "fact exist in large regions of the pixel space, [19] shows that even intermediate layers of neural nets " \
           "are not robust to adversarial noise, and [3] seeks to explain why neural nets may generalize well despite " \
           "poor robustness properties. Starting with [5], a major focus has been on devising faster algorithms for " \
           "finding adversarial examples. Their idea is that adversarial examples can then be computed on-the-fly and " \
           "used as training examples, analogous to data augmentation approaches typically used to train neural nets " \
           "[10]. To find adversarial examples quickly, [5] chooses the adversarial perturbation r to be in the " \
           "direction of the signed gradient of loss(f(x∗ + r), `) with fixed magnitude. Intuitively, given only the " \
           "gradient of the loss function, this choice of r is most likely to produce an adversarial example with " \
           "‖r‖∞ ≤ . In this direction, [16] improves upon [5] by taking multiple gradient steps, [7] extends this " \
           "idea to norms beyond the L∞ norm, [6] takes the approach of [21] but fixes c, and [20] formalizes [5] as " \
           "robust optimization. A key shortcoming of these lines of work is that robustness is typically measured " \
           "using the same algorithm used to find adversarial examples, in which case the resulting neural net may " \
           "have overfit to adversarial examples generating using that algorithm. For example, [5] shows improved " \
           "accuracy to adversarial examples generated using their own signed gradient method, but do not consider " \
           "whether robustness increases for adversarial examples generated using more precise approaches such as [" \
           "21]. Similarly, [7] compares accuracy to adversarial examples generated using both itself and [5] (but " \
           "not [21]), and [20] only considers accuracy on adversarial examples generated using their own approach on " \
           "the baseline network. The aim of our paper is to provide metrics for evaluating robustness, " \
           "and to demonstrate the importance of using such impartial measures to compare robustness. Additionally, " \
           "there has been work on designing neural network architectures [6] and learning procedures [1] that " \
           "improve robustness to adversarial perturbations, though they do not obtain state-of-theart accuracy on " \
           "the unperturbed test sets. There has also been work using smoothness regularization related to [5] to " \
           "train neural nets, focusing on improving accuracy rather than robustness [14]. Robustness has also been " \
           "studied in more general contexts; [23] studies the connection between robustness and generalization, " \
           "[2] establishes theoretical lower bounds on the robustness of linear and quadratic classifiers, " \
           "and [4] seeks to improve robustness by promoting resiliance to deleting features during training. More " \
           "broadly, robustness has been identified as a desirable property of classifiers beyond prediction " \
           "accuracy. Traditional metrics such as (out-of-sample) accuracy, precision, and recall help users assess " \
           "prediction accuracy of trained models; our work aims to develop analogous metrics for assessing " \
           "robustness.Consider a classifier f : X → L, where X ⊆ Rn is the input space and L = {1, ..., L} are the " \
           "labels. We assume that training and test points x ∈ X have distribution D. We first formalize the notion " \
           "of robustness at a point, and then describe two statistics to measure robustness. Our two statistics " \
           "depend on a parameter , which captures the idea that we only care about robustness below a certain " \
           "threshold—we disregard adversarial examples x whose L∞ distance to x∗ is greater than . We use = 20 in " \
           "our experiments on MNIST and CIFAR-10 (on the pixel scale 0-255). Pointwise robustness. Intuitively, " \
           "f is robust at x∗ ∈ X if a “small” perturbation to x∗ does not affect the assigned label. We are " \
           "interested in perturbations sufficiently small that they do not affect human classification; an " \
           "established condition is ‖x− x∗‖∞ ≤ for some parameter . Formally, we say f is (x∗, )-robust if for every " \
           "x such that ‖x−x∗‖∞ ≤ , f(x) = f(x∗). Finally, the pointwise robustness ρ(f,x∗) of f at x∗ is the minimum " \
           "for which f fails to be (x∗, )-robust: ρ(f,x∗) def = inf{ ≥ 0 | f is not (x∗, )-robust}. (1) This " \
           "definition formalizes the notion of robustness in [5, 6, 21]. Adversarial frequency. Given a parameter , " \
           "the adversarial frequency φ(f, ) def = Prx∗∼D[ρ(f,x∗) ≤ ] measures how often f fails to be (x∗, " \
           ")-robust. In other words, if f has high adversarial frequency, then it fails to be (x∗, )-robust for many " \
           "inputs x∗. Adversarial severity. Given a parameter , the adversarial severity µ(f, ) def = Ex∗∼D[ρ(f," \
           "x∗) | ρ(f,x∗) ≤ ] measures the severity with which f fails to be robust at x∗ conditioned on f not being " \
           "(x∗, )-robust. We condition on pointwise robustness since once f is (x∗, )-robust at x∗, then the degree " \
           "to which f is robust at x∗ does not matter. Smaller µ(f, ) corresponds to worse adversarial severity, " \
           "since f is more susceptible to adversarial examples if the distances to the nearest adversarial example " \
           "are small. The frequency and severity capture different robustness behaviors. A neural net may have high " \
           "adversarial frequency but low adversarial severity, indicating that most adversarial examples are about " \
           "distance away from the original point x∗. Conversely, a neural net may have low adversarial frequency but " \
           "high adversarial severity, indicating that it is typically robust, but occasionally severely fails to be " \
           "robust. Frequency is typically the more important metric, since a neural net with low adversarial " \
           "frequency is robust most of the time. Indeed, adversarial frequency corresponds to the accuracy on " \
           "adversarial examples used to measure robustness in [5, 20]. Severity can be used to differentiate between " \
           "neural nets with similar adversarial frequency. Given a set of samples X ⊆ X drawn i.i.d. from D, " \
           "we can estimate φ(f, ) and µ(f, ) using the following standard estimators, assuming we can compute ρ: φ̂(" \
           "f, ,X) def = |{x∗ ∈ X | ρ(f,x∗) ≤ }| |X| µ̂(f, ,X) def = ∑ x∗∈X ρ(f,x∗)I[ρ(f,x∗) ≤ ] |{x∗ ∈ X | ρ(f," \
           "x∗) ≤ }| . An approximation ρ̂(f,x∗) ≈ ρ(f,x∗) of ρ, such as the one we describe in Section 4, " \
           "can be used in place of ρ. In practice, X is taken to be the test set Xtest.Consider the training points " \
           "in Figure 1 (a) colored based on the ground truth label. To classify this data, we train a two-layer " \
           "neural net f(x) = argmax`{(W2g(W1x))`}, where the ReLU function g is applied pointwise. Figure 1 (a) " \
           "includes contours of the per-point loss function of this neural net. Exhaustively searching the input " \
           "space to determine the distance ρ(f,x∗) to the nearest adversarial example for input x∗ (labeled `∗) is " \
           "intractable. Recall that neural nets with rectified-linear (ReLU) units as activations are piecewise " \
           "linear [15]. Since adversarial examples exist because of this linearity in the neural net [5], " \
           "we restrict our search to the region Z(x∗) around x∗ on which the neural net is linear. This region " \
           "around x∗ is defined by the activation of the ReLU function: for each i, if (W1x∗)i ≥ 0 (resp., " \
           "(W1x∗) ≤ 0), we constrain to the half-space {x | (W1x)i ≥ 0} (resp., {x | (W1x)i ≤ 0}). The intersection " \
           "of these half-spaces is convex, so it admits efficient search. Figure 1 (b) shows one such convex region " \
           "1. Additionally, x is labeled ` exactly when f(x)` ≥ f(x)`′ for each `′ 6= `. These constraints are " \
           "linear since f is linear on Z(x∗). Therefore, we can find the distance to the nearest input with label ` " \
           "6= `∗ by minimizing ‖x − x∗‖∞ on Z(x∗). Finally, we can perform this search for each label ` 6= `∗, " \
           "though for efficiency we take ` to be the label assigned the second-highest score by f . Figure 1 (b) " \
           "shows the adversarial example found by our algorithm in our running example. In Figure 1 note that the " \
           "direction of the nearest adversarial example is not necessary aligned with the signed gradient of the " \
           "loss function, as observed by others [7]. 1Our neural net has 8 hidden units, but for this x∗, " \
           "6 of the half-spaces entirely contain the convex region.We compute ρ(f, ) by expressing (1) as " \
           "constraints C, which consist of • Linear relations; specifically, inequalities C ≡ (wTx+ b ≥ 0) and " \
           "equalities C ≡ (wTx+ b = 0), where x ∈ Rm (for some m) are variables and w ∈ Rm, b ∈ R are constants. • " \
           "Conjunctions C ≡ C1 ∧ C2, where C1 and C2 are themselves constraints. Both constraints must be satisfied " \
           "for the conjunction to be satisfied. • Disjunctions C ≡ C1∨C2, where C1 and C2 are themselves " \
           "constraints. One of the constraints must be satisfied for the disjunction to be satisfied. The feasible " \
           "set F(C) of C is the set of x ∈ Rm that satisfy C; C is satisfiable if F(C) is nonempty. In the next " \
           "section, we show that the condition f(x) = ` can be expressed as constraints Cf (x, `); i.e., f(x) = ` if " \
           "and only if Cf (x, `) is satisfiable. Then, ρ(f, ) can be computed as follows: ρ(f,x∗) = min 6̀=`∗ ρ(f," \
           "x∗, `) (2) ρ(f,x∗, `) def = inf{ ≥ 0 | Cf (x, `) ∧ ‖x− x∗‖∞ ≤ satisfiable}. (3) The optimization problem " \
           "is typically intractable; we describe a tractable approximation in §4.We show how to encode the " \
           "constraint f(x) = ` as constraints Cf (x, `) when f is a neural net. We assume f has form f(x) = " \
           "argmax`∈L {[ f (k)(f (k−1)(...(f (1)(x))...)) ] ` } , where the ith layer of the network is a function f " \
           "(i) : Rni−1 → Rni , with n0 = n and nk = |L|. We describe the encoding of fully-connected and ReLU " \
           "layers; convolutional layers are encoded similarly to fully-connected layers and max-pooling layers are " \
           "encoded similarly to ReLU layers. We introduce the variables x(0), . . . ,x(k) into our constraints, " \
           "with the interpretation that x(i) represents the output vector of layer i of the network; i.e., " \
           "x(i) = f (i)(x(i−1)). The constraint Cin(x) ≡ (x(0) = x) encodes the input layer. For each layer f (i), " \
           "we encode the computation of x(i) given x(i−1) as a constraint Ci. Fully-connected layer. In this case, " \
           "x(i) = f (i)(x(i−1)) = W (i)x(i−1) + b(i), which we encode using the constraints Ci ≡ ∧ni j=1 { x (i) j " \
           "=W (i) j x (i−1) + b (i) j } , where W (i)j is the j-th row of W (i). ReLU layer. In this case, " \
           "x(i)j = max {x (i−1) j , 0} (for each 1 ≤ j ≤ ni), which we encode using the constraints Ci ≡ ∧ni j=1 Cij " \
           ", where Cij = (x (i−1) j <0 ∧ x (i) j =0) ∨ (x (i−1) j ≥ 0 ∧ x (i) j =x (i−1) j ). Finally, " \
           "the constraints Cout(`) ≡ ∧ `′ 6=` { x (k) ` ≥ x (k) `′ } ensure that the output label is `. Together, " \
           "the constraints Cf (x, `) ≡ Cin(x) ∧ (∧k i=1 Ci ) ∧ Cout(`) encodes the computation of f : Theorem 1 For " \
           "any x ∈ X and ` ∈ L, we have f(x) = ` if and only if Cf (x, `) is satisfiable.Convex restriction. The " \
           "challenge to solving (3) is the non-convexity of the feasible set of Cf (x, `). To recover tractability, " \
           "we approximate (3) by constraining the feasible set to x ∈ Z(x∗), where Z(x∗) ⊆ X is carefully chosen so " \
           "that the constraints Ĉf (x, `) ≡ Cf (x, `) ∧ (x ∈ Z(x∗)) have convex feasible set. We call Ĉf (x, " \
           "`) the convex restriction of Cf (x, `). In some sense, convex restriction is the opposite of convex " \
           "relaxation. Then, we can approximately compute robustness: ρ̂(f,x∗, `) def = inf{ ≥ 0 | Ĉf (x, " \
           "`) ∧ ‖x− x∗‖∞ ≤ satisfiable}. (4) The objective is optimized over x ∈ Z(x∗), which approximates the " \
           "optimum over x ∈ X . Choice of Z(x∗). We construct Z(x∗) as the feasible set of constraints D(x∗); i.e., " \
           "Z(x∗) = F(D(x∗)). We now describe how to construct D(x∗). Note that F(wTx+ b = 0) and F(wTx+ b ≥ 0) are " \
           "convex sets. Furthermore, if F(C1) and F(C2) are convex, then so is their conjunction F(C1 ∧ C2). " \
           "However, their disjunction F(C1 ∨ C2) may not be convex; for example, F((x ≥ 0) ∨ (y ≥ 0)). The potential " \
           "non-convexity of disjunctions makes (3) difficult to optimize. We can eliminate disjunction operations by " \
           "choosing one of the two disjuncts to hold. For example, note that for C1 ≡ C2 ∨ C3, we have both F(C2) ⊆ " \
           "F(C1) and F(C3) ⊆ F(C1). In other words, if we replace C1 with either C2 or C3, the feasible set of the " \
           "resulting constraints can only become smaller. Taking D(x∗) ≡ C2 (resp., D(x∗) ≡ C3) effectively replaces " \
           "C1 with C2 (resp., C3). To restrict (3), for every disjunction C1 ≡ C2 ∨C3, we systematically choose " \
           "either C2 or C3 to replace the constraint C1. In particular, we choose C2 if x∗ satisfies C2 (i.e., " \
           "x∗ ∈ F(C2)) and choose C3 otherwise. In our constraints, disjunctions are always mutually exclusive, " \
           "so x∗ never simultaneously satisfies both C2 and C3. We then take D(x∗) to be the conjunction of all our " \
           "choices. The resulting constraints Ĉf (x, `) contains only conjunctions of linear relations, " \
           "so its feasible set is convex. In fact, it can be expressed as a linear program (LP) and can be solved " \
           "using any standard LP solver. For example, consider a rectified linear layer (as before, max pooling " \
           "layers are similar). The original constraint added for unit j of rectified linear layer f (i) is( x (i−1) " \
           "j ≤ 0 ∧ x (i) j = 0 ) ∨ ( x (i−1) j ≥ 0 ∧ x (i) j = x (i−1) j ) To restrict this constraint, we evaluate " \
           "the neural network on the seed input x∗ and look at the input to f (i), which equals x(i−1)∗ = f (i−1)(" \
           "...(f (1)(x∗))...). Then, for each 1 ≤ j ≤ ni: D(x∗)← D(x∗) ∧ { x (i−1) j ≤ 0 ∧ x (i) j = x (i−1) j if (x " \
           "(i−1) ∗ )j ≤ 0 x (i−1) j ≥ 0 ∧ x (i) j = 0 if (x (i−1) ∗ )j > 0. Iterative constraint solving. We " \
           "implement an optimization for solving LPs by lazily adding constraints as necessary. Given all " \
           "constraints C, we start off solving the LP with the subset of equality constraints Ĉ ⊆ C, which yields a " \
           "(possibly infeasible) solution z. If z is feasible, then z is also an optimal solution to the original " \
           "LP; otherwise, we add to Ĉ the constraints in C that are not satisfied by z and repeat the process. This " \
           "process always yields the correct solution, since in the worst case Ĉ becomes equal to C. In practice, " \
           "this optimization is an order of magnitude faster than directly solving the LP with constraints C. Single " \
           "target label. For simplicity, rather than minimize over ρ(f,x∗, `) for each ` 6= `∗, we fix ` to be the " \
           "second most probable label f̃(x∗); i.e., ρ̂(f,x∗) def = inf{ ≥ 0 | Ĉf (x, f̃(x∗)) ∧ ‖x− x∗‖∞ ≤ " \
           "satisfiable}. (5) Approximate robustness statistics. We can use ρ̂ in our statistics φ̂ and µ̂ defined in " \
           "§2. Because ρ̂ is an overapproximation of ρ (i.e., ρ̂(f,x∗) ≥ ρ(f,x∗)), the estimates φ̂ and µ̂ may not " \
           "be unbiased (in particular, φ̂(f, ) ≤ φ(f, )). In §6, we show empirically that our algorithm produces " \
           "substantially less biased estimates than existing algorithms for finding adversarial examples.Finding " \
           "adversarial examples. We can use our algorithm for estimating ρ̂(f,x∗) to compute adversarial examples. " \
           "Given x∗, the value of x computed by the optimization procedure used to solve (5) is an adversarial " \
           "example for x∗ with ‖x− x∗‖∞ = ρ̂(f,x∗). Finetuning. We use fine-tuning to reduce a neural net’s " \
           "susceptability to adversarial examples. First, we use an algorithm A to compute adversarial examples for " \
           "each x∗ ∈ Xtrain and add them to the training set. Then, we continue training the network on a the " \
           "augmented training set at a reduced training rate. We can repeat this process multiple rounds (denoted T " \
           "); at each round, we only consider x∗ in the original training set (rather than the augmented training " \
           "set). Rounding errors. MNIST images are represented as integers, so we must round the perturbation to " \
           "obtain an image, which oftentimes results in non-adversarial examples. When fine-tuning, " \
           "we add a constraint x(k)` ≥ x (k) `′ + α for all ` ′ 6= `, which eliminates this problem by ensuring that " \
           "the neural net has high confidence on its adversarial examples. In our experiments, we fix α = 3.0. " \
           "Similarly, we modified the L-BFGS-B baseline so that during the line search over c, we only count x∗+r as " \
           "adversarial if x (k) ` ≥ x (k) `′ +α for all ` ′ 6= `. We choose α = 0.15, since larger α causes the " \
           "baseline to find significantly fewer adversarial examples, and small α results in smaller improvement in " \
           "robustness. With this choice, rounding errors occur on 8.3% of the adversarial examples we find on the " \
           "MNIST training set.We find adversarial examples for the neural net LeNet [12] (modified to use ReLUs " \
           "instead of sigmoids) trained to classify MNIST [11], and for the network-in-network (NiN) neural net [13] " \
           "trained to classify CIFAR-10 [9]. Both neural nets are trained using Caffe [8]. For MNIST, Figure 2 (b) " \
           "shows an adversarial example (labeled 1) we find for the image in Figure 2 (a) labeled 3, and Figure 2 (" \
           "c) shows the corresponding adversarial perturbation scaled so the difference is visible (it has L∞ norm " \
           "17). For CIFAR-10, Figure 2 (e) shows an adversarial example labeled “truck” for the image in Figure 2 (" \
           "d) labeled “automobile”, and Figure 2 (f) shows the corresponding scaled adversarial perturbation (which " \
           "has L∞ norm 3).We compare our algorithm for estimating ρ to the baseline L-BFGS-B algorithm proposed by [" \
           "21]. We use the tool provided by [22] to compute this baseline. For both algorithms, we use adversarial " \
           "target label ` = f̃(x∗). We use LeNet in our comparisons, since we find that it is substantially more " \
           "robust than the neural nets considered in most previous work (including [21]). We also use versions of " \
           "LeNet fine-tuned using both our algorithm and the baseline with T = 1, 2. To focus on the most severe " \
           "adversarial examples, we use a stricter threshold for robustness of = 20 pixels. We performed a similar " \
           "comparison to the signed gradient algorithm proposed by [5] (with the signed gradient multiplied by = 20 " \
           "pixels). For LeNet, this algorithm found only one adversarial example on the MNIST test set (out of 10," \
           "000) and four adversarial examples on the MNIST training set (out of 60,000), so we omit results 2. " \
           "Results. In Figure 3, we plot the number of test points x∗ for which ρ̂(f,x∗) ≤ , as a function of , " \
           "where ρ̂(f,x∗) is estimated using (a) the baseline and (b) our algorithm. These plots compare the " \
           "robustness of each neural network as a function of . In Table 1, we show results evaluating the " \
           "robustness of each neural net, including the adversarial frequency and the adversarial severity. The " \
           "running time of our algorithm and the baseline algorithm are very similar; in both cases, computing ρ̂(f," \
           "x∗) for a single input x∗ takes about 1.5 seconds. For comparison, without our iterative constraint " \
           "solving optimization, our algorithm took more than two minutes to run. Discussion. For every neural net, " \
           "our algorithm produces substantially higher estimates of the adversarial frequency. In other words, " \
           "our algorithm estimates ρ̂(f,x∗) with substantially better accuracy compared to the baseline. According " \
           "to the baseline metrics shown in Figure 3 (a), the baseline neural net (red) is similarly robust to our " \
           "neural net (blue), and both are more robust than the original LeNet (black). Our neural net is actually " \
           "more robust than the baseline neural net for smaller values of , whereas the baseline neural net " \
           "eventually becomes slightly more robust (i.e., where the red line dips below the blue line). This " \
           "behavior is captured by our robustness statistics—the baseline neural net has lower adversarial frequency " \
           "(so it has fewer adversarial examples with ρ̂(f,x∗) ≤ ) but also has worse adversarial severity (since " \
           "its adversarial examples are on average closer to the original points x∗). However, according to our " \
           "metrics shown in Figure 3 (b), our neural net is substantially more robust than the baseline neural net. " \
           "Again, this is reflected by our statistics—our neural net has substantially lower adversarial frequency " \
           "compared to the baseline neural net, while maintaining similar adversarial severity. Taken together, " \
           "our results suggest that the baseline neural net is overfitting to the adversarial examples found by the " \
           "baseline algorithm. In particular, the baseline neural net does not learn the adversarial examples found " \
           "by our algorithm. On the other hand, our neural net learns both the adversarial examples found by our " \
           "algorithm and those found by the baseline algorithm.We also implemented our approach for the for the " \
           "CIFAR-10 network-in-network (NiN) neural net [13], which obtains 91.31% test set accuracy. Computing ρ̂(" \
           "f,x∗) for a single input on NiN takes about 10-15 seconds on an 8-core CPU. Unlike LeNet, NiN suffers " \
           "severely from adversarial examples—we measure a 61.5% adversarial frequency and an adversarial severity " \
           "of 2.82 pixels. Our neural net (NiN fine-tuned using our algorithm and T = 1) has test set accuracy " \
           "90.35%, which is similar to the test set accuracy of the original NiN. As can be seen in Figure 3 (c), " \
           "our neural net improves slightly in terms of robustness, especially for smaller . As before, " \
           "these improvements are reflected in our metrics—the adversarial frequency of our neural net drops " \
           "slightly to 59.6%, and the adversarial severity improves to 3.88. Nevertheless, unlike LeNet, " \
           "our fine-tuned version of NiN remains very prone to adversarial examples. In this case, we believe that " \
           "new techniques are required to significantly improve robustness.We have shown how to formulate, " \
           "efficiently estimate, and improve the robustness of neural nets using an encoding of the robustness " \
           "property as a constraint system. Future work includes devising better approaches to improving robustness " \
           "on large neural nets such as NiN and studying properties beyond robustness. 2Futhermore, the signed " \
           "gradient algorithm cannot be used to estimate adversarial severity since all the adversarial examples it " \
           "finds have L∞ norm . "

# %%
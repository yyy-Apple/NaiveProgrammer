Reject ||| The authors try to address the issue of data efficiency in deep reinforcement learning by meta-learning a reinforcement learning algorithm using a hand-designed reinforcement learning algorithm (TRPO in this case). The experiments suggest comparable performance to models with prior knowledge of the distribution over environments for bandit tasks, and experiments on random maze navigation from vision is shown as well, though the random maze experiments would benefit from a clearer explanation. It was not obvious from the text how their experiments supported the thesis of the paper that the learned RL algorithm was effectively performing one-shot learning. The subject of the paper is also strikingly similar to the recently-posted paper Learning to Reinforcement Learn (https://arxiv.org/pdf/1611.05763.pdf), and while this paper was posted after the ICLR deadline, the authors should probably update the text to reflect the state of this rapidly-advancing field.
Reject ||| The problem of maximising total discounted reward across multiple trials of an unknown MDP (but sampled from a known distribution, e.g. multi-arm bandit) can be formulated as a POMDP problem. A single episode of the POMDP consists of multiple episodes of interaction with the underlying MDP during which the agent should efficiently explore and integrate information about the MDP to minimize reward across the whole trial.Using this observation (which is not original to this work), the authors compare using an existing RL algorithm (TRPO) to solve POMDPs against classic regret minimization methods on two classic tasks: multi-armed bandits and tabular MDPs. Additionally, they also demonstrate their approach can scale to a visual navigation task.While the comparison with classic regret minimization problems is useful, this paper has several weaknesses. It seems very confusing to introduce a new name (RL^2) for an existing class of algorithms (essentially any RL method for solving POMDPs). This terminology and the paper structure obscures to relationship between this and prior work.The comparison with prior work training RNNs is, while improved from the previous version, still lacking. The distinction the authors make, that prior work “focussed on memory aspect instead of fast RL”, seems somewhat arbitrary. The visual navigation task is conceptually identical to the water maze experiment [Heess et al, 2015] or Labyrinth navigation [Mnih et al, 2016]. These prior tasks require more than just memory, the also requires meta-learning such as exploration and demonstrate “fast RL” with the agent able to improve dramatically after a single episode.The introduction mentions the need for the use of priors on the environment to create agents which learn quickly and suggests that prior work in DeepRL is data inefficient. Yet, the recently prior work (e.g. previous paragraph) focussed on POMDPs demonstrated one-shot learning once trained (in the “fast RL” task to use the author’s terminology).Although the discussion highlights the potential for new algorithms and architectures which are structured to improve performance at these “multi-episode” tasks, no new algorithms or architectures are introduced.Unfortunately, because of the limited contribution, poor comparisons with prior work and confusing terminology this paper is not suitable for ICLR without substantial revision.
Reject ||| The paper proposes to use RL methods on sequences of episodes instead of single episodes. The underlying idea is the problem of 'learning to learn', and the experimental protocol proposed here allows one to understand how a neural network-based RL model can keep memory of past episodes in order to improve its ability to solve a particular problem. Experiments are made on bandit problems, but also on maze problems and show the interesting properties of such an approach, particularly on the maze problem where the agent seems to learn to first explore the maze, and then to exploit its knowledge to quickly find the goal. The paper is based on a very simple and natural idea which is acutally a good point. I really like the idea, and also the experiment on the maze which is very interesting. Experiments on bandits problem are less interesting since meta-learning models have been already proposed in the bandit problem with interesting results and the proposed model does not really bring additionnal information.  My main concerns is  based on the fact that the paper never clearly formally defines the problem that it attempts to solve. So, between the intuitive idea and the experimental results, the reader does not understand what  exactly the learning problem is, what is its impact and/or to which concrete application it belongs to. From my point of view, the article clearly lacks of maturity and does not bring yet a strong contribution to the field. Good:* Interesting experimental setting* Simple and natural idea* Nice maze experiments and model behaviourBad:* No real problem defined, only an intuition is given. Is it really useful ? For which problems ? What is the performance criterion one wants to optimize ? ...* Bandit experiments do not really bring relevant informations
Reject ||| The authors propose a transfer learning approach applied to a number of NLP tasks; the set of tasks appear to have an order in terms of complexity (from easy syntactic tasks to somewhat harder semantic tasks).Novelty: the way the authors propose to do transfer learning is by plugging models corresponding to each task, in a way that respects the known hierarchy (in terms of NLP "complexity") of those tasks. In that respect, the overall architecture looks more like a cascaded architecture than a transfer learning one. There are some existing literature in the area (first two Google results found: https://arxiv.org/pdf/1512.04412v1.pdf, (computer vision) and https://www.aclweb.org/anthology/P/P16/P16-1147.pdf (NLP)). In addition to the architecture, the authors propose a regularization technique they call "successive regularization".Experiments:- The authors performed a number of experimental analysis to clarify what parts of their architecture are important, which is very valuable;- The information "transferred" from one task to the next one is represented both using a smooth label embedding and the hidden representation of the previous task. At this point there is no analysis of which one is actually important, or if they are redundant (update: the authors mentioned they would add something there). Also, it is likely one would have tried first to feed label scores from one task to the next one, instead of using the trick of the label embedding -- it is unclear what the latter is actually bringing.- The successive regularization does not appear to be important in Table 8; a variance analysis would help to conclude.
Reject ||| this work investigates a joint learning setup where tasks are stacked based on their complexity. to this end, experimental evaluation is done on pos tagging, chunking, dependency parsing, semantic relatedness, and textual entailment. the end-to-end model improves over models trained solely on target tasks.although the hypothesis of this work is an important one, the experimental evaluation lacks thoroughness:first, a very simple multi-task learning baseline [1] should be implemented where there is no hierarchy of tasks to test the hypothesis of the tasks should be ordered in terms of complexity.second, since the test set of chunking is included in training data of dependency parsing, the results related to chunking with JMT_all are not informative. third, since the model does not guarantee well-formed dependency trees, thus, results in table 4 are not fair. minor issue:- chunking is not a word-level task although the annotation is word-level. chunking is a structured prediction task where we would like to learn a structured annotation over a sequence [2].[1] http://ronan.collobert.com/pub/matos/2011_nlp_jmlr.pdf[2] http://www.cs.cmu.edu/~nasmith/LSP/
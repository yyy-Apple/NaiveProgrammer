Summary : This paper proposes a method that saves memory and computation in the task of video prediction by low-rank tensor representations via tensor decomposition . The method is able to outperform standard convolutional lstm and other methods by using less parameters when testing it in the Moving MNIST and KTH datasets . The authors also present a proof to validate their method . Pros : + Interesting method for decomposing tensors operations in convolutional architectures + Outperforms immediate baseline ( Convolutional LSTM ) Weaknesses / comments : - Weak experimental section The authors mainly compare against Convolutional LSTM . The performance increase is there but the difference in parameters is not that significant in comparison to the performance . Needing fewer parameters is one of the claims in this paper and I am not fully convinced of the trade-off between the complexity of the model and the gain in parameter reduction / performance . In addition , the show videos do not look that much improved . The paper is also missing baselines from Villegas et al. , 2017 and Denton et al. , 2017 which both have available models for the KTH dataset . - No videos provided The paper does not provide any videos which is a must for video prediction papers . Judging the video quality from images in the paper is not easy , and also the used metrics have been shown to not be very objective in terms of video prediction quality or image generation in general . Conclusion : The proposed decomposition method is interesting , but the experimental section fails to convince me as to whether the methods performance validates the complicated formulations . My current score is between weak reject and reject so I will give a weak reject .
This paper presents a reinforcement learning approach towards using data that present best correlation with a validation set ’ s gradient signal . The broader point of this paper is that there is inevitably some distribution shift going from train to test set - and the validation set can be a small curated set whose distribution is closer to the testing distribution than what the training dataset 's distribution is . The problem setup bears relationship to several areas including domain adaptation/covariate shift problems , curriculum learning based approaches amongst others . One assumption that I see which needs to be understood more is equation ( 6 ) - wherein , somehow , there is a Markov assumption used to zero out the contribution of the scoring network on parameters unto previous time step . Trying to understand the implications of this assumption ( how the performance varies with/without this assumption ) would be instructive for understanding potential shortcomings of this framework . I think the paper is well written , handles an important question . That said , I am not too aware of recent work in this area to make a decisive judgement on this paper ’ s novelty/contributions .
The paper is fairly easily understandable in content . The question addressed in the work is very significant as is shown by multiple applications on real datasets in the paper . The reviewer is not aware of any prior work in connection to the question . This work appears to be a first of its kind . However , there are some issues which the reviewer is not clear on . 1 . The penalization criteria enforced leads to sharing of one active component among all connected components of positive weights , irrespective of the magnitude of the weightage , thereby reducing it to a single component mixture distribution for every connected subgraph of positive weights , which eliminates the need to use a mixture distribution in the first place . It is unclear why the use of mixtures could be useful therein . 2 . Although pairwise maximization for distributions does achieve the results in Proposition 1 , however , simultaneous maximization does not guarantee sparsity , unlike as pointed out by the authors . It has not been clearly explained why the results would continue to hold true for pairwise maximization setting . 3 . A regularizer based on the value of the weights rather than only on the sign could prove more effective .
Summary : This paper proposed conditional GAN models for text-to-video synthesis : ( 1 ) Text-feature-conditioned CNN filters is developed ; ( 2 ) A moving-shape dataset is constructed ; ( 3 ) Experimental results on video/image generation are shown with improved performance . Originality : ( 0 ) The overall model can be considered as MoCoGAN [ 0 ] , but with the text-conditioned information . Note that [ 0 ] also has a conditional version , but based on labels . ( 1 ) Generating CNN filters conditioned on other feature is not new for video generation , for example , Dynamic Filter Networks in [ 1 ] , where the CNN filters to produce next frame are generated . ( 2 ) The idea of creating moving shape datasets is explored and used in the previous papers [ 2 ] . This paper creates similar but a bit more comprehensive ones . The above three points to the existing work should be made CLEARLY , to reflect the contributions of this submission . Clarity : A . The current paper only reports the overall performance for a method with several `` new '' modifications : ( a ) TF , ( b ) ResNet-style architecture ( c ) Regularization . Please do some ablation study , and show which component helps solve the problem text-to-video . B . Following MoCoGAN [ 0 ] , the latent feature consists of two parts : one controls the global video ( content in [ 0 ] ) and one controls the frame-wise dynamics ( motion in [ 1 ] ) . It is okay to follow [ 0 ] . But , did the authors verify the two parts of latent codes can really control the claimed generation aspects ? Or , is it really necessary to have to the two separated parts for the problem ? if the goal is just to have text-controllable video ? Significance : Text-to-video is a very challenging task , it is encouraging to see the attempt on this problem . However , I hope each contribution to this important problem are concrete and solid . Therefore , it will much more convincing to study each part of `` contributions '' the more comprehensively ( one significant contribution would be enough ) , rather than put several minor/existing techniques together . Questions : A . The generated frames ( Figure 5 ) for one video are so similar , it raise a questions : does this model really generate video ( which captures the dynamics ) ? or it is just a model for the static image generation , with small perturbations ? If the answer is the ground-truth frames ( the employed training dataset ) are very similar , I would suggest to change the dataset at the first place , the employed dataset is not a proper dataset for text-to-VIDEO generation . B . [ 3 ] is also a text-to-video paper , the comparison to it is missing . Why ? References : [ 0 ] MoCoGAN : Decomposing motion and content for video generation , 2018 [ 1 ] Dynamic Filter Networks , 2016 [ 2 ] Visual Dynamics : Probabilistic Future Frame Synthesis via Cross Convolutional Networks , 2016 [ 3 ] To create what you tell : Generating videos from captions , 2017
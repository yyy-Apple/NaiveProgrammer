Understanding dynamics of videos and performing long-term predictions of the future is a highly challenging problem. It entails learning complex representation of real-world environment without external supervision. This arises in a wide range of applications, including autonomous driving, robot control (Finn & Levine, 2017), or other visual perception tasks like action recognition or object tracking (Alahi et al., 2016). However, long-term video prediction remains an open problem due to high complexity of the video contents. Therefore, prior works mostly focus on next or first few frames prediction (Lotter et al., 2016; Finn et al., 2016; Byeon et al., 2018). Many recent video models use Convolutional LSTM (ConvLSTM) as a basic block (Xingjian et al., 2015), where spatio-temporal information is encoded as a tensor explicitly in each cell. In ConvLSTM networks, each cell is a first-order recurrent model, where the hidden state is updated based on its immediate previous step. Therefore, they cannot easily capture higher-order temporal correlations needed for long-term prediction. Moreover, they are highly prone to error propagation. Various approaches have been proposed to augment ConvLSTM, either by modifying networks to explicitly modeling motion (Finn et al., 2016), or by integrating spatio-temporal interaction in ConvLSTM cells (Wang et al., 2017; 2018a). These approaches are often incapable of capturing longterm dependencies and produce blurry prediction. Another direction to augment ConvLSTM is to incorporate a higher-order RNNs (Soltani & Jiang, 2016) inside each LSTM cell, where its hidden state is updated using multiple past steps. However, a higher-order model for high-dimensional data (e.g. video) requires a huge number of model parameters, and the computation grows exponentially with the order of the RNNs. A principled approach to address the curse of dimensionality is tensor decomposition, where a higher-order tensor is compressed into smaller core tensors (Anandkumar et al., 2014). Tensor representations are powerful since they retain rich expressivity even with a small number of parameters. In this work, we propose a novel convolutional tensor decomposition, which allows for compact higher-order ConvLSTM. Contributions. We propose Convolutional Tensor-Train LSTM (Conv-TT-LSTM), a modification of ConvLSTM, to build a higher-order spatio-temporal model. (1) We introduce Convolutional Tensor-Train Decomposition (CTTD) that factorizes a large convolutional kernel into a chain of smaller tensors. (2) We integrate CTTD into ConvLSTM and propose Conv-TT-LSTM, which learns long-term dynamics in video sequence with a small number of model parameters. (3) We propose two versions of Conv-TT-LSTM: Fixed Window (FW) and Sliding Window (SW) (See Figures 1b and 1c), and we found that the SW version performs better than the FW one. (4) We found that training higher-order tensor models is not straightforward due to gradient instability. We present several approaches to overcome this such as good learning schedules and gradient clipping. (5) In the experiments, we show our proposed Conv-TT-LSTM consistently produces sharp prediction over a long period of time for both Moving-MNIST-2 and KTH action datasets. Conv-TT-LSTM outperforms the state-of-the-art PredRNN++ (Wang et al., 2018a) in LPIPS (Zhang et al., 2018) by 0.050 on the Moving-MNIST-2 and 0.071 on the KTH action dataset, with 5.6 times fewer parameters. Thus, we obtain best of both worlds: better long-term prediction and model compression.Tensor Decomposition In machine learning, tensor decompositions, including CP decomposition (Anandkumar et al., 2014), Tucker decomposition (Kolda & Bader, 2009), and tensor-train decomposition (Oseledets, 2011), are widely used for dimensionality reduction (Cichocki et al., 2016) and learning probabilistic models (Anandkumar et al., 2014). In deep learning, prior works focused on their application in model compression, where the parameters tensors are factorized into smaller tensors. This technique has been used in compressing convolutional networks (Lebedev et al., 2014; Kim et al., 2015; Novikov et al., 2015; Su et al., 2018; Kossaifi et al., 2017; Kolbeinsson et al., 2019; Kossaifi et al., 2019), recurrent networks (Tjandra et al., 2017; Yang et al., 2017) and transformers (Ma et al., 2019). Specifically, Yang et al. (2017) demonstrates that the accuracy of video classification increases if the parameters in recurrent networks are compressed by tensor-train decomposition (Oseledets, 2011). Yu et al. (2017) used tensor-train decomposition to constrain the complexity of higher-order LSTM, where each next step is computed based on the outer product of previous steps. While this work only considers vector input at each step, we extend their approach to higher-order ConvLSTM, where each step also encodes spatial information. Video Prediction Prior works on video prediction have focused on several directions: predicting short-term video (Lotter et al., 2016; Byeon et al., 2018), decomposing motion and contents (Finn et al., 2016; Villegas et al., 2017; Denton et al., 2017; Hsieh et al., 2018), improving the objective function Mathieu et al. (2015), and handling diversity of the future (Denton & Fergus, 2018; Babaeizadeh et al., 2017; Lee et al., 2018). Many of these works use Convolutional LSTM (ConvLSTM) (Xingjian et al., 2015) as a base module, which deploys 2D convolutional operations in LSTM to efficiently exploit spatio-temporal information. Finn et al. (2016) used ConvLSTM to model pixel motion. Some works modified the standard ConvLSTM to better capture spatio-temporal correlations (Wang et al., 2017; 2018a). Wang et al. (2018b) integrated 3D convolutions into ConvLSTM. In addition, current cell states are combined with its historical records using self-attention to efficiently recall the history information. Byeon et al. (2018) applied ConvLSTM in all possible directions to capture full contexts in video and also demonstrated strong performance using a deep ConvLSTM network as a baseline. This baseline is adapted to obtain the base architecture in the present paper.The goal of tensor decomposition is to represent a higher-order tensor as a set of smaller and lowerorder core tensors, with fewer parameters while preserve essential information. In Yu et al. (2017), tensor-train decomposition (Oseledets, 2011) is used to reduce both parameters and computations in higher-order recurrent models, which we review in the first part of this section. However, the approach in Yu et al. (2017) only considers recurrent models with vector inputs and cannot cope with image inputs directly. In the second part, we extend the standard tensor-train decomposition to convolutional tensor-train decomposition (CTTD). With CTTD, a large convolutional kernel is factorized into a chain of smaller kernels. We show that such decomposition can reduce both parameters and operations of higher-order spatio-temporal recurrent models. Standard Tensor-train decomposition Given an m-order tensor T ∈ RI1×···×Im , where Il is the dimension of its l-th order, a standard tensor-train decomposition (TTD) (Oseledets, 2011) factorizes the tensor T into a set of m core tensors {T (l)}ml=1 with T (l) ∈ RIl×Rl×Rl+1 such that Ti1,··· ,im , R1∑ r1=1 · · · Rm−1∑ rm−1=1 T (1)i1,1,r1 T (2) i2,r1,r2 · · · T (m)im,rm−1,1 (1) where tensor-train ranks {Rl}ml=0 (with R0 = Rm = 1) control the number of parameters in the tensor-train format Eq.(1). With TTD, the original tensor T of size ( ∏m l=1 Il) is compressed to ( ∑m l=1 IlRl−1Rl) entries, which grows linearly with the order m (assuming Rl’s are constant). Therefore, TTD is commonly used to approximate higher-order tensors with fewer parameters. The sequential structure in tensor-train decomposition makes it particularly suitable for sequence modeling (Yu et al., 2017). Consider a higher-order recurrent model that predicts a scalar output v ∈ R based on the outer product of a sequence of input vectors {u(l) ∈ RIl}ml=1 according to: v = 〈 T , ( u(1) ⊗ · · · ⊗ u(m) )〉 = I1∑ i1=1 · · · Im∑ im=1 Ti1,··· ,im u (1) i1 · · · u(m)im (2) This model is intractable in practice since the number of parameters in T ∈ RI1×···Im (and therefore computational complexity of Eq. (2)) grows exponentially with the order m. Now suppose T takes a tensor-train format as in Eq. (1), we prove in Appendix A that (2) can be efficiently computed as v(l)rl = Il∑ il=1 Rl∑ rl−1=1 T (l)il,rl−1,rl v (l−1) rl−1 u (l) il , ∀l ∈ [m] (3) where the vectors {v(l) ∈ RRl}ml=1 are the intermediate steps, with v(0) ∈ R initialized as v(0) = 1, and final output v = v(m). Notice that the higher-order tensor T is never reconstructed in the sequential process in Eq. (3), therefore both space and computational complexities grow linearly (not exponentially compared to Eq. (2))with the order m assuming all tensor-train ranks are constants. Convolutional Tensor-Train Decomposition A convolutional layer in neural network is typically parameterized by a 4-th order tensor T ∈ RK×K×Rm×R0 , where K is the kernel size, Rm and R0 are the number of input and output channels respectively. Suppose the kernel size K takes the form K = m(k − 1) + 1 (e.g. K = 7 and m = 3, k = 3), a convolutional tensor-train decomposition (CTTD) factorizes T into a set of m core tensors {T (l)}ml=1 with T (l) ∈ Rk×k×Rl×Rl−1 such that T:,:,rm,r0 , R1∑ r1=1 · · · Rm−1∑ rm−1=1 T (1):,:,r1,r0 ∗ T (2) :,:,r2,r1 ∗ · · · ∗ T (m) :,:,rm,rm−1 (4) where ∗ denotes convolution between 2D-filters, and {Rl}ml=1 are the convolutional tensor-train ranks that control the complexity of the convolutional tensor-train format in Eq. (4). With CTTD, the number of parameters in the decomposed format reduces from K2R0Rm to (∑m l=1 k 2Rl−1Rl ) . Similar to standard TTD, its convolutional counterpart can also be used to compress higher-order spatio-temporal recurrent models with convolutional operations. Consider a model that predicts a 3-rd order feature V ∈ RH×W×R0 based on a sequence of 3-rd features {U (l) ∈ RH×W×Rl}ml=1 (where H , W are height/width of the features and Rl is the number of channels in U (l)) such that V:,:,r0 = m∑ l=1 W(l):,:,rl,r0 ∗ U (l) :,:,rl , withW(l) = CTTD ( {T (l)}ml=k ) ,∀l ∈ [m] (5) whereW(l) ∈ R[l(k−1)+1]×[l(k−1)+1]×Rl×R0 is the corresponding weights tensor for U (l). Suppose each W(l) takes a convolutional tensor-train format in Eq. (4), we prove in Appendix A that the model in Eq. (5) can be computed sequentially similarly without reconstructing the originalW(l)’s: V(l−1):,:,rl−1 = Rl∑ rl=1 T (l):,:,rl,rl−1 ∗ ( V(l):,:,rl + U (l) :,:,rl ) , ∀l ∈ [m] (6) where {V(l) ∈ RH×W×Rl}ml=1 are intermediate results of the sequential process, where V(m) ∈ RH×W×Rm is initialized as all zeros and final prediction V = V(0). The operations in Eq. (5) is illustrated in Figure 1a. In this paper, we denote the Eq.(5) simply as V = CTT({T (l)}ml=1, {U (l)}ml=1).Convolutional LSTM is a basic block for most recent video forecasting models (Xingjian et al., 2015), where the spatial information is encoded explicitly as tensors in the LSTM cells. In a ConvLSTM network, each cell is a first-order Markov model, i.e. the hidden state is updated based on its previous step. In this section, we propose convolutional tensor-train LSTM, where convolutional tensor-train is incorporated to model multi-steps spatio-temporal correlation explicitly. Notations. In this section, the symbol ∗ is overloaded to denote convolution between higher-order tensors. For instance, given a 4-th order weights tensor W ∈ RK×K×S×C and a 3-rd order input tensor X ∈ RH×W×S , Y = W ∗ X computes a 3-rd output tensor Y ∈ RH×W×T as Y:,:,c =∑ s=1W:,:,s,c ∗ X:,:,s. The symbol ◦ is used to denote element-wise product between two tensors, and σ represents a function that performs element-wise (nonlinear) transformation on a tensor. Convolutional LSTM Xingjian et al. (2015) extended fully-connected LSTM (FC-LSTM) to Convolutional LSTM (ConvLSTM) to model spatio-temporal structures within each recurrent unit, where all features are encoded as 3-rd order tensors with dimensions (height × width × channels) and matrix multiplications are replaced by convolutions between tensors. In a ConvLSTM cell, the parameters are characterized by two 4-th order tensorsW ∈ RK×K×S×4C and T ∈ RK×K×C×4C , where K is the kernel size of all convolutions and S and C are the numbers of channels of the input X (t) ∈ RH×W×S and hidden states H(t) ∈ RH×W×C respectively. At each time step t, a ConvLSTM cell updates its hidden states H(t) ∈ RH×W×C based on the previous step H(t−1) and the current input X (t), where H and W are the height/width that are the same for X (t) andH(t).[ I(t);F (t); C̃(t);O(t) ] = σ ( W ∗ X (t) + T ∗ H(t−1) ) (7) C(t) = C̃(t) ◦ I(t); H(t) = O(t) ◦ C(t) (8) where σ(·) applies sigmoid on the input gate I(t), forget gate F (t), output gateO(t), and hyperbolic tangent on memory cell C̃(t). Note that all tensors C(t), I(t), F (t), O(t) ∈ RH×W×C are 3-rd order. Convolutional Tensor-Train LSTM In Conv-TT-LSTM, we introduce a higher-order recurrent unit to capture multi-steps spatio-temporal correlations in LSTM, where the hidden state H(t) is updated based on its n previous steps {H(t−l)}nl=1 with anm-order convolutional tensor-train (CTT) as in Eq. (5). Concretely, suppose the parameters in CTT are characterized bym tensors of 4-th order {T (o)}mo=1, Conv-TT-LSTM replaces Eq. (7) in ConvLSTM by two equations: H̃(t,o) = f ( K(o), {H(t−l)}nl=1 ) ,∀o ∈ [m] (9)[ I(t);F (t); C̃(t);O(t) ] = σ ( W ∗ X (t) + CTT ( {T (o)}mo=1, {H̃(t,o)}mo=1 )) (10) (1) Since CCT({T (l)}ml=1, ·) takes a sequence ofm tensors as inputs, the first step in Eq. (9) maps the n inputs {H(t−l)}nl=1 to m intermediate tensors {H(t,o)}mo=1 with a function f . (2) These m tensors {H̃(t,o)}mo=1 are then fed into CCT({T (l)}ml=1, ·) and compute the gates according to Eq. (10). We propose two realizations of Eq. (9), where the first realization uses a fixed window of {H(t−l)}nl=1 to compute each H̃(t,o), while the second one adopts a sliding window strategy. At each step, the Conv-TT-LSTM model computesH(t) by replacing Eq. (9) by either Eq. (11a) or (11b). Conv-TT-LSTM-FW: H̃(t,o) = K(o) ∗ Ĥ(t,o) = K(o) ∗ [ H(t−n); · · · ;H(t−1) ] (11a) Conv-TT-LSTM-SW: H̃(t,o) = K(o) ∗ Ĥ(t,o) = K(o) ∗ [ H(t−n+m−l); · · · ;H(t−l) ] (11b) In the fixed window version, the previous steps {H(l)}nl=1 are concatenated into a 3-rd order tensor Ĥ(t,o) ∈ RH×W×nC , which is then mapped to a tensor H̃(t,o) ∈ RH×W×R by 2D-convolution with a kernel K(l) ∈ Rk×k×nC×R. And in the sliding window version, {H(l)}nl=1 are concatenated into a 4-th order tensor Ĥ(t,o) ∈ RH×W×D×C (with D = n − m + 1), which is mapped to H̃(t,o) ∈ RH×W×R by 3D-convolution with a kernel K(l) ∈ Rk×k×D×R. For later reference, we name the model with Eqs.(11a) and (10) as Conv-TT-LSTM-FW and the one with Eqs.(11b) and (10) as Conv-TT-LSTM-SW. Figure 1b and Figure 1c visualize the difference between these two variants.We first evaluate our approach extensively on the synthetic Moving-MNIST-2 dataset (Srivastava et al., 2015). In addition, we use KTH human action dataset (Laptev et al., 2004) to test the performance of our models in more realistic scenario. Model Architecture All experiments use a stack of 12-layers of ConvLSTM or Conv-TT-LSTM with 32 channels for the first and last 3 layers, and 48 channels for the 6 layers in the middle. A convolutional layer is applied on top of all LSTM layers to compute the predicted frames. Following Byeon et al. (2018), two skip connections performing concatenation over channels are added between (3, 9) and (6, 12) layers. Illustration of the network architecture is included in the appendix. All parameters are initialized by Xavier’s normalized initializer (Glorot & Bengio, 2010) and initial states in ConvLSTM or Conv-TT-LSTM are initialized as zeros. Evaluation Metrics We use two traditional metrics MSE (or PSNR) and SSIM (Wang et al., 2004), and a recently proposed deep-learning based metric LPIPS (Zhang et al., 2018), which measures the similarity between deep features. Since MSE (or PSNR) is based on pixel-wise difference, it favors vague and blurry predictions, which is not a proper measurement of perceptual similarity. While SSIM was originally proposed to address the problem, Zhang et al. (2018) shows that their proposed LPIPS metric aligns better to human perception. Learning Strategy All models are trained with ADAM optimizer (Kingma & Ba, 2014) with L1 + L2 loss. Learning rate decay and scheduled sampling (Bengio et al., 2015) are used to ease training. Scheduled sampling is started once the model does not improve in 20 epochs (in term of validation loss), and the sampling ratio is decreased linearly from 1 until it reaches zero (by 2 × 10−4 each epoch for Moving-MNIST-2 and 5× 10−4 for KTH). Learning rate decay is further activated if the loss does not drop in 20 epochs, and the rate is decreased exponentially by 0.98 every 5 epochs. Hyper-parameters Selection We perform a wide range of hyper-parameters search for Conv-TTLSTM to identify the best model, and Table 1 summarizes our search values. The initial learning rate of 10−3 is found for the models of kernel size 3 and 10−4 for the models of kernel size 5. We found that Conv-TT-LSTM models suffer from exploding gradients when learning rate is high (e.g. 10−3 in our experiments), therefore we also explore various gradient clipping values and select 1 for all Conv-TT-LSTM models. All hyper-parameters are selected using the best validation performance.The Moving-MNIST-2 dataset is generated by moving two digits of size 28× 28 in MNIST dataset within a 64 × 64 black canvas. These digits are placed at a random initial location, and move with constant velocity in the canvas and bounce when they reach the boundary. Following Wang et al. (2018a), we generate 10,000 videos for training, 3,000 for validation, and 5,000 for test with default parameters in the generator1. All our models are trained to predict 10 frames given 10 input frames. Multi-Steps Prediction Table 2 reports the average statistics for 10 and 30 frames prediction, and Figure 2 shows comparison of per-frame statistics for PredRNN++ model, ConvLSTM baseline and our proposed Conv-TT-LSTM models. (1) Our Conv-TT-LSTM models consistently outperform the 1 https://github.com/jthsieh/DDPAE-video-prediction/blob/master/data/moving_mnist.py 2The results are cited from the original paper, where the miscalculation of MSE is corrected in the table. 3The results are reproduced from https://github.com/Yunbo426/predrnn-pp with the same datasets in this paper. The original implementation crops each frame into patches as the input to the model. We find out such pre-processing is unnecessary and the performance is better than the original paper. 12-layer ConvLSTM baseline for both 10 and 30 frames prediction with fewer parameters; (2) The Conv-TT-LSTMs outperform previous approaches in terms of SSIM and LPIPS (especially on 30 frames prediction), with less than one fifth of the model parameters. We reproduce the PredRNN++ model (Wang et al., 2018a) from their source code2, and we find that (1) The PredRNN++ model tends to output vague and blurry results in long-term prediction (especially after 20 steps). (2) and our Conv-TT-LSTMs are able to produce sharp and realistic digits over all steps. An example of comparison for different models is shown in Figure 3. The visualization is consistent with the results in Table 2 and Figure 2. Ablation Study To understand whether our proposed Conv-TT-LSTM universally improves upon ConvLSTM (i.e. not tied to specific architecture, loss function and learning schedule), we perform three ablation studies: (1) Reduce the number of layers from 12 layers to 4 layers (same as Xingjian et al. (2015) and Wang et al. (2018a)); (2) Change the loss function from L1 + L2 to L1 only; (3) Disable the scheduled sampling and use teacher forcing during training process. We evaluate the ConvLSTM baseline and our proposed Conv-TT-LSTM in these three settings, and summarize their comparisons in Table 3. The results show that our proposed Conv-TT-LSTM outperforms ConvLSTM consistently for all settings, i.e. the Conv-TT-LSTM model improves upon ConvLSTM in a board range of setups, which is not limited to the certain setting used in our paper. These ablation studies further show that our setup is optimal for predictive learning in Moving-MNIST-2.KTH action dataset (Laptev et al., 2004) contains videos of 25 individuals performing 6 types of actions on a simple background. Our experimental setup follows Wang et al. (2018a), which uses persons 1-16 for training and 17-25 for testing, and each frame is resized to 128 × 128 pixels. All our models are trained to predict 10 frames given 10 input frames. During training, we randomly select 20 contiguous frames from the training videos as a sample and group every 10,000 samples into one epoch to apply the learning strategy as explained at the beginning of this section. Results In Table 4, we report the evaluation on both 20 and 40 frames prediction. (1) Our models are consistently better than the ConvLSTM baseline for both 20 and 40 frames prediction. (2) While our proposed Conv-TT-LSTMs achieve lower SSIM value compared to the state-of-the-art models in 20 frames prediction, they outperform all previous models in LPIPS for both 20 and 40 frames prediction. An example of the predictions by the baseline and Conv-TT-LSTMs is shown in Figure 3.In this paper, we proposed convolutional tensor-train decomposition to factorize a large convolutional kernel into a set of smaller core tensors. We applied this technique to efficiently construct convolutional tensor-train LSTM (Conv-TT-LSTM), a high-order spatio-temporal recurrent model whose parameters are represented in tensor-train format. We empirically demonstrated that our proposed Conv-TT-LSTM outperforms standard ConvLSTM and produce better/comparable results compared to other state-of-the-art models with fewer parameters. Utilizing the proposed model for high-resolution videos is still challenging due to gradient vanishing or explosion. Future direction will include investigating other training strategies or a model design to ease the training process. 4 Wang et al. (2018b) mentions that the number of parameters is similar to PredRNN++ (Wang et al., 2018a).In this section, we prove the sequential algorithms in Eq. (3) for tensor-train decomposition (1) and Eq. (6) for convolutional tensor-train decomposition (4) both by induction. Proof of Eq. (3) For simplicity, we denote the standard tensor-train decomposition in Eq. (1) as T = TTD({T (l)}ml=1), then Eq. (2) can be rewritten as Eq. (12) since R0 = 1 and v (0) 1 = 1. v = R0∑ r0=1 I1∑ i1=1 · · · Im∑ im=1 TTD ( {T (l)}ml=1 ) i1,··· ,im v(0)r0 ( u(1) ⊗ · · · ⊗ u(m) ) i1,··· ,im (12) = R0∑ r0=1 I1∑ i1=1 · · · Im∑ im=1  R1∑ r1=1 · · · Rm−1∑ rm−1=1 T (1)i1,r0,r1 · · · T (m) im,rm−1,rm  v(0)r0 u(1)i1 · · ·u(m)im (13) = R1∑ r1=1 I2∑ i2=1 · · · Im∑ im=1  R2∑ r2=1 · · · Rm−1∑ rm−1=1 T (2)i2,r1,r2 · · · T (m) im,rm−1,rm  ( R0∑ r0=1 I1∑ i1=1 T (1)i1,r0,r1 v (0) r0 u (1) i1 ) u (2) i2 · · ·u(m)im (14) = R1∑ r1=1 I2∑ i2=1 · · · Im∑ im=1 TTD ( {T (l)}ml=2 ) i1,··· ,im v(1)r1 ( u(2) ⊗ · · · ⊗ u(m) ) i2,··· ,im (15) where R0 = 1, v (0) 1 = 1 and the sequential algorithm in Eq. (3) is performed at Eq. (14). Proof of Eq. (6) For simplicity, we denote the convolutional tensor-train decomposition in Eq. (4) as T = CTTD(T (l))ml=1, then Eq. (5) can be rewritten as (16) since V(m) is an all zeros tensor. V:,:,r0 = m∑ l=1 Rl∑ rl=1 CTTD ( {T (t)}lt=1 ) :,:,rl,r0 ∗ U (l):,:,rl+ Rm∑ rm=1 CTTD ( {T (t)}mt=1 ) :,:,rm,r0 ∗ V(m):,:,rm (16) = m−1∑ l=1 Rl∑ rl=1 CTTD ( {T (t)}lt=1 ) :,:,rl,r0 ∗ U (l):,:,rl+ Rm∑ rm=1 CTTD ( {T (t)}mt=1 ) :,:,rm,r0 ∗ ( U (m):,:,rm + V (m) :,:,rm ) (17) Note that the second term in Eq. (17) can now be simplified as Rm∑ rm=1 CTTD ( {T (t)}mt=1 ) :,:,rm,r0 ∗ ( U (m):,:,rm + V (m) :,:,rm ) (18) = Rm∑ rm=1  R1∑ r1=1 · · · Rm−1∑ rm−1=1 T (1):,:,r1,r0 ∗ · · · ∗ T (m) :,:,rm,rm−1  ∗ (U (m):,:,rm + V(m):,:,rm) (19) = Rm−1∑ rm−1=1  R1∑ r1=1 · · · Rm−1∑ rm−1=1 T (1):,:,r1,r0 ∗ · · · ∗ T (m−1) :,:,rm−1,rm−2  ∗ [ Rm∑ rm=1 T (m):,:,rm,rm−1 ∗ ( U (m):,:,rm + V (m) :,:,rm )] (20) = Rm−1∑ rm−1=1 CTTD ( {T (t)}m−1t=1 ) :,:,rm−1,r0 ∗ V(m−1):,:,rm−1 (21) where the sequential algorithm in Eq. (5) is performed to achieve Eq. (21) from Eq. (20). Plugging Eq. (21) into Eq. (17), we reduce Eq. (17) back to the form as Eq. (16). V:,:,r0 = m−1∑ l=1 Rl∑ rl=1 CTTD ( {T (t)}lt=1 ) :,:,rl,r0 ∗ U (l):,:,rl+ Rm∑ rm=1 CTTD ( {T (t)}m−1t=1 ) :,:,rm−1,r0 ∗ V(m−1):,:,rm−1 (22) which completes the induction.All experiments use a stack of 12-layers of ConvLSTM or Conv-TT-LSTM with 32 channels for the first and last 3 layers, and 48 channels for the 6 layers in the middle. A convolutional layer is applied on top of all LSTM layers to compute the predicted frames, followed by an optional sigmoid function (In the experiments, we add sigmoid for KTH dataset but not for Moving-MNIST-2). Additionally, two skip connections performing concatenation over channels are added between (3, 9) and (6, 12) layers as is shown in Figure 5.
While deep learning models are remarkably good at fitting large data sets, their performance is also highly sensitive to the structure and domain of their training data. Training on out-of-domain data can lead to worse model performance, while using more relevant data can assist transfer learning. Previous work has attempted to create strategies to handle this sensitivity by selecting subsets of the data to train the model on (Jiang & Zhai, 2007; Wang et al.; Axelrod et al., 2011; Moore & Lewis, 2010), providing different weights for each example (Sivasankaran et al., 2017; Ren et al., 2018), or changing the presentation order of data (Bengio et al., 2009; Kumar et al., 2019). However, there are several challenges with the existing work on better data usage strategies. Most work data filtering criterion or training curriculum rely on domain-specific knowledge and handdesigned heuristics, which can be sub-optimal. To avoid hand designed heuristics, several works propose to optimize a parameterized neural network to learn the data usage schedule, but most of them are tailored to specific use cases, such as handling noisy data for classification (Jiang et al., 2018), learning a curriculum learning strategy for NMT (Kumar et al., 2019), and actively selecting data for annotation (Fang et al., 2017; Wu et al., 2018). Fan et al. (2018) proposes a more general teacher-student framework that first trains a teacher network to select data that directly optimizes development set accuracy over multiple training runs. However, because running multiple runs of training simply to train this teacher network entails an n-fold increase in training time for n runs, this is infeasible in many practical settings. In addition, in preliminary experiments we also found the single reward signal provided by dev set accuracy at the end of training noisy to the extent that we were not able to achieve results competitive with simpler heuristic training methods. In this paper, we propose an alternative: a general Reinforcement Learning (RL) framework for optimizing training data usage by training a scorer network that minimizes the model loss on the development set. We formulate the scorer network as a function of the current training examples only, making it possible to re-use the model architecture which is designed and trained for the main task. 1We will make the code publicly available upon acceptance. Thus, our method requires no heuristics and is generalizable to various tasks. To make the scorer adaptive, we perform frequent and efficient updates of the scorer network using a reward function inspired by recent work on learning using data from auxiliary tasks (Du et al., 2018; Liu et al., 2019b), which use the similarity between two gradients as a measure of task relevance. We propose to use the gradient alignment between the training examples and the dev set as a reward signal for a parametric scorer network, as illustrated in Figure 1. We then formulate our framework as an optimization problem found in many prior works such as meta-learning (Finn et al., 2017), noisy data filtering (Ren et al., 2018), and neural architecture search (Liu et al., 2019a), and demonstrate that our proposed update rules follow a direct differentiation of the scorer parameters to optimize the model loss on the dev set. Thus we refer to our framework as “Differentiable Data Selection” (DDS). We demonstrate two concrete instantiations of the DDS framework, one for a more general case of image classification, and the other for a more specific case of neural machine translation (NMT). For image classification, we test on both CIFAR-10 and ImageNet. For NMT, we focus on a multilingual setting, where we optimize data usage from a multilingual corpus to improve the performance on a particular language. For these two very different and realistic tasks, we find the DDS framework brings significant improvements over the baselines for all settings.Commonly in machine learning, we seek to find the parameters θ∗ that minimize the risk J(θ, P ), the expected value of a loss function `(x, y; θ), where 〈x, y〉 are pairs of inputs and associated labels sampled from a particular distribution P (X,Y ): θ∗ = argmin θ J(θ, P ) where J(θ, P ) = Ex,y∼P (X,Y )[`(x, y; θ)] (1) Ideally, we would like the risk J(·) to be minimized over the data distribution that our system sees at test time, ie. Ptest(X,Y ). Unfortunately, this distribution is unknown at training time, so instead we collect a training set Dtrain = {(xi, yi) : i = 1, ..., Ntrain} with distribution Ptrain(X,Y ) = Uniform(Dtrain), and minimize the empirical risk by taking 〈x, y〉 ∼ Ptrain(X,Y ). Since we need a sufficiently large training set Dtrain to train a good model, it is hard to ensure that Ptrain(X,Y ) ≈ Ptest(X,Y ). In fact, we often accept that training data comes from a different distribution than test data. The discrepancy between Ptrain(X,Y ) and Ptest(X,Y ) manifests itself in the form of problems such as overfitting (Zhang et al., 2017; Srivastava et al., 2014), covariate shift (Shimodaira, 2000), and label shift (Lipton et al., 2018). However, unlike the large training set, we can collect a relatively small development set Ddev = {(xi, yi) : i = 1, ..., Ndev} with distribution Pdev(X,Y ) which is much closer to Ptest(X,Y )2. Since Ddev is a better approximation of our test-time scenario3, we can use Ddev to get reliable feedback to learn to better utilize our training data from Dtrain. In particular, we propose to train a scorer network, parameterized by ψ, to provide guidance on training data usage to minimize J(θ,Ddev) .We propose to optimize the scorer’s parameters ψ in an RL setting. Our environment is the model state θ and an example 〈x, y〉. Our RL agent is the scorer network ψ, which optimizes the data usage 2As is standard in machine learning experiments, we make sure that Ddev has no overlap with Dtrain or Dtest. Details of how we construct the Ddev can be found in Appendix A.2 and A.4. 3For example, in Section 3.2 we would like to use training data from many different languages to improve the performance of a particular low-resource language. Here we can gather a small set of Ddev from the low resource language, even if we can’t gather a large training set in the language. Moreover, in a domain adaptation setting we can obtain a small dev set in the target domain, or in a setting of training on noisy data we can often obtain a small clean dev set. for the current model state. The agent’s reward on picking an example approximates the dev set performance of the resulting model after the model is updated on this example. Our scorer network is parameterized as a differentiable function that only takes as inputs the features of the example 〈x, y〉. Intuitively, it represents a distribution over the training data where more important data has a higher probability of being used, denoted P (X,Y ;ψ). Unlike prior methods which generally require complicated featurization of both the model state and the data as input to the RL agent (Fan et al., 2018; Jiang et al., 2018; Fang et al., 2017), our formulation is much simpler and generalizable to different tasks. Since our scorer network does not consider the model parameters θt as input, we update it iteratively with the model so that at training step t, P (X,Y ;ψt) provides an up-to-date data scoring feedback for a given θt. Although the above formulation is simpler and more general, it requires much more frequent updates to the scorer parameter ψ. Existing RL frameworks simply use the change in dev set risk as the regular reward signal, which makes the update expensive and unstable (Fan et al., 2018; Kumar et al., 2019). Therefore, we propose a novel reward function as an approximation to ∆Jdev(x, y) to quantify the effect of the training example 〈x, y〉. Inspired by Du et al. (2018) (which uses gradient similarity between two tasks to measure the adaptation effect between them, we use the agreement between the model gradient on data 〈x, y〉 and the gradient on the dev set to approximate the effect of 〈x, y〉 on dev set performance. This reward simply implies that we prefer data that moves θ in the direction that minimizes the dev set risk: R(x, y) = ∆Jdev(x, y) ≈ ∇θ`(x, y; θt−1)> · ∇θJ(θt,Ddev) (2) According to the REINFORCE algorithm (Williams, 1992), the update rule for ψ is thus ψt+1 ← ψt +∇θ`(x, y; θt−1) · ∇θJ(θt,Ddev)︸ ︷︷ ︸ R(x,y) ∇ψlog(P (X,Y ;ψ)) (3) The update rule for the model is simply θt ← θt−1 −∇θJ(θt−1, P (X,Y ;ψ)) (4) For simplicity of notation, we omit the learning rate term. Full derivation can be found in Appendix A.1. By alternating between Eqn. 4 and Eqn. 3, we can iteratively update θ using the guidance from the scorer network, and update ψ to optimize the scorer using feedback from the model. Our formulation of scorer network as P (X,Y ;ψ) has several advantages. First, it provides the flexibility that we can either sample a training instance or equivalently scale the update from the training instance based on its score. Specifically, we provide an algorithm under the DDS framework for multilingual NMT (see Sec. 3.2), where the former is more efficient, and another more general algorithm for image classification (see Sec. 3.1), where the latter choice is natural. Second, it allows easy integration of prior knowledge of the data, which is shown to be effective in Sec. 4.In this section, we show that the update for the scorer network in Eqn. 3 can be approximately derived as the solution of a bi-level optimization problem (Colson et al., 2007), which has been applied to many different lines of research (Baydin et al., 2018; Liu et al., 2019a; Ren et al., 2018). Under our framework, the scorer samples the data by 〈x, y〉 ∼ P (X,Y ;ψ), and ψ will be chosen so that θ∗ that optimizes J(θ, P (X,Y ;ψ)) will approximately minimize J(θ, Pdev(X,Y )): ψ∗ = argmin ψ J(θ∗(ψ),Ddev) where θ∗(ψ) = argmin θ Ex,y∼P (X,Y ;ψ) [`(x, y; θ)] (5) The connection between ψ and θ in Eqn. 5 shows that J(θt,Ddev) is differentiable with respect to ψ. Now we can approximately compute the gradient∇ψJ(θt,Ddev) as follows: ∇ψJ(θt,Ddev) = ∇θtJ(θt,Ddev) > · ∇ψθt(ψ) (chain rule) = ∇θtJ(θt,Ddev) > · ∇ψ (θt−1 −∇θJ(θt−1, ψ)) (substitute θt from Eqn 4) ≈ −∇θtJ(θt,Ddev) > · ∇ψ (∇θJ(θt−1, ψ)) (assume∇ψθt−1 ≈ 0) = −∇ψEx,y∼P (X,Y ;ψ) [ ∇θJ(θt,Ddev)> · ∇θ`(x, y; θt−1) ] = −Ex,y∼P (X,Y ;ψ) [( ∇θJ(θt,Ddev)> · ∇θ`(x, y; θt−1) ) · ∇ψ logP (x, y;ψ) ] (6) Here, we make a Markov assumption that∇ψθt−1 ≈ 0, assuming that at step t, given θt−1 we do not care about how the values of ψ from previous steps led to θt−1. Eqn. 9 leads to a rule to update ψ using gradient descent, which is exactly the same as the RL update rule in Eqn. 3. Note that our derivation above does not take into the account that we might use different optimizing algorithms, such as SGD or Adam (Kingma & Ba, 2015), to update θ. We provide detailed derivations for several popular optimization algorithms in Appendix A.1. One potential concern with our approach is that because we optimize ψt directly on the dev set using J(θt,Ddev), we may risk indirectly overfitting model parameters θt by selecting a small subset of data that is overly specialized. However we do not observe this problem in practice, and posit that this because (1) the influence of ψt on the final model parameters θt is quite indirect, and acts as a “bottleneck” which has similarly proven useful for preventing overfitting in neural models Grézl et al. (2007), and (2) because the actual implementations of DDS (which we further discuss in Section 3) only samples a subset of data from Dtrain at each optimization step, further limiting expressivity.We now turn to discuss two concrete instantiations of DDS that we use in our experiments: a more generic example of classification, which should be applicable to a wide variety of tasks, and a specialized application to the task of multilingual NMT, which should serve as an example of how DDS can be adapted to the needs of specific applications.Algorithm 1: Training a classification model with DDS. Input :Dtrain, Ddev Output :Optimal parameters θ∗ 1 Initializer θ0 and ψ0 2 for t = 1 to num_train_steps do 3 Sample B training data points xi, yi ∼ Uniform(Dtrain) 4 Sample B validation data points x′i, y ′ i ∼ Uniform(Ddev) . Optimize θ 5 Update θt ← GradientUpdate ( θt−1, ∑B i=1 p(xi, yi;ψt−1)∇θ`(xi, yi; θt−1) ) . Evaluate θt on Ddev 6 Let dθ ← 1B ∑B j=1∇θ`(x′j , y′j ; θt) . Optimize ψ 7 Let dψ ← 1B ∑B i=1 [( d>θ · ∇θ`(xi, yi; θt−1) ) · ∇ψ log p(xi, yi;ψ) ] 8 Update ψt ← GradientUpdate(ψt−1, dψ) 9 end Algorithm 1 presents the pseudo code for the training process on classification tasks, using the notations introduced in Section 2. The main classification model is parameterized by θ. The scorer p(X,Y ;ψ) is an identical network with the main model, but with independent weights, i.e. p(X,Y ;ψ) does not share weights with θ. For each example xi in a minibatch uniformly sampled from Dtrain, this DDS model outputs a scalar from the data xi. All scalars are passed through a softmax function to compute the relative probabilities of the examples in the minibatch, and their gradients are scaled accordingly when applied to θ. Note that our actual formulation of p(X,Y ;ψ) does not depend on Y , but we keep Y in the notation for consistency with the formulation of the DDS framework. Note that we have two gradient update steps, one for the model parameter θt in Line 5 and the other for the DDS scorer parameter ψ in Line 8. For the model parameter update, we can simply use any of the standard optimization update rule. For the scorer ψ, we use the update rule derived in Section 2.3. Per-Example Gradient. As seen from Line 7 of Algorithm 1, as well as from Eqn. 13, DDS requires us to compute ∇θ`(xi, yi; θt−1), i.e. the gradient for each example in a batch of training data. This operation is very slow and memory intensive, especially when the batch size is large, e.g. our experiments on ImageNet use a batch size of 4096 (see Section 4). Therefore, we propose an efficient approximation of this per-example gradient computation via the first-order Taylor expansion of `(xi, yi; θt−1). In particular, for any vector v ∈ R|θ|, with sufficiently small > 0, we have: v> · ∇θ`(xi, yi; θt−1) ≈ 1 ( ` ( xi, yi; θt−1 + v ) − ` ( xi, yi; θt−1 )) , (7) Eqn 7 can be implemented by keeping a shadow version of parameters θt−1, caching training loss `(xi, yi; θt−1), and computing the new loss with θt−1 + v. Here, v is dθ as in Line 7 of Algorithm 1.Next we demonstrate an application of DDS to multilingual models for NMT, specifically for improving accuracy on low-resource languages (LRL) (Zoph et al., 2016; Neubig & Hu, 2018). In this setting, we assume that we have a particular LRL S that we would like to translate into target language T , and we additionally have a multilingual corpus Dtrain that has parallel data between n source languages (S1, S2, ..., Sn) and target language T . We would like to pick parallel data from any of the source languages to the target language to improve translation of a particular LRL S, so we assume that Ddev exclusively consists of parallel data between S and T . Thus, DDS will attempt to select data from Dtrain that improve accuracy on S-to-T translation as represented by Ddev. Algorithm 2: Training multilingual NMT with DDS. Input :Dtrain; K: number of data to train the NMT model before updating ψ; E: number of updates for ψ; α1,α2: discount factors for the gradient Output :The converged NMT model θ∗ 1 Initialize ψ0, θ0 . Initialize the gradient of each source language 2 grad[Si]← 0 for i in n 3 while θ not converged do 4 X,Y ← load_data(ψ,Dtrain,K) . Train the NMT model 5 for xi, y in X,Y do 6 θt ← GradientUpdate ( θt−1,∇θt−1`(xi, y; θt−1) ) 7 grad[Si]← α1 × grad[Si] + α2 ×∇θt−1`(xi, y; θt−1) 8 end . Optimize ψ 9 for iter in E do 10 sample B data pairs from Dtrain 11 dψ ← 1B ∑B j=1 ∑n i=1 [ grad[Si]>grad[S] · ∇ψt−1 log (p (Si|yj ;ψt−1)) ] 12 ψt ← GradientUpdate(ψt−1, dψt−1) 13 end 14 end To make training more efficient and stable in this setting, we make three simple modifications of the main framework in Section 2.3 that take advantage of the problem structure of multilingual NMT. First, instead of directly modeling p(X,Y ;ψ), we assume a uniform distribution over the target sentence Y , and only parameterize the conditional distribution of which source language sentence to pick given the target sentence: p(X|y;ψ). This design follows the formulation of Target Conditioned Sampling (TCS; Wang & Neubig (2019)), an existing state-of-the-art data selection method that uses a similar setting but models the distribution p(X|y) using heuristics. Since the scorer only needs to model a simple distribution over training languages, we use a fully connected 2-layer perceptron network. Second, we only update ψ after updating the NMT model for a fixed number of steps. Third, we sample the data according to p(X|y;ψ) to get a Monte Carlo estimate of the objective in Eqn. 5. This significantly reduces the training time compared to using all data. The pseudo code of the training process is in Algorithm 2.We now discuss experimental results on both image classification, an instance of the general classification problem using Algorithm 1, and multilingual NMT using Algorithm 2.Data. We apply our method on established benchmarks for image classification and multilingual NMT. For image classification, we use CIFAR-10 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015). For each dataset, we consider two settings: a reduced setting where only roughly 10% of the training labels are used, and a full setting, where all labels are used. Specifically, the reduced setting for CIFAR-10 uses the first 4000 examples in the training set, and with ImageNet, the reduced setting uses the first 102 TFRecord shards as pre-processed by Kornblith et al. (2019). We use the size of 224× 224 for ImageNet. For multilingual NMT, we use the 58-language-to-English TED dataset (Qi et al., 2018). Following prior work (Qi et al., 2018; Neubig & Hu, 2018; Wang et al., 2019b), we evaluate translation from four low-resource languages (LRL) Azerbaijani (aze), Belarusian (bel), Galician (glg), and Slovak (slk) to English, where each is paired with a similar high-resource language Turkish (tur), Russian (rus), Portugese (por), and Czech (ces) (details in Appendix A.3). We combine data from all 8 languages, and use DDS to optimize data selection for each LRL. Models and Training Details. For image classification, on CIFAR-10, we use the pre-activation WideResNet-28 (Zagoruyko & Komodakis, 2016), with width factor k = 2 for the reduced setting and k = 10 for the normal setting. For ImageNet, we use the post-activation ResNet-50 (He et al., 2016). These implementations reproduce the numbers reported in the literature (Zagoruyko & Komodakis, 2016; He et al., 2016; Xie et al., 2017), and additional details can be found in Appendix A.4. For NMT, we use a standard LSTM-based attentional baseline (Bahdanau et al., 2015), which is similar to previous models used in low-resource scenarios both on this dataset (Neubig & Hu, 2018; Wang et al., 2019b) and others (Sennrich & Zhang, 2019) due to its relative stability compared to other options such as the Transformer (Vaswani et al., 2017). Accuracy is measured using BLEU score (Papineni et al., 2002). More experiment details are noted in Appendix A.2. Baselines and Our Methods. For both image classification and multi-lingual NMT, we compare the following data selection methods. Uniform where data is selected uniformly from all of the data that we have available, as is standard in training models. SPCL (Jiang et al., 2015), a curriculum learning method that dynamically updates the curriculum to focus more on the “easy” training examples based on model loss. DDS, our proposed method. For image classification, we compare with several additional methods designed for filtering noisy data on CIFAR-10, where we simply consider the dev set as the clean data. BatchWeight (Ren et al., 2018), a method that scales example training loss in a batch with a locally optimized weight vector using a small set of clean data. MentorNet (Jiang et al., 2018), a curriculum learning method that trains a mentor network to select clean data based on features from both the data and the main model. For machine translation, we also compare with two state-of-the-art heuristic methods for multi-lingual data selection. Related where data is selected uniformly from the target LRL and a linguistically related HRL (Neubig & Hu, 2018). TCS, a recently proposed method of “target conditioned sampling”, which uniformly chooses target sentences, then picks which source sentence to use based on heuristics such as word overlap (Wang & Neubig, 2019). Note that both of these methods take advantage of structural properties of the multi-lingual NMT problem, and do not generalize to other problems such as classification. DDS is a flexible framework to incorporate prior knowledge about the data using the scorer network, which can be especially important when the data has certain structural properties such as language or domain. We test such a setting of DDS for both tasks. For image classification, we use retrained DDS, where we first train a model and scorer network using the standard DDS till convergence. The trained scorer network can be considered as a good prior over the data, so we use it to train the final model from scratch again using DDS. For multilingual NMT, we experiment with TCS+DDS, where we initialize the parameters of DDS with the TCS heuristic, then continue training.The results of the baselines and our method are listed in Table 1. First, comparing the standard baseline strategy of “Uniform” and the proposed method of “DDS” we can see that in all 8 settings DDS improves over the uniform baseline. This is a strong indication of both the consistency of the improvements that DDS can provide, and the generality – it works well in two very different settings. Next, we find that DDS outperforms SPCL by a large margin for both of the tasks, especially for multilingual NMT. This is probably because SPCL weighs the data only by their easiness, while ignoring their relevance to the dev set, which is especially important in settings where the data in the training set can have very different properties such as the different languages in multilingual NMT. DDS also brings improvements over the state-of-the-art intelligent data utilization methods. For image classification, DDS outperforms MentorNet and BatchWeight on CIFAR-10 in all settings. For NMT, in comparison to Related and TCS, vanilla DDS performs favorably with respect to these state-of-the-art data selection baselines, outperforming each in 3 out of the 4 settings (with exceptions of slightly underperforming Related on glg and TCS on aze). In addition, we see that incorporating prior knowledge into the scorer network leads to further improvements. For image classification, retrained DDS can significantly improve over regular DDS, leading to the new state-of-the-art result on the CIFAR-10 dataset. For mulitlingual NMT, TCS+DDS achieves the best performance in three out of four cases (with the exception of slk, where vanilla DDS already outperformed TCS).4 DDS does not incur much computational overhead. For image classification and multilingual NMT respectively, the training time is about 1.5× and 2× the regular training time without DDS5. 4.3 ANALYSIS Image Classification. Prior work on heuristic data selection has found that the model performs better if we feed higher quality or more domain-relevant data towards the end of training (van der Wees et al., 2017; Wang et al., 2019a). Here we verify this observation by analyzing the learned importance weight at the end of training for image classification. Figure 2 shows that at the end of training, DDS learns to balance the class distribution, which is originally unbalanced due to the dataset creation. Figure 3 shows that at the end of training, DDS assigns higher probabilities to images with clearer class content from ImageNet. These results show that DDS learns to focus on higher quality data towards the end of training. NMT. Next, we focus on multi-lingual NMT, where the choice of data directly corresponds to picking a language, which has an intuitive interpretation. Since DDS adapts the data weights dynamically to the model throughout training, here we analyze how the dynamics of learned weights. We plot the probability distribution of the four HRLs (because they have more data and thus larger impact on training) over the course of training. Figure 4 shows the change of language distribution 4For the NMT significance tests (Clark et al., 2011) find significant gains over the baseline for aze, slk, and bel. For glg the gain is not significant, but DDS-uniform without heuristics performs as well as the TCS baseline. 5The code for multilingual NMT is not optimized, so its training time could be reduced further for TCS+DDS. Since TCS selects the language with the largest vocabulary overlap with the LRL, the distribution is initialized to focus on the most related HRL. For all four LRLs, the percentage of their most related HRL starts to decrease as training continues. For aze, DDS quickly comes back to using its most related HRL. However, for bel, DDS continues the trend of using all four languages. This shows that DDS is able to maximize the benefits of the multi-lingual data by having a more balanced usage of all languages. Figure 5 shows a more interesting trend of DDS without heuristic initialization. For both aze and bel, DDS focuses on the most related HRL after a certain number of training updates. Interestingly, for bel, DDS learns to focus on both rus, its most related HRL, and ces. Similarly for slk, DDS also learns to focus on ces, its most related HRL, and rus, although there is little vocabulary overlap between slk and rus. Also notably, the ratios change significantly over the course of training, indicating that different types of data may be more useful during different learning stages.Many machine learning approaches consider how to best present data to models. First, difficultybased curriculum learning estimates the presentation order based on heuristic understanding of the hardness of examples (Bengio et al., 2009; Spitkovsky et al., 2010; Tsvetkov et al., 2016; Zhang et al., 2016; Graves et al., 2017; Zhang et al., 2018; Platanios et al., 2019). These methods, though effective, often generalize poorly because they require task-specific difficulty measures. On the other hand, self-paced learning (Kumar et al., 2010; Lee & Grauman, 2011) defines the hardness of the data based on the loss from the model, but is still based on the assumption that the model should learn from easy examples. Our method does not make these assumptions. Closest to the learning to teach framework (Fan et al., 2018) but their formulation involves manual feature design and requires expensive multi-pass optimization. Instead, we formulate our reward using bi-level optimization, which has been successfully applied for a variety of other tasks (Colson et al., 2007; Anandalingam & Friesz, 1992; Liu et al., 2019a; Baydin et al., 2018; Ren et al., 2018). Data selection for domain adaptation for disparate tasks has also been extensively studied (Moore & Lewis, 2010; Axelrod et al., 2011; Ngiam et al., 2018; Jiang & Zhai, 2007; Foster et al., 2010; Wang et al.). These methods generally design heuristics to measure domain similarity. Submodular optimization (Kirchhoff & Bilmes, 2014; Tschiatschek et al., 2014) selects training data that are similar to dev set, but the criterion is often based on hand-designed features and the data usage is predefined before training. Besides domain adaptation, selecting also benefits training in the face of noisy or otherwise undesirable data (Vyas et al., 2018; Pham et al., 2018). Our method is also related to works on training instance weighting (Sivasankaran et al., 2017; Ren et al., 2018; Jiang & Zhai, 2007; Ngiam et al., 2018). These methods reweigh data based on a manually computed weight vector, instead of using a parameterized neural network. Notably, Ren et al. (2018) tackles noisy data filtering for image classification, by using meta-learning to calculate a locally optimized weight vector for each batch of data. In contrast, our work focuses on the general problem of optimizing data usage. We train a parameterized scorer network that optimizes over the entire data space, which can be essential in preventing overfitting mentioned in Sec. 2; empirically our method outperform Ren et al. (2018) by a large margin in Sec. 4. (Wu et al., 2018; Kumar et al., 2019; Fang et al., 2017) propose RL frameworks for specific natural language processing tasks, but their methods are less generalizable and requires more complicated featurization.We present Differentiable Data Selection, an efficient RL framework for optimizing training data usage. We parameterize the scorer network as a differentiable function of the data, and provide an intuitive reward function for efficiently training the scorer network. We formulate two algorithms under the DDS framework for two realistic and very different tasks, image classification and multilingual NMT, which lead to consistent improvements over strong baselines.A.1 DERIVING GRADIENT OF ψ FOR DIFFERENT OPTIMIZERS First, we rewrite the update rule of θ in Eqn. 4 to incorporate the effect of its specific optimization algorithm. For a fixed value of ψ, J(θ, ψ) can be optimized using a stochastic gradient update. Specifically, at time step t, we update θt ← θt−1 − g ( ∇θJ(θt−1, ψ) ) (8) where g(·) is any function that may be applied to the gradient∇θJ(θt−1, ψ). For instance, in standard gradient descent g(·) is simply a linear scaling of ∇θJ(θt−1, ψ) by a learning rate ηt, while with the Adam optimizer (Kingma & Ba, 2015) g also modifies the learning rate on a parameter-by-parameter basis. Due to the relationship between θt and ψ as in Eqn 8, J(θt,Ddev) is differentiable with respect to ψ. By the chain rule, we can compute the gradient∇ψJ(θt,Ddev) as follows: ∇ψJ(θt,Ddev) = ∇θtJ(θt,Ddev) > · ∇ψθt(ψ) (chain rule) = ∇θtJ(θt,Ddev) > · ∇ψ ( θt−1 − g ( ∇θJ(θt−1) )) (substitute θt from Eqn 8) ≈ −∇θtJ(θt,Ddev) > · ∇ψg ( ∇θJ(θt−1) ) (assume∇ψθt−1 ≈ 0) (9) Here, we make a Markov assumption that∇ψθt−1 ≈ 0, assuming that at step t, given θt−1 we do not care about how the values of ψ from previous steps led to θt−1. Eqn 9 leads to a rule to update ψ using gradient descent: ψt+1 ← ψt + ηψ∇θtJ(θt,Ddev) > · ∇ψg ( ∇θJ(θt−1, ψt) ) , (10) Here we first derive ∇ψg for the general stochastic gradient descent (SGD) update, then provide examples for two other common optimization algorithms, namely Momentum (Nesterov, 1983) and Adam (Kingma & Ba, 2015). SGD Updates. The SGD update rule for θ is as follows θt ← θt−1 − ηt∇θJ(θt−1, ψ) (11) where ηt is the learning rate. Matching the updates in Eqn 11 with the generic framework in Eqn 8, we can see that g in Eqn 8 has the form: g ( ∇θJ(θt−1, ψ) ) = ηt∇θJ(θt−1, ψ) (12) This reveals a linear dependency of g on ∇θJ(θt−1,ψ), allowing the exact differentiation of g with respect to ψ. From Eqn 10, we have ∇J(θt,Ddev)> · ∇ψg ( ∇θJ(θt−1, ψ) ) = ηt · ∇ψEx,y∼p(X,Y ;ψ) [ J(θt,Ddev)> · ∇θ`(x, y; θt−1) ] = ηtEx,y∼p(X,Y ;ψ) [( J(θt,Ddev)> · ∇θ`(x, y; θt−1) ) · ∇ψ log p(x, y;ψ) ] (13) Here, the last equation follows from the log-derivative trick in the REINFORCE algorithm (Williams, 1992). Momentum Updates. The momentum update rule for θ is as follows mt ← µtmt−1 + ηt∇θJ(θt−1, ψ) θt ← θt−1 −mt, (14) where µt is the momentum coefficient and ηt is the learning rate. This means that g has the form: g(x) = µmt−1 + ηtx g′(x) = ηt (15) Therefore, the computation of the gradient∇ψ for the Momentum update is exactly the same with the standard SGD update rule in Eqn 13. Adam Updates. We use a slightly modified update rule based on Adam (Kingma & Ba, 2015): gt ← ∇θJ(θt−1, ψ) vt ← β2vt−1 + (1− β2)g2t v̂t ← vt/(1− βt2) θt ← θt−1 − ηt · gt/ √ v̂t + (16) where β2 and ηt are hyper-parameters. This means that g is a component-wise operation of the form: g(x) = ηt √ 1− βt2 · x√ β2vt−1 + (1− β2)x2 + g′(x) = ηt √ 1− βt2(β2vt−1 + )( β2vt−1 + (1− β2)x2 + )3/2 ≈ ηt √ 1− βt2 β2vt−1 , (17) the last equation holds because we assume vt−1 is independent of ψ. Here the approximation makes sense because we empirically observe that the individual values of the gradient vector ∇θJ(θt−1, ψ), i.e. gt, are close to 0. Furthermore, for Adam, we usually use β2 = 0.999. Thus, the value (1 − β2)x2 in the denominator of Eqn 17 is negligible. With this approximation, the computation of the gradient ∇ψ is almost the same with that for SGD in Eqn 13, with one extra component-wise scaling by the term in Eqn 17. A.2 HYPERPARAMETERS FOR MULTILINGUAL NMT In this section, we give a detailed description of the hyperparameters used for the multilingual NMT experiments. • We use a 1 layer LSTM with hidden size of 512 for both the encoder and decoder, and set the word embedding to size 128. • For multilingual NMT, we only use the scorer to model the distribution over languages. Therefore, we use a simple 2-layer perceptron network as the scorer architecture. Suppose the training data is from n different languages. For each target sentence and its corresponding source sentences, the input feature is a n-dimensional vector of 0 and 1, where 1 indicates a source language exists for the given target sentence. • We simply use the dev set that comes with the dataset as Ddev to update the scorer. • The dropout rate is set to 0.3. • For the NMT model, we use Adam optimizer with learning rate of 0.001. For the distribution parameter ψ, we use Adam optimizer with learning rate of 0.0001. • We train all models for 20 epochs without any learning rate decay. • We optimize both the NMT and DDS models with Adam, using learning rates of 0.001 and 0.0001 for θ and ψ respectively. A.3 DATASET STATISTICS FOR MULTILINGUAL NMT A.4 HYPERPARAMETERS FOR IMAGE CLASSIFICATION In this section, we provide some additional details for the image classification task: • We use the cosine learning rate decay schedule (Loshchilov & Hutter, 2017), starting at 0.1 for CIFAR-10 and 3.2 for ImageNet, both with 2000 warmup steps. • For image classification, we use an identical network architecture with the main model, but with independent weights and a regressor to predict the score instead of a classifier to predict image classes. • To construct the Ddev to update the scorer, we hold out about 10% of the training data. For example, in CIFAR-10 (4,000), Ddev is the last 400 images, while in ImageNet-10%, since we use the first 102 TFRecord shards, Ddev consists of the last 10 shards. Here, “last” follows the order in which the data is posted on their website for CIFAR-10, and the order in which the TFRecord shards are processed for ImageNet. All data in Ddev are excluded from Dtrain. Thus, for example, with CIFAR-10 (4,000), |Dtrain| = 3600, ensuring that in total, we are only using the amount of data that we claim to use. • We maintain a moving average of all model parameters with the rate of 0.999. Follow- ing Kornblith et al. (2019), we treat the moving statistics of batch normalization (Ioffe & Szegedy, 2015) as untrained parameters and also add them to the moving averages. • For ImageNet, we use the post-activation ResNet-50 (He et al., 2016). The batch sizes for CIFAR-10 and for ImageNet are 128 and 4096, running for 200K steps and 40K steps, respectively.
We propose a framework to model the distribution of sequential data coming from a set of entities connected in a graph with a known topology. The method is based on a mixture of shared hidden Markov models (HMMs), which are trained in order to exploit the knowledge of the graph structure and in such a way that the obtained mixtures tend to be sparse. Experiments in different application domains demonstrate the effectiveness and versatility of the method.Hidden Markov models (HMMs) are a ubiquitous tool for modelling sequential data. They started by being applied to speech recognition systems and from there they have spread to almost any application one can think of, encompassing computational molecular biology, data compression, and computer vision. In the emerging field of cognitive radars (Greco et al., 2018), for the task of opportunistic usage of the spectrum, HMMs have been recently used to model the occupancy of the channels by primary users (Stinco et al., 2016). When the expressiveness of an HMM is not enough, mixtures of HMM have been adopted. Roughly speaking, mixtures of HMMs can be interpreted as the result of the combination of a set of independent standard HMMs which are observed through a memoryless transformation (Couvreur, 1996; Dias et al., 2010; Subakan et al., 2014; Helske & Helske, 2016). In many real-life settings one does not have a single data stream but an arbitrary number of network connected entities that share and interact in the same medium and generate data streams in real-time. The streams produced by each of these entities form a set of time series with both intra and inter relations between them. In neuroimaging studies, the brain can be regarded as a network: a connected system where nodes, or units, represent different specialized regions and links, or connections, represent communication pathways. From a functional perspective, communication is coded by temporal dependence between the activities of different brain areas (De Vico Fallani et al., 2014). Also team sports intrinsically involve fast, complex and interdependent events among a set of entities (the players), which interact as a team (Tora et al., 2017; Theagarajan et al., 2018). Thus, understanding a player’s behavior implies understanding the behavior of his teammates and opponents over time. The extraction of knowledge from these streams to support the decision-making process is still challenging and the adaptation of HMM to this scenario is immature at best. Ferles & Stafylopatis (2013) proposed a hybrid approach combining the Self-Organizing Map (SOM) and the HMM with applications in clustering, dimensionality reduction and visualization of large-scale sequence spaces. Note that the model at each node is limited to a simple HMM. Wireless local area networks have also been modeled with Markov-based approaches. For instance, Allahdadi et al. (2014) use HMMs for outlier detection in 802.11 wireless access points. However, the typical approaches include a common HMM model for all nodes (with strong limited flexibility) and a HMM model per node, independent of the others (not exploring the dependencies between nodes). Bolton et al. (2018) built a sparse coupled hidden Markov model (SCHMM) framework to parameterize the temporal evolution of data acquired with functional magnetic resonance imaging (fMRI). The coupling is captured in the transition matrix, which is assumed to be a function of the activity levels of all the streams; the model per node is still restricted to a simple HMM. In general, in networked data streams, the stream observed in each sensor is often modeled by HMMs but the intercorrelations between sensors are seldom explored. The proper modeling of the intercorrelations has the potential to improve the learning process, acting as a regularizer in the learning process. In here we propose to tackle this void, by proposing as observation model at each node a sparse mixture of HMMs, where the dependencies between nodes are used to promote the sharing of HMM components between similar nodes.The proposed model finds intersections with distributed sparse representation and multitask learning.Sparse representation/coding expresses a signal/model f , defined over some independent variable x, as a linear combination of a few atoms from a prespecified and overcomplete dictionary: fpxq “ ÿ i siφipxq, (1) where φipxq are the atoms and only a few of the scalars si are non-zero, providing a sparse representation of fpxq. Distributed sparse representation (Baron et al., 2009) is an extension of the standard version that considers networks with K nodes. At each node, the signal sensed at the same node has its sparsity property because of its intracorrelation, while, for networks with multiple nodes, signals received at different nodes also exhibit strong intercorrelation. The intra- and inter-correlations lead to a joint sparse model. An interesting scenario in distributed sparse representation is when all signals/models share the common support but with different non-zero coefficients. Multitask learning techniques rely on the idea that individual models for related tasks should share some structure (parameters or hyperparameters). An interesting approach is based on adapting knowledge instead of data, handled by parameter transfer approaches, where parameters for different tasks/models are shared or constrained to be similar (Fernandes & Cardoso, 2017). Inspired by the formulation of equation 1, we propose to model the generative distribution of the data coming from each of the K nodes of a network as a sparse mixture obtained from a dictionary of generative distributions. Specifically, we shall model each node as a sparse HMM mixture over a ‘large’ dictionary of HMMs, where each HMM corresponds to an individual atom from the dictionary. The field knowledge about the similarities between nodes is summarized in an affinity matrix. The objective function of the learning process promotes reusing HMM atoms between similar nodes. We formalize now these ideas.Assume we have a set of nodes Y “ t1, ...,Ku connected by an undirected weighted graph G, expressed by a symmetric matrix G P RKˆK . These nodes thus form a network, in which the weights are assumed to represent degrees of affinity between each pair of nodes (i.e. the greater the edge weight, the more the respective nodes like to agree). The nodes y in the graph produce D-dimensional sequences X “ ` xp1q, ...,xpT q ˘ , xptq P RD, whose conditional distribution we shall model using a mixture of HMMs: ppX|yq “ ÿ z ppz|yqppX|zq, (2) where z P t1, ...,Mu is a latent random variable, being M the size of the mixture. Here, ppX|zq is the marginal distribution of observations of a standard first-order HMM: ppX|zq “ ÿ h pphp0q|zq ź t pphptq|hpt´1q, zqppxptq|hptq, zq, (3) where h “ ´ hp0q, ..., hpT q ¯ , hptq P t1, ..., Su, is the sequence of hidden states of the HMM, being S the number of hidden states. Note that the factorization in equation 2 imposes conditional independence between the sequence X and the node y, given the latent variable z. This is a key assumption of this model, since this way the distributions for the observations in the nodes in Y share the same pool of HMMs, promoting parameter sharing among the K mixtures.Given an observed sequence X and its corresponding node y P Y, the inference problem here consists in finding the likelihood ppX “X|y “ yq (from now on, abbreviated as ppX|yq) as defined by equations 2 and 3. The marginals ppX|zq of each HMM in the mixture may be computed efficiently, in OpS2T q time, using the Forward algorithm (Rabiner & Juang, 1986). Then, ppX|yq is obtained by applying equation 2, so inference in the overall model is done in at most OpMS2T q time. As we shall see, however, the mixtures we get after learning will often be sparse (see section 2.2.3), leading to an even smaller time complexity.Given an i.i.d. dataset consisting of N tuples pXi, yiq of sequences of observations Xi “ ´ x p1q i , ...,x pTiq i ¯ and their respective nodes yi P Y, the model defined by equations 2 and 3, may be easily trained using the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), (locally) maximizing the usual log-likelihood objective: Jpθq “ ÿ i log ppXi|yi, θq, (4) where θ represents all model parameters, namely: 1. the mixture coefficients, αk :“ pppz “ 1|y “ kq, ..., ppz “M |y “ kqq, for k “ 1, ...,K; 2. the initial state probabilities, πm :“ ´ pphp0q “ 1|z “ mq, ..., pphp0q “ S|z “ mq ¯ , for m “ 1, ...,M ; 3. the state transition matrices, Am, where Ams,u :“ pphptq “ u|hpt´1q “ s, z “ mq, for s, u “ 1, ..., S and m “ 1, ...,M ; 4. the emission probability means, µm,s P RD, for m “ 1, ...,M and s “ 1, ..., S; 5. the emission probability diagonal covariance matrices, σ2m,sI , where σ 2 m,s P R`D, for m “ 1, ...,M and s “ 1, ..., S. Here, we are assuming that the observations are continuous and the emission probabilities ppxptq|hptq, zq are gaussian with diagonal covariances. This introduces almost no loss of generality, since the extension of this work to discrete observations or other types of continuous emission distributions is straightforward. The procedure to maximize equation 4 using EM is described in Algorithm 1, in section A.1. The update formulas follow from the standard EM procedure and can be obtained by viewing this model as Bayesian network or by following the derivation detailed in section A.2. However, the objective 4 does not take advantage of the known structure of G. In order to exploit this information, we introduce a regularization term, maximizing the following objective instead: Jrpθq “ 1 N ÿ i log ppXi|yi, θq ` λ 2 ÿ j,k‰j Gj,kEz„ppz|y“j,θqrppz|y “ k, θqs, (5) where λ ě 0 controls the relative weight of the two terms in the objective. The expectations Ez„ppz|y“j,θqrppz|y “ k, θq have interesting properties which are enlightened and proven in Proposition 1. Proposition 1. Let P be the set of all M -nomial probability distributions. We have: 1. minp,qPP Ez„prqpzqs “ 0; 2. argminp,qPP Ez„prqpzqs “ tp, q P P | @m P t1, ...,Mu : ppz “ mqqpz “ mq “ 0u; 3. maxp,qPP Ez„prqpzqs “ 1; 4. argmaxp,qPP Ez„prqpzqs “ tp, q P P | Dm P t1, ...,Mu : ppz “ mq “ qpz “ mq “ 1u . Proof. By the definition of expectation, Ez„prqpzqs “ ÿ m ppz “ mqqpz “ mq. (6) Statements 1 and 2 follow immediately from the fact that every term in the right-hand side of equation 6 is non-negative. For the remaining, we note that equation 6 may be rewritten as the dot product of twoM -dimensional vectorsαp andαq , representing the two distributions p and q, respectively, and we use the following linear algebra inequalities to build an upper bound for this expectation: Ez„prqpzqs “ αᵀpαq ď ||αp||2||αq||2 ď ||αp||1||αq||1 “ 1, (7) where || ¨ ||1 and || ¨ ||2 are the L1 and L2 norms, respectively. Clearly, the equality Ez„prqpzqs “ 1 holds if p and q are chosen from the set defined in statement 4, where the distributions p and q are the same and they are non-zero for a single assignment of z. This proves statement 3. Now, to prove statement 4, it suffices to show that there are no other maximizers. The first inequality in equation 7 is transformed into an equality if and only ifαp “ αq , which means p ” q. The second inequality becomes an equality when the L1 and L2 norms of the vectors coincide, which happens if and only if the vectors have only one non-zero component, concluding the proof. Specifically, given two distinct nodes j, k P Y , if Gj,k ą 0, the regularization term for these nodes is maximum (and equal to Gj,k) when the mixtures for these two nodes are the same and have one single active component (i.e. one mixture component whose coefficient is non-zero). On the contrary, if Gj,k ă 0, the term is maximized (and equal to zero) when the mixtures for the two nodes do not share any active components. In both cases, though, we conclude from Proposition 1 that we are favoring sparse mixtures. We see sparsity as an important feature since: 1 – it allows the coefficients to better capture the graph structure, which is usually sparse, and 2 – it leads to mixtures with fewer components, yielding faster inference and (possibly) less overfitting. By setting λ “ 0, we clearly get the initial objective 4, where inter-node correlations are modeled only via parameter sharing. As λ Ñ 8, two interesting scenarios may be anticipated. If Gj,k ą 0, @j, k, all nodes will tend do share the same single mixture component, i.e. we would be learning one single HMM to describe the whole network. If Gj,k ă 0, @j, k, and M ě K, each node would tend to learn its own HMM model independently from all the others. The objective function 5 can still be maximized via EM (see details in section A.3). However, the introduction of the regularization term in the objective makes it impossible to find a closed form solution for the update formula of the mixture coefficients. Thus, in the M-step, we need to resort to gradient ascent to update these parameters. In order to ensure that the gradient ascent iterative steps lead to admissible solutions, we adopt the following reparameterization from Yang et al. (2018): αk,m “ σ pβk,mq2 ř l σ pβk,lq 2 , (8) where σp¨q is the rectifier linear (ReLU) function. This reparameterization clearly resembles the softmax function, but, contrarily to that one, admits sparse outputs. The squared terms in equation 8 aim only to make the optimization more stable. The optimization steps for the objective 5 using this reparameterization are described in Algorithm 2, in section A.1.The model was developed on top of the library hmmlearn1 for Python, which implements inference and unsupervised learning for the standard HMM using a wide variety of emission distributions. 1https://github.com/hmmlearn/hmmlearn Both learning and inference use the hmmlearn API, with the appropriate adjustments for our models. For reproducibility purposes, we make our source code, pre-trained models and the datasets publicly available2. We evaluate four different models in our experiments: a model consisting of a single HMM (denoted as 1-HMM) trained on sequences from all graph nodes; a model consisting of K HMMs trained independently (denoted as K-HMM), one for each graph node; a mixture of HMMs (denoted as MHMM) as defined in this work (equations 2 and 3), trained to maximize the usual log-likelihood objective (equation 4); a mixture of HMMs (denoted as SpaMHMM) as the previous one, trained to maximize our regularized objective (equation 5). Models 1-HMM, K-HMM and MHMM will be our baselines. We shall compare the performance of these models with that of SpaMHMM and, for the case of MHMM, we shall also verify if SpaMHMM actually produces sparser mixtures in general, as argued in section 2.2.3. In order to ensure a fair comparison, we train models with approximately the same number of possible state transitions. Hence, given an MHMM or SpaMHMM with M mixture components and S states per component, we train a 1-HMM with « S ? M states and a K-HMM with « S a M{K states per HMM. We initialize the mixture coefficients in MHMM and SpaMHMM randomly, while the state transition matrices and the initial state probabilities are initialized uniformly. Means are initialized using k-means, with k equal to the number of hidden states in the HMM, and covariances are initialized with the diagonal of the training data covariance. Models 1-HMM and K-HMM are trained using the Baum-Welch algorithm, MHMM is trained using Algorithm 1 and SpaMHMM is trained using Algorithm 2. However, we opted to use Adam (Kingma & Ba, 2014) instead of “vanilla” gradient ascent in the inner loop of Algorithm 2, since its per-parameter learning rate proved to be beneficial for faster convergence.A typical Wi-Fi network infrastructure is constituted byK access points (APs) distributed in a given space. The network users may alternate between these APs seamlessly, usually connecting to the closest one. There is a wide variety of anomalies that may happen during the operation of such network and their automatic detection is, therefore, of great importance for future mitigation plans. Some anomalous behaviors are: overloaded APs, failed or crashed APs, persistent radio frequency (RF) interference between adjacent APs, authentication failures, etc. However, obtaining reliable ground truth annotation of these anomalies in entire wireless networks is costly and time consuming. Under these circumstances, using data obtained through realistic network simulations is a common practice. In order to evaluate our model in the aforementioned scenario, we have followed the procedure of Allahdadi & Morla (2017), performing extensive network simulations using OMNeT++3 and INET4. The former is a discrete event simulator for modeling communication networks that is used in many problem domains, including wireless networks. The latter is a framework that provides detailed models for several communication protocols (TCP, IP, IEEE 802.11, etc.). Here, we used OMNeT++ and INET to generate traffic in a typical Wi-Fi network setup (IEEE 802.11 WLANg 2.4 GHz in infrastructure mode). Our network consists of 10 APs and 100 users accessing it. The pairwise distances between APs are known and fixed. Each sequence contains information about the traffic in a given AP during 10 consecutive hours and is divided in time slots of 15 minutes without overlap. Thus, every sequence has the same length, which is equal to 40 samples (time slots). Each sample contains the following 7 features: the number of unique users connected to the AP, the number of sessions within the AP, the total duration (in seconds) of association time of all current users, the number of octets transmitted and received in the AP and the number of packets transmitted and received in the AP. Anomalies typically occur for a limited amount of time within the whole sequence. However, in this experiment, we label a sequence as “anomalous” if there is at least one anomaly period in the sequence and we label it as “normal” otherwise. One of the simulations includes normal data only, while the remaining include both normal and anomalous sequences. In order to avoid contamination of normal data with anomalies that may occur simultaneously in other APs, we used the data of the 2URL available after decision 3https://www.omnetpp.org 4https://inet.omnetpp.org/ normal simulation for training (150 sequences) and the remaining data for testing (378 normal and 42 anomalous sequences). In a Wi-Fi network, as users move in the covered area, they disconnect from one AP and they immediately connect to another in the vicinity. As such, the traffic in adjacent APs may be expected to be similar. Following this idea, the weight Gj,k, associated with the edge connecting nodes j and k in graph G, was set to the inverse distance between APs j and k and normalized so that maxj,kGj,k “ 1. As in Allahdadi & Morla (2017), sequences were preprocessed by subtracting the mean and dividing by the standard deviation and applying PCA, reducing the number of features to 3. For MHMM, we did 3-fold cross validation of the number of mixture components M and hidden states per component S. We ended up using M “ 10 and S “ 10. We then used the same values of M and S for SpaMHMM and we did 3-fold cross validation for the regularization hyperparameter λ in the range r10´4, 1s. The value λ “ 10´1 was chosen. We also cross-validated the number of hidden states in 1-HMM and K-HMM around the values indicated in section 3. Every model was trained for 100 EM iterations or until the loss plateaus. For SpaMHMM, we did 100 iterations of the inner loop on each M-step, using a learning rate ρ “ 10´3. Models were evaluated by computing the average log-likelihood per sample on normal and anomalous test data, plotting the receiver operating characteristic (ROC) curves and computing the respective areas under the curves (AUCs). Figure 1 shows that the ROC curves for MHMM and SpaMHMM are very similar and that these models clearly outperform 1-HMM and K-HMM. This is confirmed by the AUC and log-likelihood results in table 1. Although K-HMM achieved the best (lowest) average log-likelihood on anomalous data, this result is not relevant, since it also achieved the worst (lowest) average log-likelihood on normal data. This is in fact the model with the worst performance, as shown by its ROC and respective AUC. The bad performance of K-HMM likely results mostly from the small amount of data that each of the K models is trained with: in K-HMM, each HMM is trained with the data from the graph node (AP) that it is assigned to. The low log-likelihood value of the normal test data in this model confirms that the model does not generalize well to the test data and is probably highly biased towards the training data distribution. On the other hand, in 1-HMM there is a single HMM that is trained with the whole training set. However, the same HMM needs to capture the distribution of the data coming from all APs. Since each AP has its own typical usage profile, these data distributions are different and one single HMM may not be sufficiently expressive to learn all of them correctly. MHMM and SpaMHMM combine the advantages and avoid the disadvantages of both previous models. Clearly, since the mixtures for each node share the same pool of HMMs, every model in the mixture is trained with sequences from all graph nodes (at least in the first few training iterations). Thus, at this stage, the models may capture behaviors that are shared by all APs. As mixtures become sparser during training, mixture components specialize on the distribution of a few APs. This avoids the problem observed in 1-HMM, which is unaware of the AP where a sequence comes from. We would also expect SpaMHMM to be sparser and have better performance than MHMM, but only the former supposition was true (see figure 2) and by a small difference. The inexistence of performance gains in SpaMHMM might be explained from the fact that this dataset consists of simulated data, where users are static (i.e. they do not swap between APs unless the AP where they are connected stops working) and so the assumption that closer APs have similar distributions does not bring any advantage.The human body is constituted by several interdependent parts, which interact as a whole producing sensible global motion patterns. These patterns may correspond to multiple activities like walking, eating, etc. Here, we use our model to make short-time prediction of sequences of human joint positions, represented as motion capture (mocap) data. The current state of the art methodologies use architectures based on deep recurrent neural networks (RNNs), achieving remarkable results both in short-time prediction (Fragkiadaki et al., 2015; Martinez et al., 2017) and in long-term motion generation (Jain et al., 2016; Pavllo et al., 2018). Our experiments were conducted on the Human3.6M dataset from Ionescu et al. (2011; 2014), which consists of mocap data from 7 subjects performing 15 distinct actions. In this experiment, we have considered only 4 of those actions, namely “walking”, “eating”, “smoking” and “discussion”. There, the human skeleton is represented with 32 joints whose position is recorded at 50 Hz. We build our 32x32-dimensional symmetric matrixG representing the graph G in the following sensible manner: Gj,k “ 1, if there is an actual skeleton connection between joints j and k (e.g. the elbow joint is connected to the wrist joint by the forearm); Gj,k “ 1, if joints j and k are symmetric (e.g. left and right elbows); Gj,k “ 0, otherwise.We reproduced as much as possible the experimental setup followed in Fragkiadaki et al. (2015). Specifically, we down-sampled the data by a factor of 2 and transformed the raw 3-D angles into an exponential map representation. We removed joints with constant exponential map, yielding a dataset with 22 distinct joints, and pruned our matrixG accordingly. Training was performed using data from 6 subjects, leaving one subject (denoted in the dataset by “S5”) for testing. We did 3-fold cross-validation on the training data of the action “walking” to find the optimal number of mixture components M and hidden states S for the baseline mixture MHMM. Unsurprisingly, since this model can hardly overfit in such a complex task, we ended up with M “ 18 and S “ 12, which were the largest values in the ranges we defined. Larger values are likely to improve the results, but the learning time would become too large to be practical. For SpaMHMM, we used these same values of M and S and we did 3-fold cross validation on the training data of the action “walking” to fine-tune the value of λ in the range r10´4, 1s. We ended up using λ “ 0.05. The number of hidden states in 1-HMM was set to 51 and in K-HMM it was set to 11 hidden states per HMM. The same values were then used to train the models for the remaining actions. Every model was trained for 100 iterations of EM or until the loss plateaus. For SpaMHMM, we did 100 iterations of the inner loop on each M-step, using a learning rate ρ “ 10´2. In order to generate predictions for a joint (node) y starting from a given prefix sequence Xpref, we build the distribution ppX|Xpref, yq (see details in section A.4) and we sample sequences from that posterior. Our evaluation method and metric again followed Fragkiadaki et al. (2015). We fed our model with 8 prefix subsequences with 50 frames each (corresponding to 2 seconds) for each joint from the test subject and we predicted the following 10 frames (corresponding to 400 ms). Each prediction was built by sampling 100 sequences from the posterior and averaging. We then computed the average mean angle error for the 8 sequences at different time horizons. Results are in table 2. Among our models (1-HMM, K-HMM, MHMM and SpaMHMM), SpaMHMM outperformed the remaining in all actions except “eating”. For this action in particular, MHMM was slightly better than SpaMHMM, probably due to the lack of symmetry between the right and left sides of the body, which was one of the prior assumptions that we have used to build the graph G. “Smoking” and “discussion” activities may also be highly non-symmetric, but results in our and others’ models show that these activities are generally harder to predict than “walking” and “eating’. Thus, here, the skeleton structure information encoded in G behaves as a useful prior for SpaMHMM, guiding it towards better solutions than MHMM. The worse results for 1-HMM and K-HMM likely result from the same limitations that we have pointed out in section 3.1: each component in K-HMM is inherently trained with less data than the remaining models, while 1-HMM does not make distinction between different graph nodes. Extending the discussion to the state of the art solutions for this problem, we note that SpaMHMM compares favorably with ERD, LSTM-3LR and SRNN, which are all RNN-based architectures. Moreover, ERD and LSTM-3LR were designed specifically for this task, which is not the case for SpaMHMM. This is also true for GRU supervised and QuaterNet, which clearly outperform all remaining models, including ours. This is unsurprising, since RNNs are capable of modeling more complex dynamics than HMMs, due to their intrinsic non-linearity and continuous state representation. This also allows their usage for long-term motion generation, in which HMMs do not behave well due their linear dynamics. However, unlike GRU supervised and QuaterNet, SpaMHMM models the probability distribution of the data directly, allowing its application in domains like novelty detection. Regarding sparsity, the experiments confirm that the SpaMHMM mixture coefficients are actually sparser than those of MHMM, as shown in figure 2.We may roughly divide the human body in four distinct parts: upper body (head, neck and shoulders), arms, torso and legs. Joints that belong to the same part naturally tend to have coherent motion, so we would expect them to be described by more or less the same components in our mixture models (MHMM and SpaMHMM). Since SpaMHMM is trained to exploit the known skeleton structure, this effect should be even more apparent in SpaMHMM than in MHMM. In order to confirm this conjecture, we have trained MHMM and SpaMHMM for the action “walking” using four mixture components only, i.e. M “ 4, and we have looked for the most likely component (cluster) for each joint: Ck “ argmax mPt1,2,3,4u ppz “ m|y “ kq “ argmax mPt1,2,3,4u αk,m, (9) where Ck is, therefore, the cluster assigned to joint k. The results are in figure 3. From there we can see that MHMM somehow succeeds on dividing the body in two main parts, by assigning the joints in the torso and in the upper body mostly to the red/’+’ cluster, while those in the hips, legs and feet are almost all assigned to the green/’Ÿ’ cluster. Besides, we see that in the vast majority of the cases, symmetric joints are assigned to the same cluster. These observations confirm that we have chosen the graph G for this problem in an appropriate manner. However, some assignments are unnatural: e.g. one of the joints in the left foot is assigned to the red/‘+’ cluster and the blue/‘˝’ cluster is assigned to one single joint, in the left forearm. We also observe that the distribution of joints per clusters is highly uneven, being the green/‘Ÿ’ cluster the most represented by far. SpaMHMM, on the other hand, succeeds on dividing the body in four meaningful regions: upper body and upper spine in the green/‘Ÿ’ cluster; arms in the blue/‘˝’ cluster; lower spine and hips in the orange/‘x’ cluster; legs and feet in the red/‘+’ cluster. Note that the graph G used to regularize SpaMHMM does not include any information about the body part that a joint belongs to, but only about the joints that connect to it and that are symmetric to it. Nevertheless, the model is capable of using this information together with the training data in order to divide the skeleton in an intuitive and natural way. Moreover, the distribution of joints per cluster is much more even in this case, what may also help to explain why SpaMHMM outperforms MHMM: by splitting the joints more or less evenly by the different HMMs in the mixture, none of the HMM components is forced to learn too many motion patterns. In MHMM, we see that the green/‘+’ component, for instance, is the most responsible to model the motion of almost all joints in the legs and hips and also some joints in the arms and the red/‘+’ component is the prevalent on the prediction of the motion patterns of the neck and left foot, which are presumably very different.In this work we propose a method to model the generative distribution of sequential data coming from nodes connected in a graph with a known fixed topology. The method is based on a mixture of HMMs where its coefficients are regularized during the learning process in such a way that affine nodes will tend to have similar coefficients, exploiting the known graph structure. We also prove that the proposed regularizer promotes sparsity in the mixtures, which is achieved through a fully differentiable loss function (i.e. with no explicit L0 penalty term). We evaluate the method’s performance in two completely different tasks (anomaly detection in Wi-Fi networks and human motion forecasting), showing its effectiveness and versatility. For future work, we plan to extend/evaluate the usage of SpaMHMM for sequence clustering. This is an obvious extension that we did not explore thoroughly in this work, since its main focus was modeling the generative distribution of data. In this context, extending the idea behind SpaMHMM to mixtures of more powerful generative distributions is also in our plans. As is known, HMMs have limited expressiveness due to the strong independence assumptions they rely on. Thus, we plan to extend these ideas to develop an architecture based on more flexible generative models for sequence modeling, like those attained using deep recurrent architectures.Anonymous acknowledgments.A.1 ALGORITHMS Data: The training set, consisting of N tuples pXi, yiq, a set of initial parameters θp0q and the number of training iterations I. for j “ 1, ..., I do Sufficient statistics: 1. nk :“ ř i 1yi“k, where 1t¨u is the indicator function, for k “ 1, ...,K. 2. Obtain the mixture posteriors ηi,m :“ ppz “ m|Xi, yi, θpj´1qq, for i “ 1, ..., N and m “ 1, ...,M , by computing η̃i,m :“ ppXi|z “ m, θpj´1qqppz “ m|yi, θpj´1qq and normalizing it. 3. Obtain the state posteriors γi,m,sptq :“ pphptq “ s|z “ m,Xi, θpj´1qq and ξi,m,s,uptq :“ pphpt´1q “ s, hptq “ u|z “ m,Xi, θpj´1qq, for i “ 1, ..., N , m “ 1, ...,M and s, u “ 1, ..., S, as done in the Baum-Welch algorithm (Baum, 1972). M-step: 1. αk,m “ ř i ηi,m1yi“k nk , for k “ 1, ...,K and m “ 1, ...,M , obtaining αk. 2. πm,s “ ř i ηi,mγi,m,sp0q ř i ηi,m , for m “ 1, ...,M and s “ 1, ..., S, obtaining πm. 3. Ams,u “ ř i ηi,m řTi t“1 ξi,m,s,uptq ř i ηi,m řTi´1 t“0 γi,m,sptq , for m “ 1, ...,M and s, u “ 1, ..., S, obtainingAm. 4. µm,s “ ř i ηi,m řTi t“1 γi,m,sptqx ptq i ř i ηi,m řTi t“1 γi,m,sptq , for m “ 1, ...,M and s “ 1, ..., S. 5. σ2m,s “ ř i ηi,m řTi t“1 γi,m,sptq ´ x ptq i ´µ m s ¯2 ř i ηi,m řTi t“1 γi,m,sptq , for m “ 1, ...,M and s “ 1, ..., S. 6. θpjq “ Ť k,m,s αk,πm,A m,µm,s,σ 2 m,s ( . end Algorithm 1: EM algorithm for the mixture without regularization (MHMM). Data: The training set, consisting of N tuples pXi, yiq, the matrixG describing the graph G, the regularization hyperparameter λ, a set of initial parameters θp0q, the number of training iterations I, the number of gradient ascent iterations J to perform on each M-step, the learning rate ρ for the gradient ascent. for j “ 1, ..., I do Sufficient statistics: same as in Algorithm 1. M-step: for l “ 1, ...,J do 1. ψk,m :“ 1N ř ipηi,m ´ αk,mq1yi“k, for k “ 1, ...,K and m “ 1, ...,M . 2. ωk,m :“ αk,m ř j‰kGj,k ` αj,m ´αᵀjαk ˘ , for k “ 1, ...,K and m “ 1, ...,M . 3. δk,m :“ 1βk,mą0 2σ1pβk,mq σpβk,mq pψk,m ` λωk,mq, where σ 1p¨q is the derivative of σp¨q, for k “ 1, ...,K and m “ 1, ...,M . 4. βk,m Ð βk,m ` ρδk,m, for k “ 1, ...,K and m “ 1, ...,M . 5. Use equation 8 to obtain αk,m, for k “ 1, ...,K and m “ 1, ...,M . end Do steps 2) – 6) in the M-step of Algorithm 1. end Algorithm 2: EM algorithm for the mixture with regularization (SpaMHMM). A.2 PROOF OF ALGORITHM 1 Algorithm 1 follows straightforwardly from applying EM to the model defined by equations 2 and 3 with the objective 4. Let us define the following notation: X :“ tXiuNi“1, y :“ tyiu N i“1, z :“ tziu N i“1 and H :“ thiu N i“1. After building the usual variational lower bound for the log-likelihood and performing the E-step, we get the following well-known objective: J̃pθ, θ-q “ ÿ z,H ppX, z,H|y, θ-q log ppX, z,H|y, θq, (10) which we want to maximize with respect to θ and where θ- are the model parameters that were kept fixed in the E-step. Some of the parameters in the model are constrained to represent valid probabilities, yielding the following Lagrangian: Lpθ, θ-,λq “ J̃pθ, θ-q ` ÿ k λmixk p1´ ||αk||1q ` ÿ m,s λstatem,s ˜ 1´ ÿ u Ams,u ¸ ` ÿ m λinim p1´ ||πm||1q , (11) where λ summarizes all Lagrange multipliers used here (not to be confused with the regularization hyperparameter λ used in equation 5). Differentiating equation 11 with respect to each model parameter and Lagrange multiplier and solving for the critical points yields: αk,m “ ř i ppzi “ m|Xi, yi, θ-q1yi“k ř i 1yi“k , (12) πm,s “ ř i ppzi “ m|Xi, yi, θ-qpph p0q i “ s|zi “ m,Xi, yi, θ-q ř i ppzi “ m|Xi, yi, θ-q , (13) Ams,u “ ř i ppzi “ m|Xi, yi, θ-q řTi t“1 pph pt´1q i “ s, h ptq i “ u|zi “ m,Xi, yi, θ-q ř i ppzi “ m|Xi, yi, θ-q řTi t“1 pph pt´1q i “ s|zi “ m,Xi, yi, θ-q , (14) µm,s “ ř i ppzi “ m|Xi, yi, θ-q řTi t“1 pph ptq i “ s|zi “ m,Xi, yi, θ-qx ptq i ř i ppzi “ m|Xi, yi, θ-q řTi t“1 pph ptq i “ s|zi “ m,Xi, yi, θ-q , (15) σ2m,s “ ř i ppzi “ m|Xi, yi, θ-q řTi t“1 pph ptq i “ s|zi “ m,Xi, yi, θ-q ´ x ptq i ´ µms ¯2 ř i ppzi “ m|Xi, yi, θ-q řTi t“1 pph ptq i “ s|zi “ m,Xi, yi, θ-q , (16) @ k,m, s, u. Defining nk, ρi,m, γi,m,s and ξi,m,s,u as in Algorithm 1 the result follows. A.3 PROOF OF ALGORITHM 2 Using the same notation as in section A.2, we may rewrite equation 5 as: Jrpθq “ 1 N log ÿ z,H ppX, z,H|y, θq ` λ 2 ÿ j,k‰j Gj,kEz„ppz|y“j,θqrppz|y “ k, θqs. (17) Despite the regularization term, we may still lower bound this objective by introducing a variational distribution qpz,Hq and using Jensen’s inequality in the usual way: Jrpθq ě 1 N Ez,H„q „ log ppX, z,H|y, θq qpz,Hq  ` λ 2 ÿ j,k‰j Gj,kEz„ppz|y“j,θqrppz|y “ k, θqs :“ Vrpθ, qq. (18) Clearly, Jrpθq ´ Vrpθ, qq “ 1 N ˆ log ppX|y, θq ´ Ez,H„q „ log ppX, z,H|y, θq qpz,Hq ˙ “ 1 N DKL pqpz,Hq||ppz,H|X,y, θqq , (19) which, fixing the parameters θ to some value θ- and minimizing with respect to q, yields the usual solution q˚pz,Hq “ ppz,H|X,y, θ-q. Thus, in the M-step, we want to find: argmax θ Vrpθ, q˚q “ argmax θ 1 N ÿ z,H ppX, z,H|y, θ-q log ppX, z,H|y, θq ` λ 2 ppX|y, θ-q ÿ j,k‰j Gj,kEz„ppz|y“j,θqrppz|y “ k, θqs “ argmax θ 1 N J̃pθ, θ-q ` λRpθ, θ-q (20) :“ argmax θ J̃rpθ, θ-q, (21) where J̃pθ, θ-q is as defined in equation 10 and Rpθ, θ-q is our regularization weighted by the data likelihood, which is simply a function of the parameters α: Rpθ, θ-q “ 1 2 ppX|y, θ-q ÿ j,k‰j Gj,kEz„ppz|y“j,θqrppz|y “ k, θqs “ 1 2 ppX|y, θ-q ÿ j,k‰j Gj,kα ᵀ jαk “ Rpα1, ...,αk, θ-q (22) Now, we may build the Lagrangian as done in section A.2. Since R only depends on the α’s, the update equations for the remaining parameters are unchanged. However, for α, it is not possible to obtain a closed form update equation. Thus, we use the reparameterization defined in equation 8 and update the new unconstrained parameters β via gradient ascent. We have: BJ̃ Bαk,m “ ppX|y, θ -q αk,m ÿ i ppzi “ m|Xi, yi, θ-q1yi“k, (23) BR Bαk,m “ ppX|y, θ-q ÿ j‰k Gj,kαj,m. (24) From equations 23 and 24, we see that the the resulting gradient ∇αk J̃r “ ∇αk J̃`λ∇αkR is equal to some vector scaled by the joint data likelihood ppX|y, θ-q, which we discard since it only affects the learning rate, besides being usually very small and somewhat costly to compute. This option is equivalent to using a learning rate that changes at each iteration of the outter loop of the algorithm. Equation 8 yields the following derivatives: Bαk,m Bβk,m “ 1βk,mą0 2σ1pβk,mq σpβk,mq αk,mp1´ αk,mq, (25) Bαk,m Bβk,l “ 1βk,mą0 ´2σ1pβk,mq σpβk,mq αk,mαk,l, for l ‰ m. (26) Finally, by the chain rule, we obtain: BJ̃ Bβk,m “ ÿ l BJ̃ Bαk,l Bαk,l Bβk,m “ 1βk,mą0 2σ1pβk,mq σpβk,mq ÿ i pppzi “ m|Xi, yi, θ-q ´ αk,mq1yi“k, (27) BR Bβk,m “ ÿ l BR Bαk,l Bαk,l Bβk,m “ 1βk,mą0 2σ1pβk,mq σpβk,mq αk,m ÿ j‰k Gj,k ` αj,m ´αᵀjαk ˘ . (28) Defining δk,m :“ BJ̃rBβk,m “ 1 N BJ̃ Bβk,m ` λ BR Bβk,m and applying the gradient ascent update formula to βk,m the result follows. A.4 GETTING THE POSTERIOR DISTRIBUTION OF OBSERVATIONS IN SPAMHMM In this section, we show how to obtain posterior distribution ppX|Xpref, yq of sequences X “ ` xp1q, ...,xpT q ˘ given an observed prefix sequence Xpref “ ` xp´Tpref`1q, ...,xp0q ˘ , both coming from the graph node y. We consider the case where ppX|yq is a SpaMHMM (or MHMM) model and so we have: ppX|Xpref, yq “ ÿ z ppX|z,Xpref, yqppz|Xpref, yq “ ÿ z ppX|z,Xprefqppz|Xpref, yq, (29) where the second equality follows from the fact that the observations X are independent from the graph node y given the latent variable z. The posterior ppz|Xpref, yq may be obtained as done in Algorithm 1, so we now focus on ppX|z,Xprefq: ppX|z,Xprefq “ ÿ h pphp0q|z,Xprefq ź t pphptq|hpt´1q, z,Xprefqppxptq|hptq, z,Xprefq “ ÿ h pphp0q|z,Xprefq ź t pphptq|hpt´1q, zqppxptq|hptq, zq, (30) where we have used the independence assumptions of an HMM. Here, the initial state posteriors pphp0q|z,Xprefq are actually the final state posteriors for the sequence Xpref for each HMM in the mixture, so they can also be computed as indicated Algorithm 1. Thus, we see that, in order to obtain the posterior ppX|Xpref, yq, we only need to recompute the mixture coefficients ppz|Xpref, yq and the initial state probabilities pphp0q|z,Xprefq. All remaining parameters are unchanged.
Generative models have gained much interest in the research community over the last few years as they provide a promise for unsupervised representation learning. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have been one of the most successful generative models till date. Following its introduction in 2014, significant progress has been made towards improving the stability, quality and the diversity of the generated images (Salimans et al., 2016; Karras et al., 2017). While GANs have been successful in the image domain, recent efforts have extended it to other modalities such as texts (Wang et al., 2018a), graphs (Wang et al., 2018b), etc. In this work, we focus on the less studied domain of videos. Generating videos are much harder than images because the additional temporal dimension makes generated data extremely high dimensional, and the generated sequences must be both photo-realistically diverse and temporally consistent. We tackle the problem of text-conditioned video synthesis where the input is a text description and the goal is to synthesize a video corresponding to the input text. This problem has many potential applications, some of which include producing multimedia special effects, generating synthetic data for model-based Reinforcement Learning systems and domain adaptation, etc. Two recent works that address the problem of text-conditioned video generation include Li et al. (2018) and Pan et al. (2017). Both these methods are variants of conditional GAN model applied to the video data. In spite of some successes, they have the following limitations: (1) They employ 3D transposed convolution layers in the generator network, which constrains them to only produce fixed-length videos. (2) Their models are trained on low-resolution videos - results are shown only at a 64×64 resolution. (3) Text conditioning is performed using a simple concatenation of video and text features in the discriminator: Such a conditioning scheme may perform well on certain datasets, but has difficulty in capturing rich video-text variations. In this work, we aim to address all the concerns above. First, to model videos of varying length, we use a recurrent neural network in the latent space and employ a shared frame generator network similar to (Tulyakov et al., 2018). Second, we present a model for generating high-resolution videos by using a Resnet-style architecture in the generator and the discriminator network. Third, we propose a new multi-scale text-conditioning scheme based on convolutional filter generation to strengthen the associations between the conditioned text and the generated video. We call our model Text-Filter conditioning GAN (TFGAN). Finally, we construct a benchmark synthetic moving shapes dataset to extensively evaluate the effectiveness of the new conditioning scheme we proposed. In summary, our contributions in this work are as follows: (i) A new conditional GAN with an effective multi-scale text-conditioning scheme based on convolutional filter generation is proposed; (ii) A benchmark synthetic dataset for studying text conditioning in video generation is presented; (iii) Photo-realistic video synthesis is achieved using a deeper generator-discriminator architecture.Two popular approaches to generative modeling include GANs (Goodfellow et al., 2014) and Variational Autoencoders(VAEs) (Kingma & Welling, 2014). GANs are formulated as a 2− player minimax game between a generator and a discriminator network, while VAEs are based on variational inference where a variational lower bound of observed data log-likelihood is optimized. Among the two approaches, GANs have generated significant interest as they have been shown to produce images of high sample fidelity and diversity (Karras et al., 2017). A variant of GAN models are conditional GANs where the generator network is conditioned on input variables of interest. Such a conditioning input can be labels (Odena et al., 2017), attributes (Yan et al., 2016), text (Zhang et al., 2017; Xu et al., 2018) or even images (Zhu et al., 2017). We focus on text conditioning since it is relevant to this work. One of the first works to perform textconditioned image synthesis is Reed et al. (2016). Their method was only shown to synthesize low-resolution images. To improve the resolution, Zhang et al. (2017) proposed stacking multiple GAN architectures, each producing images of increasing resolution. While the above two methods perform conditioning using the global text representation, Xu et al. (2018) adopts an attention mechanism to focus on fine-grained word-level representations to enable improved conditioning. While image generation is a well studied problem, there has been very little progress in the domain of video generation. Vondrick et al. (2016) proposed a GAN architecture based on 3D convolutions to generate video sequences, but it can only generate fixed-length videos. Tulyakov et al. (2018) proposed using a recurrent neural network in the latent space to model videos of varying lengths. While these models are not designed to handle conditional video generation, Li et al. (2018) and Pan et al. (2017) perform text-conditioned video synthesis by using the sentence embedding as a conditional input. However, both of these conditional generative models are based on 3D convolutions, they can only produce fixed-length low-resolution videos. In this work, we address this issue by developing an architecture capable of producing high-resolution videos of varying length.We first provide a formal description of the problem being address. We are given access to n data points {(vi, ti)}ni=1 sampled from an underlying joint distribution p(v, t) in the video-sentence space. Here, each vi ∈ RT×C×W×H is a video clip and ti is a sentence description. We are interested in learning a model capable of sampling from the unknown conditional distribution p(v|t). Similar to conditional GANs, we formulate the problem as learning a transformation function G(z, t) from a known prior distribution Pz(z) and the conditional input variable t to the unknown conditional distribution p(v|t). The functionG is optimized using an adversarial training procedure.The framework of our proposed model is shown in Fig. 1. The text description t is passed to a text encoder T to get a frame-level representation tf and a video-level representation tv . Here, tf is a representation common to all frames, and contains frame-level information like background, objects, etc. from the text. The video representation tv extracts the temporal information such as actions, object motion, etc. The text representation along with a sequence of noise vectors {zi}li=1 are passed to a recurrent neural network to produce a trajectory in the latent space. Here, l denotes the number of frames in the video sequence. These sequence of latent vectors are then passed to a shared frame generator model G to produce the video sequence. The generated video is then fed to two discriminator models - DF and DV . DF is a frame-level discriminator that classifies if the individual frames in the video are real/fake, whereas the video discriminator DF is trained to classify the entire video as real/fake. The discriminator models DF and DV also take the text encoding tf and tv respectively as inputs so as to enforce text-conditioning.To build strong conditional models, it becomes important to learn good video-text associations in the GAN model. A standard technique is to sample negative (v, t) pairs (wrong associations) and train it as fake class, while the correct (v, t) pairs are trained as real class in the discriminator network. Since the generator is updated using the gradients from the discriminator network, it becomes important to effectively fuse the video and text representations in the discriminator so as to make the generator condition well on the text. Previous methods (Li et al., 2018; Pan et al., 2017) use a simple concatenation of text and video features as the feature fusion strategy. We found that this simple strategy produces poor conditioned models in datasets where there are rich text-video variations (refer to Section. 4 for more details). Our proposed model Text-Filter conditioning GAN (TFGAN) focuses on improving text conditioning. In TFGAN, we employ a scheme based on generating convolutional filters from the text features. This scheme, which we call Text-Filter conditioning, is shown in Fig. 2. Let us first divide the discriminator network D (which can be DF or DV ) into multiple sub-networks {D(i)}mi=1 so that D(x) = D(m) ◦D(m−1) ◦ . . . D(1)(x). These sub-networks can be as small as a single layer, or can be a cascade of multiple layers. Let d(i) denote the output of the ith sub-network of the discriminator. From the text features, we generate a set of convolution filters {fi}mi=1. Each filter fi is now convolved with the discriminator response d(i), and this convolved output is passed through additional convolutions after which they are pooled to get a single video-text representation. We use this pooled feature vector to classify the (v, t) pairs as real or fake. Because the generated convolutional filters {fi} are applied to discriminator sub-network outputs {d(i)} from different layers, the resulting text conditioning effectively imposes semantic constraints extracted from input texts to the generated individual frames and video clips at different feature abstraction levels.The discriminator model D and the generator model G are trained using an adversarial game as done in the standard conditional GANs. However, since we employ deep Resnet-style architectures for our G −D networks, it was important to stabilize the GAN training. We use the regularizer as proposed in Mescheder et al. (2018) where the norm of the discriminator gradients are penalized. With this regularizer, our optimization objective can be expressed as Lreal =E(v,t)∼pdata,real [log(D(v, T (t))) + γ 2 ‖∇D(v, t)‖2] (1) Lfake = 1 2 [E(v,t)∼pdata,fake log(1−D(v, T (t))) + Ez∼pz log(1−D(G(z, T (t)), T (t)))] (2) min G max D Lreal + Lfake (3) The text encoder T is optimized as follows max T LT = E(v,t)∼pdata,real log(D(v, T (t))) + E(v,t)∼pdata,fake log(1−D(v, T (t))) (4) In the above set of equations, pdata,real denotes the real data distribution with correct video-text correspondences, whereas pdata,fake refers to the distribution with incorrect video-text correspondences. Note that we have two discriminator networks - DF , DV in our models, and the above equations have to be repeated for both models. Eq.1-4 are optimized by alternating between the minimization and maximization problems as done in standard GAN. Please refer to Appendix A for the detailed training algorithm.This section discusses the experimental validation of our TFGAN model. We first describe a benchmark synthetic dataset we created for the task of text-to-video generation, and use it to better analyze our system. Then, we show results on a challenging real-world video dataset - the Kinetics human action video dataset (Kay et al., 2017). Finally, we show how our method can be extended to the task of text-to-image synthesis and show results on the CUB birds dataset (Welinder et al., 2010).To better understand the task of text-to-video synthesis, we created a dataset of moving shapes where a shape moves along a trajectory as described by the corresponding text description. This synthetic dataset has 5 control parameters: shape type, size, color, motion type and motion direction. Of these, the first three parameters are frame-level attributes while the last two are temporal attributes. The set of possible values each parameter can take is listed in Appendix B. The combination of all parameters results in 360 unique parameter configurations. Some samples from this dataset are shown in Fig. 3a. We call this dataset Shapes-v1 dataset. The above dataset contains videos with static background (all black). While this is a reasonable assumption to make, it is hardly true in practice as many videos have dynamic backgrounds. So, we create a second dataset called Shapes-v2 dataset which is a version of Moving Shapes dataset with dynamic backgrounds. To generate the background, we choose images from the Kylberg Texture Dataset (Kylberg, 2011) and sample a sequence of patches corresponding to a random trajectory. Each patch in this sequence forms the background of an individual frame in the video. These background textures are blended with the moving object resulting in videos as shown in Fig. 3b. This dataset is much more challenging than the Shapes-v1 dataset as the generative models should learn to ground the text description to the moving object but not to the randomly moving background. Both these datasets were created with videos containing 16 frames at a 64× 64 frame resolution.An important advantage of creating the synthetic dataset is that it provides a framework for quantitative evaluation of the text-conditioning. First, we train five attribute classifiers (shape, size, color, motion and direction classifiers) on the real data using the ground truth attributes (we have access to ground-truth attributes as we stored them while creating the dataset). We then use these trained attribute classifiers to verify if the attributes of the generated videos correspond to those described by the input text in the test set. For each text description in the test set, we generate the video using our TFGAN model and measure the attribute prediction accuracy. Higher this accuracy, better conditioned is our GAN model. We experiment with the following models: (1) FeatCat: a conditional GAN model trained using simple text-video feature concatenation in the discriminator network (2) FeatCat branchingD: conditional GAN model with branching D structure where responses at intermediate layers of D are pooled, and this pooled feature is concatenated with text embedding. This model is essentially TFGAN but without performing the convolutions from text-filters, and (3) TFGAN with Text-Filter conditioning. The architecture and hyper-parameter details are described in the Appendix C. Some sample generations of our Text-Filter conditioned GAN model is shown in Fig. 3a and 3b Table 1 reports the quantitative evaluation of the three conditional GAN models on Shapes dataset. We observe that TFGAN with text-filter conditioning achieves the best performance among the three models on both the datasets. An important observation to note is that using a branching architecture in the discriminator network alone does not improve the text conditioning. This shows that the effectiveness of our method comes not from the branching architecture, but in how text conditioning is applied (using convolutions) at multiple layers of the discriminator network.In this section, we report some exploratory experiments we perform on the Shapes dataset. Sentence interpolation In this experiment, we depict conditional interpolation whereby frames in a video transition corresponding to the interpolation between two sentence descriptions. Let S1 and S2 denote the two sentences that are interpolated, and (tS1f , t S1 v ) and (t S2 f , t S2 v ) denote their corresponding feature representation obtained by passing through the text encoder T . For each frame to be generated, the corresponding conditioning feature is obtained by a linear interpolation between these two representations: (tif , t i v) = (1− α)(tS1f , tS1v ) + α(tS2f , tS2v ) Instead of using a fixed text representation (tf , tv) as conditioning argument to all the frames in the Generator network, we use (tif , t i v) as input to the frame i. The resulting interpolated videos are shown in Fig. 4. We observe that we are able to obtain smooth transitions. When interpolating between the blue square and the red square, we obtain some intermediate frames with pink shade. Interestingly, none of the samples in the dataset contain pink color. In the second figure, we observe a smooth decrease in the object size while the object continues to move in the specified trajectory. Generating novel categories To characterize if the model has learned to generalize and not naively memorize the dataset, this experiment aims to study the ability of our TFGAN model to produce videos not seen during training. Of the 360 unique parameter configurations in the Shapes dataset, we randomly hold out n configurations from the training set. After training the model on this training set, we feed the text descriptions from the held-out n configurations and measure the attribute classification accuracy in this set. In this experiment, we choose n = 20. The results are reported in Table 2. We observe that our model achieves good accuracy and this illustrates the ability of our method to generalize. Long Sequence Generation One of the benefits of using a RNN-based GAN model is that it allows us to model variable-length video sequences. To demonstrate this, we perform an experiment where we train our TFGAN model on 16-length video sequences and generate 32-length sequences. This can be performed easily as we could potentially generate a latent trajectory of any length using the RNN model in the latent space, and the videos are generated using a shared generated acting on this latent trajectory. Fig. 4 shows the output of one such 32-length sequence generated. We observe that the model is able to clearly perform the zig-zag motion beyond 16 frames.To demonstrate the practical relevance of our approach, we perform experiments on real-world video datasets. We use the dataset proposed in Li et al. (2018) for this purpose. This dataset contains videos of human actions, and was curated from YouTube and Kinetics human action video dataset Kay et al. (2017). The dataset contains the following action classes - biking in snow, playing hockey, jogging, playing soccer ball, playing football, kite surfing, playing golf, swimming, sailing and water skiing. This is an extremely challenging dataset for the task of video generation due to the following reasons: (1) videos are extremely diverse, and there are a lot of variations within the video, and (2) some videos have low-resolution and poor-quality video frames. Some sample videos from the dataset are shown in Fig. 5 The results of training our TFGAN model on the Kinetics dataset are shown in Fig. 5. We observe that our model is able to produce videos of much higher quality than the comparison method (Li et al., 2018). We are able to generate fine motions like golf swing, while Li et al. (2018) produces a blobby region. Also, we train the model to produce 128× 128 resolution videos, while the method in Li et al. (2018) was trained only on 64 × 64 videos. As done in Li et al. (2018), we report a simplified version of inception score whereby a video classification model is trained on the real data, and the accuracy on generated data is reported. We report the performance on the following five categories as done in Li et al. (2018): kite surfing, playing golf, biking in snow, sailing, swimming and water skiing. As can be seen from Table. 3, our methods achieves significantly higher accuracy than the method in Li et al. (2018). In-set refers to the performance obtained on the test set of real videos, thus serves as an upper bound. We report additional results, architecture and hyper-parameter details in Appendix C. 4.3 TEXT TO IMAGE GENERATION Text-to-image generation is a relatively easier problem than text-to-video generation due to the absence of temporal constraints. Although the focus of this paper is on text-to-video synthesis, our framework is flexible and can be trivially extended to the problem of text-to-image synthesis. This can be accomplished by removing the videolevel discriminator DV and the RNN network in the latent space. We train our GAN model with Text-Filter conditioning on the CUB-Birds dataset Welinder et al. (2010), a benchmark dataset for text-to-image generation. Some of the samples from the generated images are shown in Fig. 6. We observe that our model is able to produce photo-realistic images. We also report Inception score as a quantitative metric. As can be seen from Table. 4, our method achieves higher inception scores than the comparison methods.In this work, we address the problem of generating videos conditioned on text. We propose a novel text-conditioning framework whereby conditioning is performed using convolution operations on image feature maps with filters generated from text. To better understand the text conditioning, we construct a synthetic dataset and show that our conditioning scheme achieves superior performance compared to other techniques. Finally, by using deeper architectures in the discriminator and generator networks, we generate photo-realistic videos on the challenging Kinetics dataset.Section. 3 discusses the various loss terms we use in our TFGAN model. Equation 1 gives the expressions for the loss terms Lreal, Lfake and LT . Denoting LFreal, L F fake, L F T as the losses for frame-level discriminator DF and LVreal, L V fake, L V T as the losses for video-level discriminator DV , the training algorithm we use is mentioned in Alg. 1 Algorithm 1 Training algorithm Require: θ: Parameters of G, φF : Parameters of DF , φV : Parameters of DV , φT : Parameters of T Require: Niter: number of training iterations Require: α: Learning rate, Nb: batch size 1: for t in 1 : Niter do 2: Sample Nb real samples with correct video-text correspondence {(vri , tri )} Nb i=1 3: Sample Nb real samples with incorrect video-text correspondence {(vfi , t f i )} Nb i=1 4: Update DF : φF = φF + α∇[LFreal + LFfake] 5: Update DV : φV = φV + α∇[LVreal + LVfake] 6: Update G: θ = θ − α∇[LFreal + LFfake + LVreal + LVfake] 7: Update T : φT = φT + α∇[LFT + LFT + LVT + LVT ] 8: end forBoth Shapes-v1 and Shapes-v2 dataset contain videos with objects moving along a specific trajectory. There are 5 control parameters - shape, size and color of object, type and direction of motion. Table 5 lists the possible values each parameter can take. The (shape, color, size) tuple describes the structure of the object, while (motion type, direction) tuple dictates how the object moves in the video. For straight line and zig-zag motion, the object could move in north, south, west and east direction, while for diagonal motion, the possible directions include north-west, north-east, south-west and south-east. The zig-zag motion was generated using a sinusoidal function.We first define some basic architecture blocks: • ResnetBlock(x, y): x + Conv2D(x → y, kernel=3, str=2, pad=1) if x == y, else Shortcut(x → y) + Conv(x− >y, 3 × 3, str=2, pad=1). Here, Shortcut(x → y) is the 1 × 1 convolution that is maps from x filters to y filters. This is a simplified resnet block that was proposed in He et al. (2016). • ResnetBlock3D(x, y): x + Conv3D(x → y, kernel=3, str=2, pad=1) if x == y, else Shortcut(x→ y) + Conv3D(x− >y, 3× 3, str=2, pad=1) • Text-Img feat concat: Can be any feature concatenation technique. We use Text-Filt conditioning as discussed in Sec. 3In all our experiments, we used the following architecture for Text-Filter conditioning. The discriminator (both DV and DF ) were divided into three sub-networks (m = 3). In Moving Shapes experiments, the sub-network D(0) is the network till Resnet-1 block, D(1) is the sub-network from Resnet-2 block till the end of Renset-3 block and D(3) forms the rest of the network. For the Kinetics experiment, we the sub-network D(0) is the network till Renset-11 block, D(1) spans from Renset-20 block to Resnet-31 block, and D(3) forms the rest of the network. We first take the output of D(0) and apply 1D convolution to bring the number of filters to 8. So, if the output ofD(0) was a n×k×wmap, this transformation will bring it down to 8×k×w. Let the text embedding (which is obtained by passing through the text encoder) be a d− dimensional vector. We first apply a FC(d→ 5.5.8.8) to this embedding and reshape it to 8×8×5×5 filter. This filter is then convolved with the transfomed outputs of D0. These resulting convolved feature maps are passed through two conv-2D layers with Avg-Pool and then reshaped to 128 dimensional vector. The same procedure is done for D(1). For D(2), we just take the output and pass it through a fully connected layer to get 128 dimensional vector. These three 128 dimensional vectors are concatenated to get one resulting vector which classifies if the input is real or fake. The exact sample procedure is repeated for Video discriminators, only difference being the size of Text-Filters: 3D filters of size 3 × 5 × 5 is used instead of 5× 5 filter . So, the FC applied on text embedding will be FC(d→ 5.5.3.8.8)For Shapes-v1 and Shapes-v2 datasets, we used an architecture based on ResnetBlock as shown in Tables. 6, 7 and 8. For the text encoder, we first obtained the GloVE embeddings of individual words, then applied a 1D-CNN based network with the following network architecture: Conv1D(512, kernel=3) → ReLU → MaxPool(2) → Conv1D(512, kernel=3) → ReLU → MaxPool(2)→ Conv1D(256) 1D-CNN was sufficient in this case as most sentences were of rougly similar lenghts. The inputs were zero padded to making every sentence have the dimension. We tried using a LSTM model and it gave similar performance as 1D CNN.The architectures used in Kinetics dataset are shown in Tables. 9, 11 and 10. For the text encoder, we used the same architecture as that of Shapes dataset.Some additional results on the Kinetics dataset are shown in Fig. 7. We observe that we are able to generate videos of high quality. To illustrate the variations that occur within a class, we generate multiple videos of the same text description. Figure. 8 shows one such example for swimming class. We find that out model is capable of generating diverse predictions.In this experiment, we consider Epic-Kitchens dataset (Kay et al., 2017) which is a ego-centric dataset containing videos of people cooking in a kitchen. The videos were recorded using a highdefinition head mounted camera. The dataset is annotated with text descriptions of step-by-step instructions of a cooking recipe with the corresponding timestamp as people perform the action. We extracted clips of 16 frames containing the following action classes: ’take’, ’cut’, ’dicing’, ’pour’, ’stir’, ’wash’, ’grate’, ’rinse’, ’put’. This resulted in 4793 (video, text) pairs. We trained our TFGAN model on this dataset. The generations we obtained are shown in Fig. 9. We observe that our model is able to produce good videos: In the first video, we observe the action of a person adding the cherry Conv3D 32× 16× 64× 64 3→ 32 ResnetBlock3D-0 64× 16× 64× 64 32→ 64 AvgPool3D 64× 8× 32× 32 - ResnetBlock3D-1 128× 8× 32× 32 64→ 128 AvgPool3D-spatial 128× 8× 16× 16 - ResnetBlock3D-2 128× 8× 16× 16 128→ 128 AvgPool3D 128× 4× 8× 8 - ResnetBlock3D-3 128× 4× 8× 8 128→ 128 AvgPool3D 128× 2× 4× 4 - ResnetBlock3D-4 256× 2× 4× 4 128→ 256 AvgPool3D 256× 1× 2× 2 - ResnetBlock3D-5 256× 1× 2× 2 256→ 256 Reshape 256.2.2 - Text-Img feat concat nfused - Fully connected nfused → 1 nfused → 1 tomato to the bowl and taking the hands off. The images of water splashing and spinach placed in the side can be seen from the third video.Some additional results on CUB-Birds dataset are shown in Fig. 10. We see that our model can generate photo-realistic images with good variations.
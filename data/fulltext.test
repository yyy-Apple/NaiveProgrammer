Recurrent neural networks (RNNs) model sequential data by observing one sequence element at a time and updating their internal (hidden) state towards being useful for making future predictions. RNNs are theoretically appealing due to their Turing-completeness Siegelmann and Sontag (1995), and, crucially, have been tremendously successful in complex real-world tasks, including machine translation Cho et al. (2014); Sutskever et al. (2014), language modelling Mikolov et al. (2010), and reinforcement learning Mnih et al. (2016). Still, training RNNs in practice is one of the main open problems in deep learning, as the following issues prevail. (1) Learning long-term dependencies is extremely difficult because it requires that the gradients (i.e. the error signal) have to be propagated over many steps, which easily causes them to vanish or explode Hochreiter (1991); Bengio et al. (1994); Hochreiter (1998). (2) Truncated Backpropagation Through Time (TBPTT) Williams and Peng (1990), the standard training algorithm for RNNs, requires memory that grows linearly in the length of the sequences on which the network is trained. This is because all past hidden states must be stored. Therefore, the memory requirements of training RNNs with large hidden states on long sequences become prohibitively large. (3) In TBPTT, parameters cannot be updated until the full forward and backward passes have been completed. This phenomenon is known as the parameter update lock Jaderberg et al. (2017). As a consequence, the frequency at which parameters can be updated is inversely proportional to the length of the time-dependencies that can be learned, which makes learning exceedingly slow for long sequences. The problem of vanishing/exploding gradients has been alleviated by a plethora of approaches ranging from specific RNN architectures Hochreiter and Schmidhuber (1997); Cho et al. (2014) to optimization techniques aiming at easing gradient flow Martens and Sutskever (2011); Pascanu et al. (2013). A candidate for effectively resolving the vanishing/exploding gradient problem is hierarchical RNNs (HRNNs) Schmidhuber (1992); El Hihi and Bengio (1996); Koutnik et al. (2014); Sordoni et al. (2015); Chung et al. (2016). In HRNNs, the network itself is split into a hierarchy of levels, which are updated at decreasing frequencies. As higher levels of the hierarchy are updated less frequently, these architectures have short (potentially logarithmic) gradient paths that greatly reduce the vanishing/exploding gradients issue. In this paper, we show that in HRNNs, the lower levels of the hierarchy can be decoupled from the higher levels, in the sense that the gradient flow from higher to lower levels can effectively be replaced by locally computable losses. Also, we demonstrate that in consequence, the decoupled HRNNs admit training with memory decreased by a factor exponentially in the depth of the hierarchy compared to HRNNs with standard TBPTT. The local losses stem from decoder networks which are trained to decode past inputs to each level from the hidden state that is sent up the hierarchy, thereby forcing this hidden state to contain all relevant information. We experimentally show that in a diverse set of tasks which rely on long-term dependencies and include deep hierarchies, the performance of the decoupled HRNN with local losses is indistinguishable from the standard HRNN. In summary, we introduce a RNN architecture with short gradient paths that can be trained memoryefficiently, thereby addressing issues (1) and (2). In the bigger picture, we believe that our approach of replacing gradient flow in HRNNs by locally computable losses may eventually help to attempt solving issue (3) as well.Several techniques have been proposed to deal with the memory issues of TBPTT Chen et al. (2016); Gruslys et al. (2016). Specifically, they trade memory for computation by storing only certain hidden states and recomputing the missing ones on demand. This is orthogonal to our ideas and thus, both can potentially be combined. The memory problem and the update lock have been tackled by online optimization algorithms such as Real Time Recurrent Learning (RTRL) Williams and Zipser (1989) and its recent approximations Tallec and Ollivier (2017); Mujika et al. (2018); Benzing et al. (2019); Cooijmans and Martens (2019). Online algorithms are promising, as the parameters can be updated in every step while the memory requirements do not grow with the sequence length. Yet, large computation costs and noise in the approximations make these algorithms impractical so far. Another way to deal with the parameter update lock and the memory requirements are Decoupled Neural Interfaces, introduced by by Jaderberg et al. in Jaderberg et al. (2017). Here, a neural network is trained to predict incoming gradients, which are then used instead of the true gradients. HRNNs have been widely studied over the last decades. One early example by Schmidhuber in Schmidhuber (1992) proposes updating the upper hierarchy only if the lower one makes a prediction error. El Hihi and Bengio showed that HRNNs with fixed but different frequencies per level are superior in learning long-term dependencies El Hihi and Bengio (1996). More recently, Koutník et al. introduced the Clockwork RNN Koutnik et al. (2014), here the RNN is divided into several modules that are updated at exponentially decreasing frequencies. Many approaches have also been proposed that are explicitly provided with the hierarchical structure of the data Sordoni et al. (2015); Ling et al. (2015); Tai et al. (2015), for example character-level language models with word boundary information. Interestingly, Chung et al. Chung et al. (2016) present an architecture where this hierarchical structure is extracted by the network itself. As our model utilizes fixed or given hierarchical structure, yet does not learn it, models that can learn useful hierarchies may improve the performance of our approach. Auxiliary losses in RNNs have been used to improve generalization or the length of the sequences that can be learnt. In Schmidhuber (1992), Schmidhuber presented an approach, where a RNN should not only predict a target output to solve the task, but also its next input as an auxiliary task. More recently, Goyal et al. showed that in a variational inference setting, training of the latent variables can be eased by an auxiliary loss which forces the latent variables to reconstruct the state of a recurrent network running backwards Goyal et al. (2017). Subsequently, Trinh et al. demonstrated that introducing auxiliary losses at random anchors in time which aim to reconstruct or predict previous or subsequent input subsequences significantly improve optimization and generalization of LSTMs Hochreiter and Schmidhuber (1997) and helps to learn time dependencies beyond the truncation horizon of TBPTT. The main difference to our approach is that they use auxiliary losses to incentivize an RNN to keep information in memory. This extends the sequences that can be solved by an additive factor through an echo-state network-like effect. On the contrary, we use auxiliary losses to replace gradient paths in hierarchical models. This, together with the right memorization scheme, allows us to discard all hidden states of the lower hierarchical levels when doing TBPTT, which reduces the memory requirements by a multiplicative factor.In this section we describe our approach in detail. We start by defining a standard HRNN, then explain how cutting its gradient flow saves memory during training, and conclude by introducing an auxiliary loss, that can be computed locally, which prevents a performance drop despite the restricted gradient flow (as shown in Section 4 below).The basis of our architecture is a simple HRNN with two hierarchical levels (deeper HRNNs are considered below). We describe it in terms of general RNNs. Such a RNNX is simply a parameterized and differentiable function fX which maps a hidden state h and an input x to a new hidden state h′ = fX(h, x). The HRNN consists of the lower RNN (indicated by superscript L) and the upper RNN (superscript U ). The lower RNN receives the input xt and generates the output yt in addition to the next hidden state ht at every time-step. Every k steps, the upper RNN receives the hidden state of the lower RNN, updates its own hidden state and sends its hidden state to the lower RNN. Then, the hidden state of the lower RNN is reset to all zeros. The update rules are summarized in Equation (1) for the lower and Equation (2) for the upper RNN. The unrolled computation graph is depicted in Figure 1. hLt = { fL(0, [xt, h U t ]) if t mod k = 0, fL(hLt−1, [xt, 0]) else. (1) hUt = { fU (hUt−1, h L t−1) if t mod k = 0, hUt−1 else. (2) While we have outlined the case for two hierarchical levels, this model naturally extends to deeper hierarchies by applying it recursively (see Section 4.4). This leads to levels that are updated exponentially less frequently. Moreover, the update frequencies can also be flexible in case the hierarchical structure of the input is explicitly provided (see Section 4.3).In TBPTT, the network is first unrolled for T steps in the forward pass, see Figure 1, while the hidden states are stored in memory. Thereafter, the gradients of the loss with respect to the parameters are computed using the stored hidden states in the backward pass, see Figure 2 (a). To save memory during gradient computation, we simply ignore the edges from higher to lower levels in the computation graph of the backward pass, see Figure 2. Importantly, the resulting gradients are not the true gradients and we term them restricted gradients as they are computed under restricted gradient flow. We will refer to the restricted gradients with the ∂̃ symbol.Thus, ∂̃h U t ∂̃hLt−1 = 0 if t is a time-step when the upper RNN ticks (i.e. t mod k = 0) and it is equal to the true partial derivatives everywhere else. We call the HRNN that is trained using these restricted gradients the gradient restricted HRNN (gr-HRNN). Before we address the issue of compensating for the restricted gradients in the next section, we first analyze how much memory is needed to compute the restricted gradients. In the following, let t be a time-step when the upper RNN ticks (i.e. t mod k = 0). A direct consequence of the restricted gradients is that ∑T i=t ∂̃Li ∂̃hLt−1 = 0, where Li is the loss at time-step i and we consider the network unrolled for time-steps 1, . . . , T . Therefore, right before the upper RNN updates, ∂̃Li ∂̃θL , where θ are the parameters of the network, can be computed for the previous k time-steps using just the previous k hidden states, hLt−k, . . . , h L t−1, of the lower RNN. We further need these hidden states to compute ∑k i=1 ∂̃Lt−i ∂̃hUt−k . However, we will not need the hidden states hLt−k, . . . , h L t−1 for any other step of the backward pass and thus, we do not need to keep them in memory. Therefore, the restricted gradients can be computed using memory proportional to k for the hidden states hLt−k, . . . , h L t−1 of the lower RNN and proportional to 2T/k for both the T/k hidden states of the upper RNN and the T/k accumulated restricted gradients of the loss with respect to the hidden states of the upper RNN. Standard TBPTT requires memory proportional to T in order to compute the true gradients. Here, we showed that the restricted gradients can be computed using memory proportional to k + 2T/k, thus improving by a factor of 2/k. For (deep) HRNNs with l levels, the above argument can be applied recursively, which yields memory requirements of (l − 1)k for the hidden states of the lower RNNs and 2T/kl−1 for the uppermost RNN.In the gr-HRNN, the lower RNN is not incentivized to provide a useful representation as input for the upper RNN. However, it is clear that if this hidden state which is sent upwards contains all information about the last k inputs, then the upper RNN has access to all information about the input, and thus the gr-HRNN should be able to learn as effectively as the HRNN without gradient restrictions. Hence, we introduce an auxiliary loss term to force this hidden state to contain all information about the last k inputs. This auxiliary loss comes from a simple feed-forward decoder network, which given this hidden state and a randomly sampled index i ∈ {1, . . . , k} (in one-hot encoding) must output the i-th previous input. Then, the combined loss of the HRNN is the sum of the loss defined by the task and the auxiliary decoder loss multiplied by a hyper-parameter β. For hierarchies with l levels, we add l − 1 decoder networks with respectively weighted losses. Crucially, the auxiliary loss can be computed locally in time. Thus, the memory requirements for training increase only by an additive term in the size of the decoder network.In this section, we experimentally test replacing the gradient flow from higher to lower levels by adding the local auxiliary losses. Therefore, we evaluate the gr-HRNN with auxiliary loss (simply termed ’ours’ from here on) on four different tasks: Copy task, pixel MNIST classification, permuted pixel MNIST classification, and character-level language modeling. All these tasks require that the model effectively learns long-term dependencies to achieve good performance. Additionally, to our model, we also evaluate several ablations/augmentations of it to fully understand the importance and effects of its individual components. These other models are: • HRNN: This model is identical to ours, except that all gradients from higher to lower levels are propagated. The weight of the auxiliary loss is a hyper-parameter which is set using a grid-search (as the auxiliary loss may improve the performance, we keep it here to make a fair comparison with our model). While this model has a much larger memory cost for training, it provides an upper bound on the performance we can expect from our model. • gr-HRNN: This model is equivalent to our model, except that the weight of the auxiliary loss is set to zero. Thereby, it permits studying if using the auxiliary loss function (and the corresponding decoder network) is really necessary when the gradients are stopped. • mr-HRNN: This model is identical to the HRNN, except that it is trained using only as much memory as our model requires for training. That is, it is unrolled for a much smaller number of steps than the other models. In consequence, it admits a fair performance comparison (in terms of memory) to our model. If not mentioned otherwise below, the parameters are as follows. All RNNs are LSTMs with a hidden state of 256 units. The network for the auxiliary loss is a two layer network with 256 units, a ReLU Glorot et al. (2011) non-linearity and uses either the cross-entropy loss for discrete values or the mean squared loss for continuous ones. For each model we pick the optimal β from [10−3, 10−2, 10−1, 0, 100, 101, 102], except for the gr-HRNN for which β = 0. The models are trained using a batch size of 100 and the Adam optimizer Kingma and Ba (2014) with the default Tensorflow Abadi et al. (2015) parameters: learning rate of 0.001, β1 = 0.9 and β2 = 0.999.In the copy task, the network is presented with a binary string and should afterwards output the presented string in the same order (an example for a sequence of length 5 is: input 01101***** with target output *****01101). We refer to the maximum sequence length that a model can solve (i.e. achieves loss less than 0.15 bits/char) as Lmax. Table 1 summarizes the results for a sequence of length 100. The copy task requires exact storage of the input sequence over many steps. Since the length of the sequence and therefore the dependencies are a controllable parameter in this task, it permits to explicitly assess how long dependencies a model can capture. Both our model and the model using all gradients (HRNN) achieve very similar performance, which is limited by the number of steps for which the network is unrolled in training. Moreover, the auxiliary loss is necessary, as the model without it (gr-HRNN) performs poorly. Moreover, our model drastically outperforms the model with all gradients, when given the same memory budget, as the memory restricted HRNN (mr-HRNN) also performs poorly. Notably, our model is remarkably robust to the choice of the auxiliary loss weight β, as for all values except 0 it learns sequences of length at least 100. Finally, the HRNN actually uses the capacity of the decoder network, as the grid-search yields a nonzero β. All models, except the mr-HRNN, are unrolled for T = 200 steps and the upper RNN ticks every k = 10 steps of the lower one. To equate memory budgets, the mr-HRNN is unrolled for 2T/k+ k = 50 steps. Hence, for sequence lengths at most 100, gradients are propagated through the whole sequence. The model parameters are only updated once per batch.In the pixel MNIST classification task, the pixels of MNIST digits are presented sequentially. After observing the whole sequence the network must predict the class label of the presented digit. The permuted pixel MNIST task is exactly the same, but the pixels are presented in a fixed random order. Table 2 summarizes the accuracy of the different models on the two tasks. Both pixel MNIST classification tasks require learning long-term dependencies, especially when using the default permutation, as the most informative pixels are around the center of the image (i.e. in the middle of the input sequence) and the class prediction is only made at the end. Additionally, unlike in the copy task, the input has to be processed in a more complex manner in order to make a class prediction. The results on both tasks are in line with the copy task. Our model performs on par with the model using the true gradient (HRNN). Again, the auxiliary loss is necessary, as the accuracy of the grHRNN is significantly worse. Given the same memory budget, our model outperforms the HRNN with all gradients. All models are unrolled for T = 784 steps (the number of pixels of an MNIST image) except for the memory restricted model which is unrolled for 166 steps. The upper RNN ticks every k = 10 steps of the lower one. The same permutation is used for all runs of the permuted task.In the character-level language modeling task, a text is presented character by character and at each step, the network must predict the next character. The results for this task on the Penn TreeBank corpus Marcus and Marcinkiewicz (1993) are summarized in Table 3. In contrast to the previous tasks, character-level language modeling contains a complex mix of both short- and long-term dependencies, where short-term dependencies typically dominate long-term dependencies. Thus, one may expect that replacing the true gradients by the local loss is the most harmful here. The performance of all 3 models is very close in this task, even for the gr-HRNN, which has neither gradients nor the auxiliary loss to capture long-term dependencies. We believe that this is due to the dominance of short-term dependencies in character-level language modeling. It has been widely reported that the length of the unrolling, and thus long-term dependencies, have a minimal effect on the final performance of character-level language models Jaderberg et al. (2017), particularly for small RNNs, as in our experiments. Still, we observe the best performance when using the true gradients (HRNN), a slight degradation when replacing gradients by the auxiliary loss (our), which slightly improves on not using the auxiliary loss (gr-HRNN). Here, we explicitly provide the hierarchical structure of the data to the model by updating the upper RNN once per word, while the lower RNN ticks once per character. As discussed in Section 2, there are models that can extract the hierarchical structure, which is a separate goal from ours. We refrain from comparing with a memory restricted model and displaying memory budgets because the dynamic memory requirements depending on the input sequence prohibit a fair comparison. However, as stated earlier, the unroll length usually has a minimal effect on the performance of character-level language modeling. We unroll the models for 50 characters and use an upper RNN with 512 units to deal with the extra complexity of the task.In deeper hierarchies, the output modalities of the individual decoder networks are different: whereas the decoder network in the lowest level has to predict elements of the input sequence (e.g. bits in the copy task), the decoder networks in higher levels have to predict hidden states of the lower level’s RNNs. Here, we confirm that our approach generalizes to deeper hierarchies. In particular, we use the copy task with sequence length 500 (and truncation horizon 1000) to test this. We consider a HRNN with 3 levels, where the lowest RNN is updated in every step, the middle RNN every 5 steps and the upper RNN every 25 steps, where no gradients from higher to lower RNNs are propagated. We compare using the auxiliary loss only on the lowest level with using it in the lowest and the middle layer. Thereby, we check whether applying the auxiliary loss in the middle RNN (i.e. over hidden states rather than raw inputs) is necessary to solving the task. The RNNs have 64, 256 and 1024 units (from lower to higher levels). The β for the lowest level is 0.1 and 1 for the middle level. All other experimental details are kept as in Section 4.1. We run several repetitions which are shown in Figure 3. Without the auxiliary loss in the middle layer, the network cannot solve the task (in fact, not even close as performance is close to chance level). However, when the auxiliary loss is added also to the middle layer, the model can solve the task perfectly on every run.In this paper, we have shown that in hierarchical RNNs the gradient flow from higher to lower levels can be effectively replaced by locally computable losses. This allows memory savings up to an exponential factor in the depth of the hierarchy. In particular, we first explained how not propagating gradients from higher to lower levels permits these memory savings. Then, we introduced auxiliary losses that encourage information to flow up the hierarchy. Finally, we demonstrated experimentally that the memory-efficient HRNNs with our auxiliary loss perform on par with the memory-heavy HRNNs and strongly outperform HRNNs given the same memory budget on a wide range of tasks, including deeper hierarchies. High capacity RNNs, like Differentiable Plasticity Miconi et al. (2018), or Neural Turing Machines Graves et al. (2014) have been shown to be useful and even achieve state-of-the-art in many tasks. However, due to the memory cost of TBPTT, training such models is often impractical for long sequences. We think that combining these models with our techniques in future work could open the possibility for using high capacity RNNs for tasks involving long-term dependencies that have been out of reach so far. Still, the problem of the parameter update lock remains. While this is the most under-explored of the three big problems when training RNNs (vanishing/exploding gradients and memory requirements being the other two), resolving it is just as important in order to be able to learn long-term dependencies. We believe that the techniques laid out in this work (i.e. replacing gradients in HRNNs by locally computable losses) can be a stepping stone towards solving the parameter update lock. We leave this for future work.
Polynomial feature expansion has long been used in statistics to approximate nonlinear functions Gergonne (1974); Smith (1918). The compressed sparse row (CSR) matrix format is a widelyused data structure to hold design matrices for statistics and machine learning applications. However, polynomial expansions are typically not performed directly on sparse CSR matrices, nor on any sparse matrix format for that matter, without intermediate densification steps. This densification not only adds extra overhead, but wastefully computes combinations of features that have a product of zero, which are then discarded during conversion into a sparse format. We provide an algorithm that allows CSR matrices to be the input of a polynomial feature expansion without any densification. The algorithm leverages the CSR format to only compute products of features that result in nonzero values. This exploits the sparsity of the data to achieve an improved time complexity of O(dkDk) on each vector of the matrix where k is the degree of the expansion, D is the dimensionality, and d is the density. The standard algorithm has time complexity O(Dk). Since 0 ≤ d ≤ 1, our algorithm is a significant improvement. While the algorithm we describe uses CSR matrices, it could be modified to operate on other sparse formats.Matrices are denoted by uppercase bold letters thus: A. The ithe row of A is written ai. All vectors are written in bold, and a, with no subscript, is a vector. A compressed sparse row (CSR) matrix representation of an r-row matrix A consists of three vectors: c, d, and p and a single number: the number of columns of A. The vectors c and d contain the same number of elements, and hold the column indices and data values, respectively, of all nonzero elements of A. The vector p has r entries. The values in p index both c and d. The ith entry pi of p tells where the data describing nonzero columns of ai are within the other two vectors: cpi:pi+1 contain the column indices of those entries; dpi:pi+1 contain the entries themselves. Since only nonzero elements of each row are held, the overall number of columns of A must also be stored, since it cannot be derived from the other data. Scalars, vectors, and matrices are often referenced with the superscript k. This is not to be interpreted as an exponent, but to indicate that it is the analogous aspect of that which procedes it, but in its polynomial expansion form. For example, c2 is the vector that holds columns for nonzero values in A’s quadratic feature expansion CSR representation. For simplicity in the presentation, we work with polynomial expansions of degree 2, but continue to use the exponent k to show how the ideas apply in the general case. ∗Now at Google †The authors contributed equally important and fundamental aspects of this work. We do provide an algorithm for third degree expansions, and derive the big-O time complexity of the general case. We have also developed an algorithm for second and third degree interaction features (combinations without repetition), which can be found in the implementation.In this section, we present a strawman algorithm for computing polynomial feature expansions on dense matrices. We then modify the algorithm slightly to operate on a CSR matrix, in order to expose its infeasibility in that context. We then show how the algorithm would be feasible with an added component, which we then derive in the following section.A natural way to calculate polynomial features for a matrix A is to walk down its rows and, for each row, take products of all k-combinations of elements. To determine in which column of Aki products of elements in Ai belong, a simple counter can be set to zero for each row of A and incremented efter each polynomial feature is generated. This counter gives the column of Aki into which each expansion feature belongs. SECOND ORDER (k = 2) DENSE POLYNOMIAL EXPANSION ALGORITHM(A) 1 N = row count of A 2 D = column count of A 3 Ak = empty N × ( D 2 ) matrix 4 for i = 0 to N − 1 5 cp = 0 6 for j1 = 0 to D − 1 7 for j2 = j1 to D − 1 8 Akicp = Aij1 ·Aij2 9 cp = cp + 1Now consider how this algorithm might be modified to accept a CSR matrix. Instead of walking directly down rows of A, we will walk down sections of c and d partitioned by p, and instead of inserting polynomial features into Ak, we will insert column numbers into ck and data elements into dk. INCOMPLETE SECOND ORDER (k = 2) CSR POLYNOMIAL EXPANSION ALGORITHM(A) 1 N = row count of A 2 pk = vector of size N + 1 3 pk0 = 0 4 nnzk = 0 5 for i = 0 to N − 1 6 istart = pi 7 istop = pi+1 8 ci = cistart:istop 9 nnzki = (|ci| 2 ) 10 nnzk = nnzk + nnzki 11 pki+1 = p k i + nnz k i // Build up the elements of pk, ck, and dk 12 pk = vector of size N + 1 13 ck = vector of size nnzk 14 dk = vector of size nnzk 15 n = 0 16 for i = 0 to N − 1 17 istart = pi 18 istop = pi+1 19 ci = cistart:istop 20 di = distart:istop 21 for c1 = 0 to |ci| − 1 22 for c2 = c1 to |ci| − 1 23 dkn = dc0 · dc1 24 ckn =? 25 n = n+ 1 The crux of the problem is at line 24. Given the arbitrary columns involved in a polynomial feature of Ai, we need to determine the corresponding column of Aki . We cannot simply reset a counter for each row as we did in the dense algorithm, because only columns corresponding to nonzero values are stored. Any time a column that would have held a zero value is implicitly skipped, the counter would err. To develop a general algorithm, we require a mapping from columns of A to a column of Ak. If there are D columns of A and ( D k ) columns of Ak, this can be accomplished by a bijective mapping of the following form: (j0, j1, . . . , jk−1) pj0j1...ik−1 ∈ {0, 1, . . . , ( D k ) − 1} (1) such that 0 ≤ j0 ≤ j1 ≤ · · · ≤ jk−1 < D where (j0, j1, . . . , jk−1) are elements of c and pj0j1...ik−1 is an element of ck.Within this section, i, j, and k denote column indices. For the second degree case, we seek a map from matrix indices (i, j) (with 0 ≤ i < j < D ) to numbers f(i, j) with 0 ≤ f(i, j) < D(D−1)2 , one that follows the pattern indicated by x 0 1 3x x 2 4x x x 5 x x x x  (2) where the entry in row i, column j, displays the value f(i, j). We let T2(n) = 12n(n + 1) be the nth triangular number; then in Equation 2, column j (for j > 0) contains entries with T2(j − 1) ≤ e < T2(j); the entry in the ith row is just i + T2(j − 1). Thus we have f(i, j) = i + T2(j − 1) = 1 2 (2i + j 2 − j). For instance, in column j = 2 in our example (the third column), the entry in row i = 1 is i+ T2(j − 1) = 1 + 1 = 2. With one-based indexing in both the domain and codomain, the formula above becomes f1(i, j) = 1 2 (2i+ j 2 − 3j + 2). For polynomial features, we seek a similar map g, one that also handles the case i = j. In this case, a similar analysis yields g(i, j) = i+ T2(j) = 12 (2i+ j 2 + j + 1). To handle three-way interactions, we need to map triples of indices in a 3-index array to a flat list, and similarly for higher-order interactions. For this, we’ll need the tetrahedral numbers T3(n) =∑n i=1 T2(n) = 1 6 (n 3 + 3n2 + 2n). For three indices, i, j, k, with 0 ≤ i < j < k < D, we have a similar recurrence. Calling the mapping h, we have h(i, j, k) = i+ T2(j − 1) + T3(k − 2); (3) if we define T1(i) = i, then this has the very regular form h(i, j, k) = T1(i) + T2(j − 1) + T3(k − 2); (4) and from this the generalization to higher dimensions is straightforward. The formulas for “higher triangular numbers”, i.e., those defined by Tk(n) = n∑ i=1 Tk−1(n) (5) for k > 1 can be determined inductively. The explicit formula for 3-way interactions, with zero-based indexing, is h(i, j, k) = 1 + (i− 1) + (j − 1)j 2 + (6) (k − 2)3 + 3(k − 2)2 + 2(k − 2) 6 . (7)With the mapping from columns of A to a column of Ak, we can now write the final form of the innermost loop of the algorithm from 3.2. Let the mapping for k = 2 be denoted h2. Then the innermost loop becomes: for c2 = c1 to |ci| − 1 j0 = cc0 j1 = cc1 cp = h 2(j0, j1) dkn = dc0 · dc1 ckn = cp n = n+ 1 The algorithm can be generalized to higher degrees by simply adding more nested loops, using higher order mappings, modifying the output dimensionality, and adjusting the counting of nonzero polynomial features in line 9.Calculating k-degree polynomial features via our method for a vector of dimensionality D and density d requires ( dD k ) (with repetition) products. The complexity of the algorithm, for fixed k dD, is therefore O (( dD + k − 1 k )) = O ( (dD + k − 1)! k!(dD − 1)! ) (8) = O ( (dD + k − 1)(dD + k − 2) . . . (dD) k! ) (9) = O ((dD + k − 1)(dD + k − 2) . . . (dD)) for k dD (10) = O ( dkDk ) (11)To demonstrate how our algorithm scales with the density of a matrix, we compare it to the traditional polynomial expansion algorithm in the popular machine library scikit-learn Pedregosa et al. (2011) in the task of generating second degree polynomial expansions. Matrices of size 100× 5000 were randomly generated with densities of 0.2, 0.4, 0.6, 0.8, and 1.0. Thirty matrices of each density were randomly generated, and the mean times (gray) of each algorithm were plotted. The red or blue width around the mean marks the third standard deviation from the mean. The time to densify the input to the standard algorithm was not counted. The standard algorithm’s runtime stays constant no matter the density of the matrix. This is because it does not avoid products that result in zero, but simply multiplies all second order combinations of features. Our algorithm scales quadratically with respect to the density. If the task were third degree expansions rather than second, the plot would show cubic scaling. The fact that our algorithm is approximately 6.5 times faster than the scikit-learn algorithm on 100× 5000 matrices that are entirely dense is likely a language implementation difference. What matters is that the time of our algorithm increases quadratically with respect to the density in accordance with the big-O analysis.We have developed an algorithm for performing polynomial feature expansions on CSR matrices that scales polynomially with respect to the density of the matrix. The areas within machine learning that this work touches are not en vogue, but they are workhorses of industry, and every improvement in core representations has an impact across a broad range of applications.
The efficient coding hypothesis [1, 2] plays a fundamental role in understanding neural codes, particularly in early sensory processing. Going beyond the original idea of redundancy reduction by Horace Barlow [2], efficient coding has become a general conceptual framework for studying optimal neural coding [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]. Efficient coding theory hypothesizes that the neural code is organized in a way such that maximal information is conveyed about the stimulus variable. Notably, any formulation of efficient coding necessarily relies on a set of constraints due to real world limitations imposed on neural systems. For example, neural noise, metabolic energy budgets, tuning curve characteristics and the size of the neural population all can have impacts on the quality of the neural code. Most previous studies have only considered a small subset of these constraints. For example, the original redundancy reduction argument proposed by Barlow has focused on utilizing the dynamical ∗equal contribution †current affiliation: Center for Neural Science, New York University ‡current affiliation: Department of Statistics and Center for Theoretical Neuroscience, Columbia University 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. range of the neurons efficiently [2, 15], but did not take neural noise model and energy consumption into consideration. Some studies explicitly dealt with the metabolic costs of the system but did not consider the constraints imposed by the limited firing rates of neurons as well as their detailed tuning properties [16, 7, 17, 18]. As another prominent example, histogram equalization has been proposed as the mechanism for determining the optimal tuning curve of a single neuron with monotonic response characteristics [19]. However, this result only holds for a specific neural noise model and does not take metabolic costs into consideration either. In terms of neural population, most previous studies have focused on bell-shaped tuning curves. Optimal neural coding for neural population with monotonic tuning curves have received much less attention [20, 21]. We develop a formulation of efficient coding that explicitly deals with multiple biologically relevant constraints, including neural noise, limited range of the neural output, and metabolic consumption. With this formulation, we can study neural codes based on monotonic response characteristics that have been frequently observed in biological neural systems. We are able to derive analytical solutions for a wide range of conditions in the small noise limit. We present results for neural populations of different sizes, including the cases of a single neuron, pairs of neurons, as well as a brief treatment for larger neural populations. The results are in general agreements with observed coding schemes for monotonic tuning curves. The results also provide various quantitative predictions which are readily testable with targeted physiology experiments.We start with the simple case where a scalar stimulus s with prior p(s) is encoded by a single neuron. To model the neural response for a stimulus s, we first denote the mean output level as a deterministic function h(s). Here h(s) could denote the mean firing rate in the context of rate coding or just the mean membrane potential. In either case, the actual response r is noisy and can be modeled by a probabilistic model P (r|h(s)). Throughout the paper, we limit the neural codes to be monotonic functions h(s). The mutual information between the input stimulus r and the neural response is denoted as MI(s, r). We formulate the efficient coding problem as the maximization of the mutual information between the stimulus and the response, e.g., MI(s, r) [3]. To complete the formulation of this problem, it is crucial to choose a set of constraints which characterizes the limited resource available to the neural system. One important constraint is the finite range of the neural output [19]. Another constraint is on the mean metabolic cost [16, 7, 17, 18], which limits the mean activity level of neural output, averaged over the stimulus prior. Under these constraints, the efficient coding problem can mathematically be formulated as following: maximize MI(s, r) subject to 0 ≤ h(s) ≤ rmax, h′(s) ≥ 0 (range constraint) Es[K(h(s))] ≤ Ktotal (metabolic constraint) We seek the optimal response function h(s) under various choices of the neural noise model P (r|h(s)) and certain metabolic cost function K(h(s)), as discussed below. Neural Noise Models: Neural noise can often be well characterized by a Poisson distribution at relatively short time scale [22]. Under the Poisson noise model, the number of spikes NT over a duration of T is a Poisson random variable with mean h(s)T and variance h(s)T . In the long T limit, the mean response r = NT /T approximately follows a Gaussian distribution r ∼ N (h(s), h(s)/T ) (1) Non-Poisson noise have also been observed physiologically. In these cases, the variance of response NT can be greater or smaller than the mean firing rate [22, 23, 24, 25]. We thus consider a more generic family of noise models parametrized by α r ∼ N (h(s), h(s)α/T ) (2) This generalized family of noise model naturally includes the additive Gaussian noise case (when α = 0), which is useful for describing the stochasticity of the membrane potential of a neuron. Metabolic Cost: We model the metabolic cost K is a power-law function of the neural output K(h(s)) = h(s)β (3) where β > 0 is a parameter to model how does the energy cost scale up as the neural output is increasing. For a single neuron we will demonstrate with the general energy cost function but when we generalize to the case of multiple neurons, we will assume β = 1 for simplicity. Note that it does not require extra effort to solve the problem if the cost function takes the general form of K̃(h(s)) = K0+K1h(s) β , as reported in [26]. This is because of the linear nature of the expectation term in the metabolic constraint. 2.2 Derivation of the Optimal h(s) This efficient coding problem can be greatly simplified due to the fact that it is invariant under any re-parameterization of the stimulus variable s. We take this advantage by mapping s to another uniform random variable u ∈ [0, 1] via the cumulative distribution function u = F (s) [27]. If we choose g(u) = g(F (s)) = h(s), it suffices to solve the following new problem which optimizes g(u) for a re-parameterized input u with uniform prior maximize MI(u, r) subject to 0 ≤ g(u) ≤ rmax, g′(u) ≥ 0 Eu[K(g(u))] ≤ Ktotal Once the optimal form of g∗(u) is obtained, the optimal h∗(s) is naturally given by g∗(F (s)). To solve this simplified problem, first we express the objective function in terms of g(u). In the small noise limit (large integration time T ), the Fisher information IF (u) of the neuron with noise model in Eq. (2) is calculated and the mutual information can be approximated as (see [28, 14]) IF (u) = T g′(u)2 g(u)α +O(1) (4) MI(u, r) = H(U) + 1 2 ∫ p(u) log IF (u) du = 1 2 ∫ 1 0 log g′(u)2 g(u)α du+ 1 2 log T +O(1/T ) (5) where H(U) = 0 is the entropy and p(u) = 1{0≤u≤1} is the density of the uniform distribution. Furthermore, each constraints can be rewritten as integrals of g′(u) and g(u) respectively: g(1)− g(0) = ∫ 1 0 g′(u) du ≤ rmax (6) Eu[K(g(u))] = ∫ 1 0 g(u)β du ≤ Ktotal (7) This form of the problem (Eq. 5-7) can be analytically solved by using the Lagrangian multiplier method and the optimal response function must take the form g(u) = rmax · [ 1 a γ−1q (uγq(a)) ]1/β , h(s) = g(F (s)) (8) where q def= (1− α/2)/β, γq(x) def = ∫ x 0 zq−1 exp(−z) dz. (9) The function γq(x) is called the incomplete gamma function and γ−1q is its inverse. Due to space limitation we only present a sketch derivation. Readers who are interested in the detailed proof are referred to the supplementary materials. Let us now turn to some intuitive conclusions behind this solution (also see Fig.1, in which we have assumed rmax = 1 for simplicity). From Eq. (8), it is clear that the optimal solution g(u) depend on the constant a which should be determined by equalizing the metabolic constraint (see the horizontal dash lines in Fig.1a). Furthermore, the optimal solution h(s) depends on the specific input distribution p(s). Depending on the relative magnitude of rmax and Ktotal: 2.3 Properties of the Optimal h(s) We have predicted the optimal response function for arbitrary values of α (which corresponds to the noise model) and β (which quantifies the metabolic cost model). Here we specifically focus on a few situations with most biological relevance. We begin with the simple additive Gaussian noise model, i.e. α = 0. This model could provide a good characterization of the response mapping from the input stimulus to the membrane potential of a neuron [19]. With more than sufficient metabolic supply, the optimal solution falls back to the histogram equalization principle where each response magnitude is utilized to the same extent (red curve in Fig. 1b and Fig.2a). With less metabolic budget, the optimal tuning curve bends downwards to satisfy this constraint and large responses will be penalized, resulting in more density at smaller response magnitude (purple curve in Fig.2a). In the other extreme, when the available metabolic budget Ktotal is diminishing, the response magnitude converges to the max-entropy distribution under the metabolic constraint E[g(u)β ] = const (blue curve in Fig.2a). Next we discuss the case of Poisson spiking neurons. In the extreme case when the range constraint dominates, the model predicts a square tuning curve for uniform input (red curve in Fig.1f), which is consistent with previous studies [29, 30]. We also found that Poisson noise model leads to heavier penalization on large response magnitude compared to Gaussian noise, suggesting an interaction between noise and metabolic cost in shaping the optimal neural response distribution. In the other extreme whenKtotal goes to 0, the response distribution converges to a gamma distribution, with heavy tail (see Fig.2). Our analytical result gives a simple yet quantitative explanation of the emergence of sparse coding [7] from an energy-efficiency perspective.We next study the optimal coding in the case of two neurons with monotonic response functions. We denote the neural responses as r = (r1, r2). Therefore the efficient coding problem becomes: maximize MI(s, r) subject to 0 ≤ hi(s) ≤ rmax, i = 1, 2. (range constraint) Es [K(h1(s)) +K(h2(s))] ≤ 2Ktotal (metabolic constraint) Assuming the neural noise is independent across neurons, the system of two neurons has total Fisher information just as the linear sum of Fisher information contributed from each neuron IF (s) = I1(s) + I2(s).Previous studies on neural coding with monotonic response functions have typically assumed that each hi(s) has sigmoidal shape. It is important to emphasize that we do not make any a priori assumptions on the detailed shape of the tuning curve other than being monotonic and smooth. We define each neuron’s active region Ai = A+i ∪A − i , where A + i = {s|h′i(s) > 0}, A − i = {s| − h′i(s) > 0}. Due to the monotonicity of tuning curve, either A+i or A − i has to be empty. We find the following results (proof in the supplementary materials) 1. Different neurons should have non-overlapping active regions. 2. If the metabolic constraint is binding, ON-OFF coding is better than ON-ON coding or OFF- OFF coding. Otherwise all three coding schemes can achieve the same mutual information. 3. For ON-OFF coding, it is better to have ON regions on the right side. 4. For ON-ON coding (or OFF-OFF), each neuron should have roughly the same tuning curve hi(s) ≈ hj(s) while still have disjoint active regions. Note that a conceptually similar coding scheme has been previously discussed by [29]. Within the ON-pool or OFF-pool, the optimal tuning curve is same as the optimal solution from the single neuron case. In Fig.3a-d, we illustrate how these conclusions can be used to determine the optimal pair of neurons, assuming additive Gaussian noise α = 0 and linear metabolic cost β = 1 (for other α and β the process is similar). Our analytical results allow us to predict the precise shape of the optimal response functions, which goes beyond previous work on ON-OFF coding schemes [13, 31].We aim to compare the coding performance of ON-OFF and ON-ON codes. In Fig.3e we show how the mutual information depends on the available metabolic budget. For both ON-FF and ON-ON scheme, the mutual information is monotonically increasing as a function of energy available. We compare these two curves in two different ways. First, we notice that both mutual information curve saturate the limit at KON-ON = 0.5rmax and KON-OFF = 0.25rmax respectively (see the red tuning curves in Fig.3a-d). Note that this specific saturation limit is only valid for α = 0 and β = 1. For any other mutual information, we find out that the optimal ON-ON pair (or OFF-OFF pair) always cost twice energy compared to the optimal ON-OFF pair. Second, one can compare the ON-ON and ON-OFF scheme by fixing the energy available. The optimal mutual information achieved by ON-ON neurons is always smaller than that achieved by ON-OFF neurons and the difference is plotted in Fig.3. When the available energy is extremely limited Ktotal rmax, such difference saturates at −1 in the logarithm space of MI (base 2). This shows that, in the worst scenario, the ON-ON code is only half as efficient as the ON-OFF code from mutual information perspective. In other words, it would take twice the amount of time T for the ON-ON code to convey same amount of mutual information as the ON-OFF code under same noise level. These analyses quantitatively characterize the advantage of ON-OFF over ON-ON and show how it varies when the relative importance of the metabolic constraint changes. The encoding efficiency of ON-OFF ranges from double (with very limited metabolic budget) to equal amount of the ON-ON efficiency (with unlimited metabolic budget). This wide range includes the previous conclusion reported by Gjorgjieva et.al., where a mild advantage (∼ 15%) of ON-OFF scheme is found under short integration time limit [31]. It is well known that the split of ON and OFF pathways exists in the retina of many species [32, 33]. The substantial increase of efficiency under strong metabolic constraint we discovered supports the argument that metabolic constraint may be one of the main reasons for such pathway splitting in evolution. In a recent study by Karklin and Simoncelli [13], it is observed numerically that ON-OFF coding scheme can naturally emerge when a linear-nonlinear population of neurons are trained to maximize mutual information with image input and under metabolic constraint. It is tempting to speculate a generic connection of these numerical observations to our theoretical results, although our model is much more simplified in the sense that we do not directly model the higher dimensional stimulus (natural image) but just a one dimensional projection (local contrast). Intriguingly, we find that if the inputs follow certain heavy tail distribution ( Fig.3b), the optimal response functions are two rectified non-linear functions which split the encoding range. Such rectified non-linearity is consistent with both the non-linearity observed physiologically[34] and the numerical results in [13] .In this paper we presented a theoretical framework for studying optimal neural codes under biologically relevant constraints. Compared to previous works, we emphasize the importance of two types of constraint – the noise characteristics of the neural responses and the metabolic cost. Throughout the paper, we have focused on neural codes with smooth monotonic response functions. We demonstrated that, maybe surprisingly, analytical solutions exist for a wide family of noise characteristics and metabolic cost functions. These analytical results rely on the techniques of approximating mutual information using Fisher information. There are cases when such approximation would bread down, in particular for short integration time or non-Gaussian noise. For a more detailed discussion on the validity of Fisher approximation, see [29, 14, 35]. We have focused on the cases of a single neuron and a pair of neurons. However, the framework can be generalized to the case of larger population of neurons. For the case of N = 2k (k is large) neurons, we again find the corresponding optimization problem could be solved analytically by exploiting the Fisher information approximation of mutual information [28, 14]. Interestingly, we found the optimal codes should be divided into two pools of neurons of equal size k. One pool of neuron with monotonic increasing response function (ON-pool), and the other with monotonic decreasing response function (OFF-pool). For neurons within the same pool, the optimal response functions appear to be identical on the macro-scale but are quite different when zoomed in. In fast, the optimal code must have disjoint active regions for each neuron. This is similar to what has been illustrated in the inset panel of Fig.3c, where two seemingly identical tuning curves for ON-neurons are compared. We can also quantify the increase of the mutual information by using optimal coding schemes versus using all ON neurons (or all OFF). Interestingly, some of the key results presented in the Fig 3e for the a pair of neurons generalize to 2K case. When N = 2k+1, the optimal solution is similar to N = 2k for a large pool of neurons. However, when k is small, the difference caused by asymmetry between ON/OFF pools can substantially change the configuration of the optimal code. Due to the limited scope of the paper, we have ignored several important aspects when formulating the efficient coding problem. First, we have not modeled the spontaneous activity (baseline firing rate) of neurons. Second, we have not considered the noise correlations between the responses of neurons. Third, we have ignored the noise in the input to the neurons. We think that the first two factors are unlikely to change our main results. However, incorporating the input noise may significantly change the results. In particular, for the cases of multiple neurons, our current results predict that there is no overlap between the active regions of the response functions for ON and OFF neurons. However, it is possible that this prediction does not hold in the presence of the input noise. In that case, it might be beneficial to have some redundancy by making the response functions partially overlap. Including these factors into the framework should facilitate a detailed and quantitative comparison to physiologically measured data in the future. As for the objective function, we have only considered the case of maximizing mutual information; it is interesting to see whether the results can be generalized to other objective functions such as, e.g., minimizing decoding error[36, 37]. Also, our theory is based on a one dimensional input. To fully explain the ON-OFF split in visual pathway, it seems necessary to consider a more complete model with the images as the input. To this end, our current model lacks the spatial component, and it doesn’t explain the difference between the number of ON and OFF neurons in retina [38]. Nonetheless, the insight from these analytical results based on the simple model may prove to be useful for a more complete understanding of the functional organization of the early visual pathway. Last but not least, we have assumed a stationary input distribution. However, in the natural environment the input distribution often fluctuate at different time scales, it remains to be investigated how to incorporate these dynamical aspects into a theory of efficient coding.
Estimating the accuracy of classifiers is central to machine learning and many other fields. Accuracy is defined as the probability of a system’s output agreeing with the true underlying output, and thus is a measure of the system’s performance. Most existing approaches to estimating accuracy are supervised, meaning that a set of labeled examples is required for the estimation. Being able to estimate the accuracies of classifiers using only unlabeled data is important for many applications, including: (i) any autonomous learning system that operates under no supervision, as well as (ii) crowdsourcing applications, where multiple workers provide answers to questions, for which the correct answer is unknown. Furthermore, tasks which involve making several predictions which are tied together by logical constraints are abundant in machine learning. As an example, we may have two classifiers in the Never Ending Language Learning (NELL) project [Mitchell et al., 2015] which predict whether noun phrases represent animals or cities, respectively, and we know that something cannot be both an animal and a city (i.e., the two categories are mutually exclusive). In such cases, it is not hard to observe that if the predictions of the system violate at least one of the constraints, then at least one of the system’s components must be wrong. This paper extends this intuition and presents an unsupervised approach (i.e., only unlabeled data are needed) for estimating accuracies that is able to use information provided by such logical constraints. Furthermore, the proposed approach is also able to use any available labeled data, thus also being applicable to semi-supervised settings. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. We consider a “multiple approximations” problem setting in which we have several different approximations, f̂d1 , . . . , f̂ d Nd , to a set of target boolean classification functions, f d : X 7→ {0, 1} for d = 1, . . . , D, and we wish to know the true accuracies of each of these different approximations, using only unlabeled data, as well as the response of the true underlying functions, fd. Each value of d characterizes a different domain (or problem setting) and each domain can be interpreted as a class or category of objects. Similarly, the function approximations can be interpreted as classifying inputs as belonging or not to these categories. We consider the case where we may have a set of logical constraints defined over the domains. Note that, in contrast with related work, we allow the function approximations to provide soft responses in the interval [0, 1] (as opposed to only allowing binary responses — i.e., they can now return the probability for the response being 1), thus allowing modeling of their “certainty”. As an example of this setting, to which we will often refer throughout this paper, let us consider a part of NELL, where the input space of our functions, X , is the space of all possible noun phrases (NPs). Each target function, fd, returns a boolean value indicating whether the input NP belongs to a category, such as “city” or “animal”, and these categories correspond to our domains. There also exist logical constraints between these categories that may be hard (i.e., strongly enforced) or soft (i.e., enforced in a probabilistic manner). For example, “city” and “animal” may be mutually exclusive (i.e., if an object belongs to “city”, then it is unlikely that it also belongs to “animal”). In this case, the function approximations correspond to different classifiers (potentially using a different set of features / different views of the input data), which may return a probability for a NP belonging to a class, instead of a binary value. Our goal is to estimate the accuracies of these classifiers using only unlabeled data. In order to quantify accuracy, we define the error rate of classifier j in domain d as edj , PD[f̂dj (X) 6= fd(X)], for the binary case, for j = 1, . . . , Nd, where D is the true underlying distribution of the input data. Note that accuracy is equal to one minus error rate. This definition may be relaxed for the case where f̂dj (X) ∈ [0, 1] representing a probability: edj , f̂ d j (X)PD[fd(X) 6=1] + (1− f̂dj (X))PD[fd(X) 6=0], which resembles an expected probability of error. Even though our work is motivated by the use of logical constraints defined over the domains, we also consider the setting where there are no such constraints.The literature covers many projects related to estimating accuracy from unlabeled data. The setting we are considering was previously explored by Collins and Singer [1999], Dasgupta et al. [2001], Bengio and Chapados [2003], Madani et al. [2004], Schuurmans et al. [2006], Balcan et al. [2013], and Parisi et al. [2014], among others. Most of their approaches made some strong assumptions, such as assuming independence given the outputs, or assuming knowledge of the true distribution of the outputs. None of the previous approaches incorporated knowledge in the form of logical constraints. Collins and Huynh [2014] review many methods that were proposed for estimating the accuracy of medical tests in the absence of a gold standard. This is effectively the same problem that we are considering, applied to the domains of medicine and biostatistics. They present a method for estimating the accuracy of tests, where these tests are applied in multiple different populations (i.e., different input data), while assuming that the accuracies of the tests are the same across the populations, and that the test results are independent conditional on the true “output”. These are similar assumptions to the ones made by several of the other papers already mentioned, but the idea of applying the tests to multiple populations is new and interesting. Platanios et al. [2014] proposed a method relaxing some of these assumptions. They formulated the problem of estimating the error rates of several approximations to a function as an optimization problem that uses agreement rates of these approximations over unlabeled data. Dawid and Skene [1979] were the first to formulate the problem in terms of a graphical model and Moreno et al. [2015] proposed a nonparametric extension to that model applied to crowdsourcing. Tian and Zhu [2015] proposed an interesting max-margin majority voting scheme for combining classifier outputs, also applied to crowdsourcing. However, all of these approaches were outperformed by the models of Platanios et al. [2016], which are most similar to the work of Dawid and Skene [1979] and Moreno et al. [2015]. To the best of our knowledge, our work is the first to use logic for estimating accuracy from unlabeled data and, as shown in our experiments, outperforms all competing methods. Logical constraints provide additional information to the estimation method and this partially explains the performance boost.Our method consists of: (i) defining a set of logic rules for modeling the logical constraints between the fd and the f̂dj , in terms of the error rates e d j and the known logical constraints, and (ii) performing probabilistic inference using these rules as priors, in order to obtain the most likely values of the edj and the f d, which are not observed. The intuition behind the method is that if the constraints are violated for the function approximation outputs, then at least one of these functions has to be making an error. For example, in the NELL case, if two function approximations respond that a NP belongs to the “city” and the “animal” categories, respectively, then at least one of them has to be making an error. We define the form of the logic rules in section 3.2 and then describe how to perform probabilistic inference over them in section 3.3. An overview of our system is shown in figure 1. In the next section we introduce the notion of probabilistic logic, which fuses classical logic with probabilistic reasoning and that forms the backbone of our method.In classical logic, we have a set of predicates (e.g., mammal(x) indicating whether x is a mammal, where x is a variable) and a set of rules defined in terms of these predicates (e.g., mammal(x) → animal(x), where “→” can be interpreted as “implies”). We refer to predicates and rules defined for a particular instantiation of their variables as ground predicates and ground rules, respectively (e.g., mammal(whale) and mammal(whale) → animal(whale)). These ground predicates and rules take boolean values (i.e., are either true or false — for rules, the value is true if the rule holds). Our goal is to infer the most likely values for a set of unobserved ground predicates, given a set of observed ground predicate values and logic rules. In probabilistic logic, we are instead interested in inferring the probabilities of these ground predicates and rules being true, given a set of observed ground predicates and rules. Furthermore, the truth values of ground predicates and rules may be continuous and lie in the interval [0, 1], instead of being boolean, representing the probability that the corresponding ground predicate or rule is true. In this case, boolean logic operators, such as AND (∧), OR (∨), NOT (¬), and IMPLIES (→), need to be redefined. For the next section, we will assume their classical logical interpretation.As described earlier, our goal is to estimate the true accuracies of each of the function approximations, f̂d1 , . . . , f̂ d Nd for d = 1, . . . , D, using only unlabeled data, as well as the response of the true underlying functions, fd. We now define the logic rules that we perform inference over in order to achieve that goal. The rules are defined in terms of the following predicates, for d = 1, . . . , D: • Function Approximation Outputs: f̂dj (X), defined over all approximations j = 1, . . . , Nd, and inputs X ∈ X , for which the corresponding function approximation has provided a response. Note that the values of these ground predicates lie in [0, 1] due to their probabilistic nature (i.e., they do not have to be binary, as in related work), and some of them are observed. • Target Function Outputs: fd(X), defined over all inputs X ∈ X . Note that, in the purely unsupervised setting, none of these ground predicate values are observed, in contrast with the semi-supervised setting. • Function Approximation Error Rates: edj , defined over all approximations j = 1, . . . , Nd. Note that none of these ground predicate values are observed. The primary goal of this paper is to infer their values. The goal of the logic rules we define is two-fold: (i) to combine the function approximation outputs in a single output value, and (ii) to account for the logical constraints between the domains. We aim to achieve both goals while accounting for the error rates of the function approximations. We first define a set of rules that relate the function approximation outputs with the true underlying function output. We call this set of rules the ensemble rules and we describe them in the following section. We then discuss how to account for the logical constraints between the domains.This first set of rules specifies a relation between the target function outputs, fd(X), and the function approximation outputs, f̂dj (X), independent of the logical constraints: f̂dj (X) ∧ ¬edj→fd(X), ¬f̂dj (X) ∧ ¬edj→¬fd(X), (1) f̂dj (X) ∧ edj→¬fd(X), and ¬f̂dj (X) ∧ edj→fd(X), (2) for d = 1, . . . , D, j = 1, . . . , Nd, and X ∈ X . In words: (i) the first set of rules state that if a function approximation is not making an error, its output should match the output of the target function, and (ii) the second set of rules state that if a function approximation is making an error, its output should not match the output of the target function. An interesting point to make is that the ensemble rules effectively constitute a weighted majority vote for combining the function approximation outputs, where the weights are determined by the error rates of the approximations. These error rates are implicitly computed based on agreement between the function approximations. This is related to the work of Platanios et al. [2014]. There, the authors try to answer the question of whether consistency in the outputs of the approximations implies correctness. They directly use the agreement rates of the approximations in order to estimate their error rates. Thus, there exists an interesting connection in our work in that we also implicitly use agreement rates to estimate error rates, and our results, even though improving upon theirs significantly, reinforce their claim. Identifiability. Let us consider flipping the values of all error rates (i.e., setting them to one minus their value) and the target function responses. Then, the ensemble logic rules would evaluate to the same value as before (e.g., satisfied or unsatisfied). Therefore, the error rates and the target function values are not identifiable when there are no logical constraints. As we will see in the next section, the constraints may sometimes help resolve this issue as, often, the corresponding logic rules do not exhibit that kind of symmetry. However, for cases where that symmetry exists, we can resolve it by assuming that most of the function approximations have error rates better than chance (i.e., < 0.5). This can be done by considering the two rules: (i) f̂dj (X) → fd(X), and ¬f̂dj (X) → ¬fd(X), for d = 1, . . . , D, j = 1, . . . , Nd, and X ∈ X . Note that all that these rules imply is that f̂dj (X) = f d(X) (i.e., they represent the prior belief that function approximations are correct). As will be discussed in section 3.3, in probabilistic frameworks where rules are weighted with a real value in [0, 1], these rules will be given a weight that represents their significance or strength. In such a framework, we can consider using a smaller weight for these prior belief rules, compared to the remainder of the rules, which would simply correspond to a regularization weight. This weight can be a tunable or even learnable parameter.The space of possible logical constraints is huge; we do not deal with every possible constraint in this paper. Instead, we focus our attention on two types of constraints that are abundant in structured prediction problems in machine learning, and which are motivated by the use of our method in the context of NELL: • Mutual Exclusion: If domains d1 and d2 are mutually exclusive, then fd1 = 1 implies that fd2 = 0. For example, in the NELL setting, if a NP belongs to the “city” category, then it cannot also belong to the “animal” category. • Subsumption: If d1 subsumes d2, then if fd2 = 1, we must have that fd1 = 1. For example, in the NELL setting, if a NP belongs to the “cat” category, then it must also belong to the “animal” category. This set of constraints is sufficient to model most ontology constraints between categories in NELL, as well as a big subset of the constraints more generally used in practice. Mutual Exclusion Rule. We first define the predicate ME(d1, d2), indicating that domains d1 and d2 are mutually exclusive1. This predicate has value 1 if domains d1 and d2 are mutually exclusive, and value 0 otherwise, and its truth value is observed for all values of d1 and d2. Furthermore, note that it is symmetric, meaning that if ME(d1, d2) is true, then ME(d2, d1) is also true. We define the mutual exclusion logic rule as: ME(d1, d2) ∧ f̂d1j (X) ∧ fd2(X)→ ed1j , (3) for d1 6= d2 = 1, . . . , D, j = 1, . . . , Nd1 , and X ∈ X . In words, this rule says that if fd2(X) = 1 and domains d1 and d2 are mutually exclusive, then f̂d1j (X) must be equal to 0, as it is an approximation to fd1(X) and ideally we want that f̂d1j (X) = f d1(X). If that is not the case, then f̂d1j must be making an error. Subsumption Rule. We first define the predicate SUB(d1, d2), indicating that domain d1 subsumes domain d2. This predicate has value 1 if domain d1 subsumes domain d2, and 0 otherwise, and its truth value is always observed. Note that, unlike mutual exclusion, this predicate is not symmetric. We define the subsumption logic rule as: SUB(d1, d2) ∧ ¬f̂d1j (X) ∧ fd2(X)→ ed1j , (4) for d1, d2 = 1, . . . , D, j = 1, . . . , Nd1 , and X ∈ X . In words, this rule says that if fd2(X) = 1 and d1 subsumes d2, then f̂d1j (X) must be equal to 1, as it is an approximation to f d1(X) and ideally we want that f̂d1j (X) = f d1(X). If that is not the case, then f̂d1j must be making an error. Having defined all of the logic rules that comprise our model, we now describe how to perform inference under such a probabilistic logic model, in the next section. Inference in this case comprises determining the most likely truth values of the unobserved ground predicates, given the observed predicates and the set of rules that comprise our model. 1A set of mutually-exclusive domains can be reduced to pairwise ME constraints for all pairs in that set.In section 3.1 we introduced the notion of probabilistic logic and we defined our model in terms of probabilistic predicates and rules. In this section we discuss in more detail the implications of using probabilistic logic, and the way in which we perform inference in our model. There exist various probabilistic logic frameworks, each making different assumptions. In what is arguably the most popular such framework, Markov Logic Networks (MLNs) [Richardson and Domingos, 2006], inference is performed over a constructed Markov Random Field (MRF) based on the model logic rules. Each potential function in the MRF corresponds to a ground rule and takes an arbitrary positive value when the ground rule is satisfied and the value 0 otherwise (the positive values are often called rule weights and can be either fixed or learned). Each variable is boolean-valued and corresponds to a ground predicate. MLNs are thus a direct probabilistic extension to boolean logic. It turns out that due to the discrete nature of the variables in MLNs, inference is NP-hard and can thus be very inefficient. Part of our goal in this paper is for our method to be applicable at a very large scale (e.g., for systems like NELL). We thus resorted to Probabilistic Soft Logic (PSL) [Bröcheler et al., 2010], which can be thought of as a convex relaxation of MLNs. Note that the model proposed in the previous section, which is also the primary contribution of this paper, can be used with various probabilistic logic frameworks. Our choice, which is described in this section, was motivated by scalability. One could just as easily perform inference for our model using MLNs, or any other such framework.In PSL, models, which are composed of a set of logic rules, are represented using hinge-loss Markov random fields (HL-MRFs) [Bach et al., 2013]. In this case, inference amounts to solving a convex optimization problem. Variables of the HL-MRF correspond to soft truth values of ground predicates. Specifically, a HL-MRF, f , is a probability density over m random variables, Y = {Y1, . . . , Ym} with domain D = [0, 1]m, corresponding to the unobserved ground predicate values. Let X = {X1, . . . , Xn} be an additional set of variables with known values in the domain [0, 1]n, corresponding to observed ground predicate values. Let φ = {φ1, . . . , φk} be a finite set of k continuous potential functions of the form φj(X,Y) = (max {`j(X,Y), 0})pj , where `j is a linear function of X and Y, and pj ∈ {1, 2}. We will soon see how these functions relate to the ground rules of the model. Given the above, for a set of non-negative free parameters λ = {λ1, . . . , λk} (i.e., the equivalent of MLN rule weights), the HL-MRF density is defined as: f(Y) = 1 Z exp− k∑ j=1 λjφj(X,Y), (5) where Z is a normalizing constant so that f is a proper probability density function. Our goal is to infer the most probable explanation (MPE), which consists of the values of Y that maximize the likelihood of our data2. This is equivalent to solving the following convex problem: min Y∈[0,1]m k∑ j=1 λjφj(X,Y). (6) Each variable Xi or Yi corresponds to a soft truth value (i.e., Yi ∈ [0, 1]) of a ground predicate. Each function `j corresponds to a measure of the distance to satisfiability of a logic rule. The set of rules used is what characterizes a particular PSL model. The rules represent prior knowledge we might have about the problem we are trying to solve. For our model, these rules were defined in section 3.2. As mentioned above, variables are allowed to take values in the interval [0, 1]. We thus need to define what we mean by the truth value of a rule and its distance to satisfiability. For the logical operators AND (∧), OR (∨), NOT (¬), and IMPLIES (→), we use the definitions from Łukasiewicz Logic [Klir and Yuan, 1995]: P ∧Q , max {P +Q− 1, 0}, P ∨Q , min {P +Q, 1}, ¬P , 1− P , and P → Q , min{1− P +Q, 1}. Note that these operators are a simple continuous relaxation of the corresponding boolean operators, in that for boolean-valued variables, with 0 corresponding to FALSE and 1 to TRUE, they are equivalent. By writing all logic rules in the form B1 ∧B2 ∧ · · · ∧Bs → H1 ∨H2 ∨ · · · ∨Ht, it is easy to observe that the distance to satisfiability 2As opposed to performing marginal inference which aims to infer the marginal distribution of these values. (i.e., 1 minus its truth value) of a rule evaluates to max {0,∑si=1Bi −∑tj=1Ht + 1− s}. Note that any set of rules of first-order predicate logic can be represented in this form [Bröcheler et al., 2010], and that minimizing this quantity amounts to making the rule “more satisfied”. In order to complete our system description we need to describe: (i) how to obtain a set of ground rules and predicates from a set of logic rules of the form presented in section 3.2 and a set of observed ground predicates, and define the objective function of equation 6, and (ii) how to solve the optimization problem of that equation to obtain the most likely truth values for the unobserved ground predicates. These two steps are described in the following two sections.Grounding is the process of computing all possible groundings of each logic rule to construct the inference problem variables and the objective function. As already described in section 3.3.1, the variables X and Y correspond to ground predicates and the functions `j correspond to ground rules. The easiest way to ground a set of logic rules would be to go through each one and create a ground rule instance of it, for each possible value of its arguments. However, if a rule depends on n variables and each variable can takem possible values, thenmn ground rules would be generated. For example, the mutual exclusion rule of equation 3 depends on d1, d2, j, and X , meaning that D2×Nd1×|X| ground rule instances would be generated, where |X| denotes the number of values that X can take. The same applies to predicates; f̂d1j (X) would result in D×Nd1×|X| ground instances, which would become variables in our optimization problem. This approach would thus result in a huge optimization problem rendering it impractical when dealing with large scale problems such as NELL. The key to scaling up the grounding procedure is to notice that many of the possible ground rules are always satisfied (i.e., have distance to satisfiability equal to 0), irrespective of the values of the unobserved ground predicates that they depend upon. These ground rules would therefore not influence the optimization problem solution and can be safely ignored. Since in our model we are only dealing with a small set of predefined logic rule forms, we devised a heuristic grounding procedure that only generates those ground rules and predicates that may influence the optimization. Our grounding algorithm is shown in the supplementary material and is based on the idea that a ground rule is only useful if the function approximation predicate that appears in its body is observed. It turns out that this approach is orders of magnitude faster than existing state-of-the-art solutions such as the grounding solution used by Niu et al. [2011].For large problems, the objective function of equation 6 will be a sum of potentially millions of terms, each one of which only involving a small set of variables. In PSL, the method used to solve this optimization problem is based on the consensus Alternating Directions Method of Multipliers (ADMM). The approach consists of handling each term in that sum as a separate optimization problem using copies of the corresponding variables, while adding the constraint that all copies of each variable must be equal. This allows for solving the subproblems completely in parallel and is thus scalable. The algorithm is summarized in the supplementary material. More details on this algorithm and on its convergence properties can be found in the latest PSL paper [Bach et al., 2015]. We propose a stochastic variation of this consensus ADMM method that is even more scalable. During each iteration, instead of solving all subproblems and aggregating their solutions in the consensus variables, we sample K << k subproblems to solve. The probability of sampling each subproblem is proportional to the distance of its variable copies from the respective consensus variables. The intuition and motivation behind this approach is that at the solution of the optimization problem, all variable copies should be in agreement with the consensus variables. Therefore, prioritizing subproblems whose variables are in greater disagreement with the consensus variables might facilitate faster convergence. Indeed, this modification to the inference algorithm allowed us to apply our method to the NELL data set and obtain results within minutes instead of hours.Our implementation as well as the experiment data sets are available at https://github.com/ eaplatanios/makina. Data Sets. First, we considered the following two data sets with logical constraints: • NELL-7: Classify noun phrases (NPs) as belonging to a category or not (categories correspond to domains in this case). The categories considered for this data set are Bird, Fish, Mammal, City, Country, Lake, and River. The only constraint considered is that all these categories are mutually exclusive. • NELL-11: Perform the same task, but with the categories and constraints illustrated in figure 2. For both of these data sets, we have a total of 553,940 NPs and 6 classifiers, which act as our function approximations and are described in [Mitchell et al., 2015]. Not all of the classifiers provide a response every input NP. In order to show the applicability of our method in cases where there are no logical constraints between the domains, we also replicated the experiments of Platanios et al. [2014]: • uNELL: Same task as NELL-7, but without considering the constraints and using 15 categories, 4 classifiers, and about 20,000 NPs per category. • uBRAIN: Classify which of two 40 second long story passages corresponds to an unlabeled 40 second time series of Functional Magnetic Resonance Imaging (fMRI) neural activity. 11 classifiers were used and the domain in this case is defined by 11 different locations in the brain, for each of which we have 924 examples. Additional details can be found in [Wehbe et al., 2014]. Methods. Some of the methods we compare against do not explicitly estimate error rates. Rather, they combine the classifier outputs to produce a single label. For these methods, we produce an estimate of the error rate using these labels and compare against this estimate. 1. Majority Vote (MV): This is the most intuitive method and it consists of taking the most common output among the provided function approximation responses, as the combined output. 2. GIBBS-SVM/GD-SVM: Methods of Tian and Zhu [2015]. 3. DS: Method of Dawid and Skene [1979]. 4. Agreement Rates (AR): This is the method of Platanios et al. [2014]. It estimates error rates but does not infer the combined label. To that end, we use a weighted majority vote, where the classifiers’ predictions are weighted according to their error rates in order to produce a single output label. We also compare against a method denoted by AR-2 in our experiments, which is the same method, except only pairwise function approximation agreements are considered. 5. BEE/CBEE/HCBEE: Methods of Platanios et al. [2016]. In the results, LEE stands for Logic Error Estimation and refers to the proposed method of this paper. Evaluation. We compute the sample error rate estimates using the true target function labels (which are always provided), and we then compute three metrics for each domain and average over domains: • Error Rank MAD: We rank the function approximations by our estimates and by the sample estimates to produce two vectors with the ranks. We then compute the mean absolute deviation (MAD) between the two vectors, where by MAD we mean the `1 norm of the vectors’ difference. • Error MAD: MAD between the vector of our estimates and the vector of the sample estimates, where each vector is indexed by the function approximation index. • Target AUC: Area under the precision-recall curve for the inferred target function values, relative to the true function values that are observed. Results. First, note that the largest execution time of our method among all data sets was about 10 minutes, using a 2013 15-inch MacBook Pro. The second best performing method, HCBEE, required about 100 minutes. This highlights the scalability of our approach. Results are shown in table 1. 1. NELL-7 and NELL-11 Data Sets: In this case we have logical constraints and thus, this set of results is most relevant to the central research claims in this paper (our method was motivated by the use of such logical constraints). It is clear that our method outperforms all existing methods, including the state-of-the-art, by a significant margin. Both the MADs of the error rate estimation, and the AUCs of the target function response estimation, are significantly better. 2. uNELL and uBRAIN Data Sets: In this case there exist no logical constraints between the domains. Our method still almost always outperforms the competing methods and, more specifically, it always does so in terms of error rate estimation MAD. This set of results makes it clear that our method can also be used effectively in cases where there are no logical constraints.We would like to thank Abulhair Saparov and Otilia Stretcu for the useful feedback they provided in early versions of this paper. This research was performed during an internship at Microsoft Research, and was also supported in part by NSF under award IIS1250956, and in part by a Presidential Fellowship from Carnegie Mellon University.
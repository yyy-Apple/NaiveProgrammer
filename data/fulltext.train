Deep neural networks (DNNs) have achieved impeccable success to address various real world tasks (Simonyan & Zisserman, 2014a; Hinton et al., 2012; Litjens et al., 2017). However, despite impressive, and ever-improving performance in various supervised learning tasks, DNNs tend to make over-confident predictions for every input. Predictive uncertainties of DNNs can be confronted from three different factors such as model uncertainty, data uncertainty and distributional uncertainty (Malinin & Gales, 2018). Model or epistemic uncertainty captures the uncertainty in estimating the model parameters, conditioning on training data (Gal, 2016). This uncertainty can be explained away given enough training data. Data or aleatoric uncertainty is originated from the inherent complexities of the training data, such as class overlap, label noise, homoscedastic and heteroscedastic noise (Gal, 2016). Distributional uncertainty or dataset shift arises due to the distributional mismatch between the training and test examples (Quionero-Candela et al., 2009; Malinin & Gales, 2018). In this case, as the network receives unfamiliar out-of-distribution (OOD) test data, it should not confidently make predictions. The ability to separately model these three types of predictive uncertainty is important, as it enables the users to take appropriate actions depending on the source of uncertainty. For example, in the active learning scenario, distributional uncertainty indicates that the classifier requires additional data for training. On the other hand, for various real-world applications where the cost of an error is high, such as in autonomous vehicle control, medical, financial and legal fields, the source of uncertainty informs whether an input requires manual intervention. Recently notable progress has been made to detect OOD images. Bayesian neural network based approaches conflate the distributional uncertainty through model uncertainty (Hernandez-Lobato & Adams, 2015; Gal, 2016). However, since obtaining the true posterior distribution for the model parameters are intractable, the success of these approaches depends on the chosen prior distribution over parameters and the nature of approximations. Here, the predictive uncertainties can be measured by using an an ensemble of multiple stochastic forward passes using dropouts from a single DNN (Monti-Carlo Dropout or MCDP) (Gal & Ghahramani, 2016) or by ensembling results from multiple DNNs (Lakshminarayanan et al., 2017) and computing their mean and spread. On the other hand, most of the non-Bayesian approaches model their distributional uncertainty with data uncertainty. These approaches can explicitly train the network in a multi-task fashion incorporating both in-domain and OOD examples to produce sharp and flat predictive posteriors respectively (Lee et al., 2018a; Hendrycks et al., 2019). However, none of these approaches can robustly determine the source of uncertainty. Malinin & Gales (2018) introduced Dirichlet Prior Network (DPN) to distinctly model the distributional uncertainty from the other uncertainty types. A DPN classifier aims to produce sharp distributions to indicate low-order uncertainty for the in-domain examples and flat distributions for the OOD examples. However, their complex loss function, using the Kullback-Leibler (KL) divergence between Dirichlet distributions, results in the error surface to become poorly suited for optimization and makes it difficult efficiently train the DNN classifiers for challenging datasets with a large number of classes (Malinin & Gales, 2019). In this work, we aim to improve the training efficiency of the DPN framework by proposing a novel loss function that also allows the distributional uncertainty to be modeled distinctly from both data uncertainty and model uncertainty. Instead of explicitly using Dirichlet distributions in the loss function, we propose to apply the standard cross-entropy loss on the softmax outputs along with a novel regularization term for the logit (pre-softmax activation) outputs. We train the models in a multi-task function by leveraging both in-domain training images and OOD training images. The proposed loss function can be also viewed from the perspective of the non-Bayesian frameworks (Lee et al., 2018a; Hendrycks et al., 2019) where the proposed regularizer presents an additional term to control the sharpness of the output Dirichlet distributions. In our experiments, we demonstrate that our proposed regularization term can effectively control the sharpness of the output Dirichlet distributions from the DPN to detect distributional uncertainties along with making the framework scalable for more challenging datasets. We also find that the OOD detection performance of our DPN models often improves by augmenting Gaussian noises with the OOD training images to train the DPN. Our experimental results on CIFAR-10 and CIFAR-100 suggest that our proposed approach significantly improves the performance of the DPN framework for OOD detection and out-performs the recently proposed OOD detection techniques.In Bayesian frameworks, the predictive uncertainty of a classification model, trained on a finite dataset, Din = {xi, yi}Ni=1 ∼ Pin(x, y), is expressed in terms of data (aleatoric) and model (epistemic) uncertainty (Gal, 2016). For an input x∗, the predictive uncertainty is expressed as: p(ωc|x∗,Din) = ∫ p(ωc|x∗, θ) p(θ|Din) dθ (1) Here, x and y represents the images and the corresponding class labels, sampled from an underlying probability distribution pin(x, y). Here, the data uncertainty, p(ωc|x∗, θ) is described by the posterior distribution over class labels given model parameters, θ and model uncertainty, p(θ|Din), is given by the posterior distribution over parameters given the data, Din. The expected distribution for predictive uncertainty, p(ωc|x∗,Din) is obtained by marginalizing out θ. However, true posterior for p(θ|Din) is intractable. Approaches such as Monte-Carlo dropout (MCDP) (Gal & Ghahramani, 2016), Langevin Dynamics (Welling & Teh, 2011), explicit ensembling (Lakshminarayanan et al., 2017) approximate the integral in eq. 1 as: p(ωc|x∗,Din) ≈ 1 M M∑ m=1 p(ωc|x∗,θ(m)) θ(m) ∼ q(θ) (2) where, θ(m) is sampled from an explicit or implicit variational approximation, q(θ) of the true posterior p(θ|Din). Each p(ωc|x∗, θ(m)) represents a categorical distribution, µ = [µ1, · · · , µk] = [p(y = ω1), · · · , p(y = ωK)] over the class labels and the ensemble can be visualized as a collection of points on the simplex. While for a confident prediction, the ensemble is expected to sharply appear in one corner of the simplex, the flatly spread ensembles cannot determine whether the uncertainty is due to data or distributional uncertainty. Furthermore, for standard DNNs, with millions of parameters, it becomes even harder to find an appropriate prior distribution and inference scheme to estimate the posterior distribution of the model. Dirichlet Prior Network (DPN) is introduced to explicitly model the distributional uncertainty by parameterizing a Dirichlet distribution over a simplex (Malinin & Gales, 2018). More discussions about DPN is presented in section 3.1. Alternatively, non-Bayesian frameworks derive their measure of uncertainties using the predictive posteriors obtained from DNNs. Lee et al. (2018a) and Hendrycks et al. (2019) introduce new components in their loss functions to explicitly incorporate OOD data for training. DeVries & Taylor (2018) append an auxiliary branch onto a pre-trained classifier to derive the OOD score. Shalev et al. (2018) uses multiple semantic dense representations as the target label to train the OOD detection network. Several recent works such as (Lee et al., 2018b; Liang et al., 2018) have demonstrated that by tweaking the input images during inference using adversarial perturbations can enhance the performance of a DNN for OOD detection (Goodfellow et al., 2014b). However, their discriminative scores are achieved by tailoring the parameters for each OOD distributions during test time, which is not possible for real-world OOD examples. Hein et al. (2019) propose an adversarial training (Madry et al., 2018) like approach to produce lower confident predictions for OOD examples. However, while these models can identify the total predictive uncertainties, they can not robustly determine whether the source of uncertainty is due to an in-domain input in a region of class overlap or an OOD example far away from the training distribution.This section first describes the DPN framework and the difficulties of the existing modeling techniques to scale DPNs for challenging datasets. We then present our improved version DPN by proposing a novel loss function to address these difficulties while allowing to model the distributional uncertainty distinctly from the model and data uncertainty.A DPN for classification directly parametrizes a prior Dirichlet distribution over the categorical output distributions on a simplex (Malinin & Gales, 2018). For in-domain examples, a DPN attempts to produce sharp Dirichlet in one corner of the simplex, when it is confident in its predictions (Figure 1a). It should produce a sharp distribution in the middle of the simplex to indicate the data (loworder) uncertainty for the in-domain example with a high degree of noise or belongs to a class overlapping region (Figure 1b). In contrast, for OOD examples, a DPN should produce a Dirichlet that spreads over the simplex or across the edge of the simplex to indicate the distributional uncertainty (Figure1c). Here, the data uncertainty is expressed by the point-estimate categorical distribution µ while the distributional uncertainty is described using the distribution over the predictive categorical i.e p(µ|x∗, θ). The overall predictive uncertainty is expressed as: p(ωc|x∗,D) = ∫ ∫ p(ωc|µ) p(µ|x∗,θ) p(θ|D) dµ dθ (3) This expression forms a three layered hierarchy of uncertainties: a large model uncertainty, p(θ|D) would induce a large variation in distributional uncertainty in p(µ|x∗,θ) and a large degree of uncertainty for µ leads to higher data uncertainty. DPN framework is consistent with the existing approaches, where an additional layer of uncertainty is included to capture the distributional uncertainty. For example, marginalization of µ in Eqn. 3 will reproduce Eqn. 1 while loose the control over the sharpness of the output Dirichlet distributions. The marginalization of θ produces the expected estimation of data and distributional uncertainty given model uncertainty as: p(ωc|x∗,D) = ∫ p(ωc|µ) [ ∫ p(µ|x∗,θ) p(θ|D)dθ ] dµ = ∫ p(ωc|µ) p(µ|x∗,D)dµ (4) However, similar to eq. 1 marginalizing θ is eq. 4 is also intractable. Since the model uncertainty is reducible given large training data, for simplicity, here we assume a dirac delta estimation, θ̂ for θ: p(θ|D) = δ(θ − θ̂) =⇒ p(µ|x∗,D) ≈ p(µ|x∗, θ̂) (5) Constructing a DPN. A DPN constructs a Dirichlet distribution as a prior over the categorical distributions, which is parameterized by the concentration parameters, α = α1, · · · , αK . Dir(µ|α) = Γ(α0)∏K c=1 Γ(αc) K∏ c=1 µαc−1c , αc > 0, α0 = K∑ c=0 αc (6) where, α0 is called the precision of the Dirichlet. A larger value of α0 produces sharper distributions to indicate low order uncertainties (fig 1a and 1b). A DPN, fθ̂ produces the concentration parameters, α and the posterior over class labels, p(ωc|x∗; θ̂), is given by the mean of the Dirichlet. α = fθ̂(x ∗) p(µ|x∗; θ̂) = Dir(µ|α) p(ωc|x∗; θ̂) = ∫ p(ωc|µ) p(µ|x∗; θ̂) dµ = αc α0 (7) A standard DNN with the softmax activation function can be represented as a DPN where the concentration parameters are αc = ezc(x ∗); zc(x∗) is the pre-softmax (logit) output corresponding to the class, c for an input x∗. The expected posterior probability of class label ωc is given as: p(ωc|x∗; θ̂) = αc α0 = ezc(x ∗)∑K c=1 e zc(x∗) (8) However, the mean of the Dirichlet is now insensitive to any arbitrary scaling of αc. Hence, the precision of the Dirichlet, α0, degrades under the standard cross-entropy loss. Malinin & Gales (2018) instead introduced a new loss function that explicitly minimizes the KL divergence between the output Dirichlet and a target Dirichlet to produce a predefined target precision value for the output Dirichlet distributions. For in-domain examples, the target distribution is chosen to be a sharp Dirichlet, Dir(µ|α̂y), focusing on their ground truth classes. While for OOD examples, a flat Dirichlet, Dir(µ|α̃) is selected that spreads over the whole simplex. L(θ) = EPinKL[Dir(µ|α̂y)||p(µ|x,θ)] + EPoutKL[Dir(µ|α̃)||p(µ|x,θ)] (9) where, Pin and Pout are the underlying distribution of in-domain and OOD training examples respectively. However, learning the model using sparse 1-hot continuous distributions for class labels, which are effectively a delta function, is challenging due to their complex loss function (eq. 9). Here, the error surface becomes poorly suited for optimization using the back-propagation algorithm (Malinin & Gales, 2019). Malinin & Gales (2018) tackle this problem by using label smoothing (Szegedy et al., 2016) or teacher-student training (Hinton et al., 2015a) to redistribute a small amount of probability density to each corner of the Dirichlet. This technique is found to work well for datasets with a fewer number of class labels. However, for more challenging datasets with a large number of classes, even these techniques cannot efficiently redistribute the probability densities at each corner and results in the target distribution to tend to a delta function. Hence, it becomes difficult to train the DPN to achieve competitive performances. Malinin & Gales (2019) have recently proposed to reverse the terms within the KL divergence in eq. 9 to improve the training efficiency of DPN models. This approach still requires to explicitly constrain the precision of the output Dirichlet distributions using an appropriately chosen hyper-parameter for training.We now propose an improved technique to model DPN by proposing a novel loss function using the standard cross-entropy loss along with a regularization term to control the precision of the output Dirichlet distribution from the network. As we have seen in equation 8, the precision of a Dirichlet distribution produced by the standard DNN is given as ∑K c=1 exp zc(x ∗). Hence, we can control the sharpness of the distribution by designing a regularization term that increases the sum of logit (presoftmax) outputs for the in-domain examples to produce sharp distributions to indicate their lower uncertainties. For the OOD examples, the regularization term aims to decrease the sum of logit (presoftmax) outputs to produce flat distributions to indicate distributional (higher-order) uncertainties. Hence, instead of explicitly constraining the precision the output Dirichlet with a specific hyperparameter, we allow the network to appropriately produce the precision values for different input. In this paper, we propose the regularization term as 1K ∑K c=1 sigmoid(zc(x)) to control the sharpness of the output Dirichlet distribution along with the standard cross-entropy loss for classification. Here, the sigmoid function is chosen for the regularization term as it always produces a value within the range of (0, 1) for any zc(x). For in-domain training examples, the loss function is given as: Lin(θ) = EPin [ − log p(y|x,θ)− λin K K∑ c=1 sigmoid(zc(x)) ] , (10) For OOD training examples, the loss function is given as: Lout(θ) = EPout [ Hc(U ; p(ω|x,θ))− λout K K∑ c=1 sigmoid(zc(x)) ] , (11) where, U denotes the uniform distribution over the class labels. Hc is the cross-entropy function. We train the network in a multi-task fashion with the overall loss function as: min θ L(θ) = Lin(θ) + λLout(θ), λ > 0 (12) In this loss function, we have incorporated three user-defined hyper-parameters: λin , λout and λ in Eq. 10, Eq. 11, and Eq. 12 respectively. λ balances between the loss values for in-domain examples and OOD examples. The hyper-parameters λin and λout controls the sharpness of the output Dirichlet from a DPN. By choosing λin > 0, we enforce the network to produce positive logit values for in-domain examples that lead to producing sharper Dirichlet distributions (Fig. 2(a)). We choose λin > λout to ensure that the density is either spread over the the simplex or across the boundary (Fig 1c). The choice of λin > λout > 0 will lead the network to produce comparatively flatter Dirichlet distributions for OOD examples (Fig. 2b). In contrast, by choosing λout < 0, we enforce the network to produce negative values for zc(x∗) and hence fractional values for αc’s (i.e αc ∈ (0, 1)) for OOD examples. This will cause the densities of the Dirichlet to be distributed in the edges of the simplex and produces an extremely sharp distribution, as shown in Fig 2c. The proposed loss function in Eq. 12 is also very closely related to non-Bayesian approaches, where by choosing λin, λout to zero we re-obtain similar loss functions as proposed by Lee et al. (2018a); Hendrycks et al. (2019). However, by setting λin, λout to zero, we lose control over the precision of the Dirichlet distribution that distinguishes distributional uncertainty from data uncertainty. Our multi-task loss function (eq. 12) requires training samples from the in-domain distribution, Pin as well as from OOD Pout. However, since Pout is unknown, Lee et al. (2018a) propose to synthetically generate the OOD training samples from the boundary of in-domain region, Pin using generative models such as GAN (Goodfellow et al., 2014a). Alternatively, a different, easily available, real datasets can be used as OOD training examples. In practice, the latter approach is to found to be more effective for training the OOD detectors and has been applied for our experiments on vision datasets (Hendrycks et al., 2019).We demonstrate the effectiveness of the DPN framework using the proposed loss function by conducting two sets of experiments. First, we experiment on a synthetic dataset. Next, we present a comparative study of our proposed framework with the existing approaches on CIFAR10 and CIFAR100, and show the advantages over the original DPN framework (Malinin & Gales, 2018).We construct a simple dataset with three classes where the instances are sampled from three different isotropic Gaussian distributions as shown in Figure 3(a). We select isotropic co-variances, σ2I with σ = 4, to ensure that the classes are overlapping. We train a small DPN with 2 hidden layers of 50 nodes each for the synthetic dataset. For our loss function, we set λ as 1.0. We choose both positive and negative values for λout in our experiments. Here, λin and λout are chosen as λin = (1 − β) and λout = ( 1#class − β). We train two different sets of DPN models using β = 0.0 (i.e positive λout) and 0.5 (i.e negative λout), denoted as DPN(β = 0.0) and DPN(β = 0.5), respectively (See Appendix B for additional details). An uncertainty measure can be computed as the probability of the predicted class or max probability, maxP = maxc p(ωc|x∗, Din), in the expected predictive categorical distribution, p(ωc|x∗, Din) (Figure 3(b) and 3(c) for DPN(β = 0.0) and DPN(β = 0.5), respectively). Entropy of the predicted distribution,H[p(ωc|x∗, Din)] = − ∑K c=1 p(ωc|x ∗, Din) ln p(ωc|x∗, Din), can be also applied as a total uncertainty measure that produces low scores when the model is confident in its prediction (Figure 3(d) and 3(e) for DPN(β = 0.0) and DPN(β = 0.5) respectively). Max probability and entropy are the most frequently used uncertainty measures used by the existing OOD detection models. However, since the predicted distribution is obtained by marginalizing µ (eq. 3), these measures cannot capture the sharpness of the output Dirichlet for a DPN. Hence, they cannot distinguish between misclassified examples and OOD examples. This observation also indicates the limitation of the existing nonDPN approachesto differentiate between data and distributional uncertainties (Malinin & Gales, 2018). A DPN framework address this limitation by using the differential entropy (D-Ent) as an uncertainty measure that produces lower scores for sharper Dirichlet distributions (eq. 13). Appendix D presents the expression to compute D-Ent scores for a given data point. H[p(µ|x∗, Din)] = − ∫ SK−1 p(µ|x∗, Din) ln p(µ|x∗, Din) (13) Figure 3(f) and 3(g) demonstrate that D-Ent can distinguish between in-domain and OOD examples. DPN(β = 0.0) produces smaller positive logit values, resulting in flat Dirichlet distributions for OOD examples. Hence, we obtain relatively lower D-Ent scores for OOD examples compared to the in-domain examples (fig 3 (d)). Subsequently, DPN(β = 0.5) produces negative logit values, resulting in sharp Dirichlet distributions across the edge of the simplex for OOD examples (Fig. 2(c)). As we can see in Figure 3(g), the D-Ent scores for OOD examples are even smaller than the in-domain confident predictions, indicating that the output Dirichlet distributions for OOD examples are even sharper. Since we explicitly constrain the logit outputs to produce smaller values for OOD examples, it is meaningful to define the sum of the exponential of logits, ∑K c=1 e zc(x ∗), as a new measure of predictive uncertainty. In Figure 3(d) and 3(h), we visualize this uncertainty measure for our DPN(β = 0.0) and DPN(β = 0.5), respectively. We found that our DPN models produce very high logit values. Hence, we scaled down these values by a factor of 100 before computing this uncertainty measure for better visualization. We can see, ∑K c=1 e zc(x ∗) produces high scores for all in-domain data points to distinguish them from OOD examples.Experimental setup. In this section, we present our experiments on CIFAR-10 and CIFAR-100 datasets (Krizhevsky, 2009). CIFAR-10 images belong to 10 different classes while CIFAR-100 is a more challenging dataset, containing 100 image classes. Both of these datasets contain 32×32 natural colored images of 50, 000 training and 10, 000 testing examples. In Appendix-A, we have also experimented on TinyImageNet (TIM) that consists of 64× 64 natural images from 200 classes. The CIFAR-10 classifiers are trained using CIFAR-10 training images as in-domain and CIFAR-100 training images as OOD dataset (Simonyan & Zisserman, 2014b). We use VGG-16 architecture for this case. For We present in-domain misclassification detection and OOD detection experiments to evaluate the performance of our models. For our misclassification detection experiments, we take the in-domain test set and attempt to distinguish the correctly classified examples from misclassified examples (see Table 2). The OOD detection experiments attempt to distinguish the in-domain test images from unknown OOD images. In Table 3, we present a smaller set of results where we consider TinyImageNet (TIM) as the OOD dataset and attempt to distinguish them from in-domain test data points. In Appendix A, we present an expanded version of this comparative table for a wide range of OOD examples. Note that, the OOD test images are selected from datasets different from the datasets used for training. The description of our training and test datasets are presented in Table 1. Please refer to Appendix C for further details. Evaluation of Predictive Uncertainty Estimation. To evaluate the performance of our model for misclassification detection, we consider the misclassified examples as the positive class and correctly classified examples as the negative class. For the OOD detection task, we treat the OOD examples as the positive class and in-domain examples as a negative class. The detection performance for these tasks are measured using two metrics: area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPR) (Hendrycks & Gimpel, 2016). The AUROC can be interpreted as the probability of an OOD example to produce a higher detection score than an in-domain example (Davis & Goadrich, 2006). Hence, a higher AUROC is desirable, and an uninformative detector produces an AUROC ≈ 50%. The AUPR is more informative when the positive class and negative class have greatly differing base rates. It can take these base rates into account (Manning & Schütze, 1999). Hyper-parameters for Training Loss. Unlike Liang et al. (2018); Lee et al. (2018a), and similar toMalinin & Gales (2018); Hendrycks et al. (2019), we do not require to tune any hyper-parameters during testing for different datasets. In other words, the OOD test examples remain unknown to our DPN classifiers, as in a real-world scenario. During training, we set λ = 0.5 (eq. 12), similar to Hendrycks et al. (2019). We train multiple DPN models for CIFAR-10 and CIFAR-100 classifiers using both positive and negative values for λout. We choose λin and λout as λin = (1 − β) and λout = ( 1#class − β). We train two different sets of DPN models using 0.0 (i.e positive λout) and β = 0.5 (i.e negative λout), denoted as DPNsoft(β : 0.0) and DPNsoft(β : 0.5), respectively. Detecting the source of uncertainty. [1] DPN model with λout > 0: By choosing a positive value for λout, we enforce the network to produce flat Dirichlet distributions over the simplex for OOD examples. Hence, the D-Ent measure should produce higher scores for OOD examples and lower scores for in-domain examples. Therefore, given a test example with high scores for both entropy as well as D-Ent, it indicates distributional uncertainty. In contrast, if the test example achieves high entropy scores and lower D-Ent score, it indicates data uncertainty. Our DPNsoft(β : 0.0) models achieve high AUROC and AUPR scores using D-Ent to detect OOD examples for both CIFAR-10 and CIFAR-100 (see Table 3) while achieves lower AUROC and AUPR scores for D-Ent while detecting misclasification for CIFAR-100 (see Table 2). However for CIFAR- 10, our DPNsoft(β : 0.0) models fail to produce low AUROC and AUPR scores under D-Ent measure for misclassification detection. This indicates that these models achieve high D-Ent scores for both misclassified and OOD examples and make it difficult to distinguish distributional uncertainty from data uncertainty for CIFAR-10. Note that, similar to our DPNsoft(β : 0.0), DPNDir (Malinin & Gales, 2018) also produces high AUPR and AUROC scores under D-Ent measure for for both misclassified examples and OOD examples for CIFAR-10 model (Table 2 and 3). This indicates that both DPNDir and our DPNsoft(β : 0.0) models may be ineffective to detect distributional uncertainty for CIFAR-10. In contrast, our DPNsoft(β : 0.5) models are always found to be effective to identify the source of uncertainties. [2] DPN model with λout < 0: In contrast, if we choose a negative value for λout, our DPN network produces very sharp Dirichlet distributions across the edge of the simplex for OOD examples. Hence, it produces very low D-Ent scores. Notably, these distributions are even sharper than the Dirichlet distributions obtained for confidently predicted examples. Hence, for a test example, we can detect distributional uncertainty if it achieves high entropy and low D-Ent scores. As we can see, our DPNsoft(β : 0.5) models produce very low AUROC and AUPR scores for D-Ent measure to detect OOD examples (Table 3) while achieve relatively higher AUROC and AUPR scores for detecting misclassified examples (Table 2). Data Augmentation with white-noise for Training. After training our DPN models using clean training images, we also fine-tune them using noisy OOD training images for a few epochs. We add minor Gaussian noise sampled from isotropic Gaussian distributions N (0, σ2I) with our OOD training images. Here, the idea is to add minor perturbation without distorting the features of the OOD training images. It further exposes the network into the out of distribution space. Note that, the DPN models are fine-tuned only using OOD training images. The OOD test examples remain unknown. In practice, this technique may often further improve the OOD detection performance of our DPNsoft models as we choose σ = 0.01 (see Table 3). However, for larger perturbation, performance degrades for the DPNsoft(σ = 0.05). Comparative Study. In Table 2 and 3, we compare the performance of our approach with several baselines, such as standard DNN (Hendrycks & Gimpel (2016)), MCDP (Gal & Ghahramani, 2016), DPNDir (Malinin & Gales, 2018), ODIN (Liang et al., 2018), and OE (Hendrycks et al., 2019). We use the same architecture as our DPNsoft models for all the competitive models. OE models are trained using the set of in-domain and OOD training images with their proposed loss function (Hendrycks et al., 2019). Note that, since non-DPN methods do not explicitly model the logit outputs, the D-Ent or ∑K c=1 e zc(x ∗) measures are not meaningful (Malinin & Gales, 2018). We also compare our results with the existing DPN framework, namely DPNDir Malinin & Gales (2018) for CIFAR-10. Note that, the DPNDir framework fails to scale for the CIFAR-100 dataset that consists of 100 classes. Due to the unavailability of codes and results (under the same settings), we could not compare our model with Malinin & Gales (2019). Overall, our DPNsoft models significantly improved the performance of the DPN framework and consistently out-performed the existing OOD detection models. In addition, it is able to distinguish distributional uncertainty from other uncertainty types.In this paper, we propose a novel framework to improve the training efficiency of DPN models for challenging classification tasks with large number of classes. We also propose a novel regularization term that allows controls the sharpness of the Dirichlet distributions. We show that the proposed regularizer can be easily integrated with the standard cross-entropy loss function. Our experiments on synthetic and real datasets demonstrate that our proposed framework can efficiently distinguish the distributional uncertainty from other uncertainty types. We demonstrate that the OOD detection performance of our DPN models can be often improved by training with noisy OOD examples. Experiments show that our proposed approach significantly improves DPN frameworks, and outperforms the existing OOD detectors on both CIFAR-10 and CIFAR-100 datasets.The three classes of our synthetic dataset are constructed by sampling from three different isotropic Gaussian distributions with means of (−4, 0), (4, 0) and (0, 5) and isotropic variances of σ = 4. We sample 200 training data points from each distribution for each class. We also sample 600 OOD training examples from an uniform distribution of U([−15, 15], [−13, 17]). We train a neural network with 2 hidden layers with 50 nodes each and relu activation function. The network is trained for 2, 500 epochs using stochastic gradient descent (SGD) optimization with a constant learning rate of 0.01.C.1 TRAINING DETAILS For our experiments on CIFAR-10, we train a VGG-16 model with CIFAR-10 as the in-domain and CIFAR100 as the OOD training data (Simonyan & Zisserman (2014b)). For CIFAR-100, we train a DenseNet with depth = 55, growth rate = 12 and CIFAR-100 as the in-domain and CIFAR-10 as the OOD training data (Huang et al. (2017)). We trained multiple DPNsoft models using our proposed loss functions with different hyper-parameters. For CIFAR-10, we use the VGG-16 network. Here, we use CIFAR-10 training images (50, 000 images) as our in-domain training data and CIFAR-100 training images (50, 000 images) as our OOD training data. For CIFAR-100, Densenet(55, 12) is trained using the same setup as proposed by Huang et al. (2017). Here, we use CIFAR-100 training images (50, 000 images) as our in-domain training data and CIFAR-10 training images (50, 000 images) as our OOD training data. For TinyImageNet (TIM), we use the VGG-16 network. Here, we use TIM training images (100,000 images) as our in-domain training data and ImageNet-25K images (25, 000 images) as our OOD training data. ImageNet25K is obtained by randomly selecting 25, 000 images from the ImageNet dataset (Deng et al., 2009). After training the models with clean in-domain and OOD images, we further fine-tune the models using noisy OOD training images for 50 epochs with the learning rate of 0.0001. The noises are chosen form an isotropic Gaussian distribution, N (0, σ2I). We have experimented with three different values of σ as {0.0, 0.01, 0.05} to introduce different level of noises. C.2 OOD TEST DATASETS We use a wide range of OOD dataset to evaluate the performance of our proposed OOD detection models. For CIFAR-10 and CIFAR-100 classifiers, these input test images are resized to 32× 32, while for TinyImageNet classifiers, we resize them to 64 × 64. In this task, we attempt to distinguish the unknown OOD images from the corresponding in-domain test images from different classification tasks. We compute different uncertainty measures for these images for this purpose. For our evaluations, we use the following OOD images as described in the following. 1. TinyImageNet (TIM) (Li et al. (2017)). This is a subset of Imagenet dataset. We use the validation set, that contains 10, 000 test images from 200 different image classes for our evaluation during test time. This dataset is used as an OOD test dataset only for CIFAR-10 and CIFAR-100 classifiers. Note that, for TinyImageNet classifiers, this is the in-domain test set. 2. LSUN (Yu et al. (2015)). The Large-scale Scene UNderstanding dataset (LSUN) contains images of 10 different scene categories. We use its validation set, containing 10, 000 images, as an unknown OOD test set. 3. Places 365 (Zhou et al. (2017)). The validation set of this dataset consists of 36500 images of 365 scene categories. 4. Textures (Cimpoi et al. (2014)) contains 5640 textural images in the wild belonging to 47 categories. 5. STL-10 contains 8, 000 images of natural images from 10 different classes (Coates et al., 2011). 6. Gaussian Noise. This is an artificially generated dataset obtained by adding Gaussian noise to the in-domain test images. The Gaussian noises are sampled from an isotropic Gaussian distribution, N (0, σ2I) with σ = 0.05. C.3 DETAILS OF COMPETITIVE SYSTEMS We compare the performance of our models with standard DNN as baseline model (Hendrycks & Gimpel (2016)), the Bayesian framework, monti-carlo dropout (MCDP) (Gal & Ghahramani (2016)), DPNDir using the loss function proposed by Malinin & Gales (2018), non-Bayesian frameworks such as ODIN (Liang et al. (2018)) and outlier exposure (OE) by Hendrycks et al. (2019). We use the same architecture as DPNsoftmax for the competitive models. For DPNDirichlet, we could not reproduce the same performance as given in Malinin & Gales (2018) and hence use their reported results for CIFAR-10 for our comparison. For MCDP, we use the standard DNN model with randomly dropping the nodes during test time. The predictive categorical distributions are obtained by averaging the outputs for 10 iterations. ODIN applies the standard DNN models trained only using in-domain training examples for OOD detection. During testing phase, it perturbs the input images using FGSM adversarial attack (Goodfellow et al. (2014b))and softmax activation function by incorporating the temperature hyper-parameter (Hinton et al. (2015b)). The maximum Probability score is then applied for their uncertainty measure. They propose to use different hyper-parameters for different OOD examples. However, in practice, the source of expected OOD examples cannot be known. Hence, for our comparisons, we always set the perturbation size to 0.002 and the temperature to 1000. OE models are trained using the proposed loss function by Hendrycks et al. (2019). Here, we use the same training set up as applied for our DPNsoft models: CIFAR-10 classifiers are trained using CIFAR-10 training images as in-domain examples and CIFAR-100 training images as OOD examples. For CIFAR-100, the OE models are trained using CIFAR-10 training images as OOD examples.Differential Entropy of a Dirichlet distribution can be calculated as follows (Malinin & Gales, 2018): H[p(µ|x∗, Din)] = − ∫ SK−1 p(µ|x∗, Din) ln p(µ|x∗, Din) = K∑ c=1 ln Γ(αc)− ln Γ(α0)− K∑ c=1 (αc − 1)(ψ(αc)− ψ(α0)) (14) Note that, αc is a function of x∗. Γ and ψ denotes the Gamma and digamma functions respectively.
A long-term goal in artificial intelligence is for agents to learn how to act. This endeavor relies on accurately predicting and optimizing for the outcomes of actions, and fundamentally involves estimating counterfactuals—what would have happened if the agent acted differently? In many applications, such as the treatment of patients in hospitals, experimentation is infeasible or impractical, and we are forced to learn from biased, observational data. Doing so requires adjusting for the distributional shift between groups of patients that received different treatments. A related kind of distributional shift arises in unsupervised domain adaptation, the goal of which is to learn predictive models for a target domain, observing ground truth only in a source domain. In this work, we pose both domain adaptation and treatment effect estimation as special cases of prediction across shifting designs, referring to changes in both action policy and feature domain. We separate policy from domain as we wish to make causal statements about the policy, but not about the domain. Learning from observational data to predict the counterfactual outcome under treatment B for a patient who received treatment A, one must adjust for the fact that treatment A was systematically given to patients of different characteristics from those who received treatment B. We call this predicting under a shift in policy. Furthermore, if all of our observational data comes from hospital P , but we wish to predict counterfactuals for patients in hospital Q, with a population that differs from P , an additional source of distributional shift is at play. We call this a shift in domain. Together, we refer to the combination of domain and policy as the design. The design for which we observe ground truth is called the source, and the design of interest the target. The two most common approaches for addressing distributional shift are to learn shift-invariant representations of the data (Ajakan et al., 2014) or to perform sample re-weighting or matching (Shimodaira, 2000; Kallus, 2016). Representation learning approaches attempt to extract only information from the input that is invariant to a change in design and predictive of the variable of interest. Such representations are typically learned by fitting deep neural networks in which activations of deeper layers are regularized to be distributionally similar across designs (Ajakan et al., 2014; Long et al., 2015). Although representation learning can be shown to reduce the error associated to distributional shift (Long et al., 2015) in some cases, standard approaches are biased, even in the limit of infinite data, as they penalize the use also of predictive information. In contrast, re-weighting methods correct for distributional shift by assigning higher weight to samples from the source design that are representative of the target design, often using importance sampling. This idea has been well studied in, for example, the causal inference (Rosenbaum & Rubin, 1983), domain adaptation (Shimodaira, 2000) and reinforcement learning (Precup et al., 2001) literature. For example, in causal effect estimation, importance sampling is equivalent to re-weighting units by the inverse probability of observed treatments (treatment propensity). Re-weighting with knowledge of importance sampling weights often leads to asymptotically unbiased estimators of the target outcome, but may suffer from high variance in finite samples (Swaminathan & Joachims, 2015). A significant hurdle in applying re-weighting methods is that optimal weights are rarely known in practice. There are a variety of methods to learn these weights. Weights can be estimated as the inverse of estimated feature or treatment densities (Rosenbaum & Rubin, 1983; Freedman & Berk, 2008) but this plug-in approach can lead to highly unstable estimates. More stable methods learn weights by minimizing distributional distance metrics (Gretton et al., 2009; Kallus, 2016; 2017; Zubizarreta, 2015). Closely related, matching (Stuart, 2010) produces weights by finding units in the source design that are close in some metric to units in the target design. Specifying a distributional or unit-wise metric is challenging, especially if the input space is high-dimensional where no metric incorporating all features can ever be made small. This has inspired heuristics such as first performing variable selection and then finding a matching in the selected covariates. Our key algorithmic contribution is to show how to combine the intuition behind shift-invariant representation learning and re-weighting methods by jointly learning a representation Φ of the input space and a weighting function w(Φ) to minimize a) the re-weighted empirical risk and b) a re-weighted measure of distributional shift between designs. This is useful also for the identity representation Φ(x) = x, as it allows for principled control of the variance of estimators through regularization of the re-weighting function w(x), mitigating the issues of exact importance sampling methods. Further, this allows us to evaluate w on hold-out samples to select hyperparameters or do early stopping. Finally, letting w depend on Φ alleviates the problem of choosing a metric by which to optimize sample weights, as Φ is trained to extract information predictive of the outcome. We capture these ideas in an upper bound on the generalization error under a shift in design and specialize it to the case of treatment effect estimation. Main contributions We bring together two techniques used to overcome distributional shift between designs—re-weighting and representation learning, with complementary robustness properties, generalizing existing methods based on either technique. We give finite-sample generalization bounds for prediction under design shift, without assuming access to importance sampling weights or to a well-specified model, and develop an algorithmic framework to minimize these bounds. We propose a neural network architecture that jointly learns a representation of the input and a weighting function to improve balance across changing settings. Finally, we apply our proposed algorithm to the task of predicting causal effects from observational data, achieving state-of-the art results on a widely used benchmark.The goal of this work is to accurately predict outcomes of interventions T ∈ T in contexts X ∈ X drawn from a target design pπ(X,T ). The outcome of intervening with t ∈ T is the potential outcome Y (t) ∈ Y (Imbens & Rubin, 2015, Ch. 1–2), which has a stationary distribution pt(Y | X) given context X . Assuming a stationary outcome is akin to the covariate shift assumption (Shimodaira, 2000), often used in domain adaptation.1 For example, in the classical binary setting, Y (1) represents the outcome under treatment and Y (0) the outcome under control. The target design consists of two components: the target policy pπ(T | X), which describes how one intends to map observations of contexts (such as patient prognostics) to interventions (such as pharmacological treatments) and the target domain pπ(X), which describes the population of contexts to which the policy will be applied. The target design is known to us only through m unlabeled sam- 1Equivalently, we may write pπ(Y (t) | X) = pµ(Y (t) | X). ples (x′1, t ′ 1), . . . , (x ′ m, t ′ m) from pπ(X,T ). Outcomes are only available to us in labeled samples from a source domain: (x1, t1, y1), . . . , (xn, tn, yn), where (xi, ti) are draws from a source design pµ(X,T ) and yi = yi(ti) is a draw from pT (Y | X), corresponding only to the factual outcome Y (T ) of the treatment administered. Like the target design, the source design consists of a domain of contexts for which we have data and a policy, which describes the (unknown) historical administration of treatment in the data. Only the factual outcomes of the treatments administered are observed, while the counterfactual outcomes yi(t) for t 6= ti are, naturally, unobserved. Our focus is the observational or off-policy setting, in which interventions in the source design are performed non-randomly as a function of X , pµ(T | X) 6= pµ(T ). This encapsulates both the covariate shift often observed between treated and control populations in observational studies and the covariate shift between the domain of the study and the domain of an eventual wider intervention. Examples of this problem are plentiful: in addition to the example given in the introduction, consider predicting the return of an advertising policy based on the historical results of a different policy, applied to a different population of customers. We stress that we are interested in the causal effect of an intervention T on Y , conditioned on X . As such, we cannot think of X and T as a single variable. Without additional assumptions, it is impossible to deduce the effect of an intervention based on observational data alone (Pearl, 2009), as it amounts disentangling correlation and causation. Crucially, for any unit i, we can observe the potential outcome yi(t) of at most one intervention t. In our analysis, we make the following standard assumptions. Assumption 1 (Consistency, ignorability and overlap). For any unit i, assigned to intervention ti, we observe Yi = Y (ti). Further, {Y (t)}t∈T and the data-generating process pµ(X,T, Y ) satisfy strong ignorability: {Y (t)}t∈T ⊥ T | X and overlap: Prpπ (pµ(T | X) > 0) = 1. Assumption 1 is a sufficient condition for causal identifiability (Rosenbaum & Rubin, 1983). Ignorability is also known as the no hidden confounders assumption, indicating that all variables that cause both T and Y are assumed to be measured. Under ignorability therefore, any domain shift in p(X) cannot be due to variables that causally influence T and Y , other than through X . Under Assumption 1, potential outcomes equal conditional expectations: E[Y (t) | X = x] = E[Y | X = x, T = t], and we may predict Y (t) by regression. We further assume common domain support, ∀x ∈ X : pπ(X = x) > 0⇒ pµ(X = x) > 0. Finally, we adopt the notation p(x) := p(X = x).We attempt to learn predictors f : X × T → Y such that f(x, t) approximates E[Y | X = x, T = t]. Recall that under Assumption 1, this conditional expectation is equal to the (possibly counterfactual) potential outcome Y (t), conditioned on X . Our goal is to ensure that hypotheses f are accurate under a design pπ that deviates from the data-generating process, pµ. This is unlike standard supervised learning for which pπ = pµ. We measure the (in)ability of f to predict outcomes under π, using the expected risk, Rπ(f) := Ex,t,y∼pπ [`f (x, t, y)] (1) based on a sample from µ, Dnµ = {(xi, ti, yi) ∼ pµ; i = 1, ..., n}. Here, `f (x, t, y) := L(f(x, t), y) is an appropriate loss function, such as the squared loss, L(y, y′) := (y − y′)2 or the log-loss, depending on application. As outcomes under the target design pπ are not observed, even through a finite sample, we cannot directly estimate (1) using the empirical risk under pπ . A common way to resolve this is to use importance sampling (Shimodaira, 2000)—the observation that if pµ and pπ have common support, with w∗(x, t) = pπ(x, t)/pµ(x, t), Rw ∗ µ (f) := Ex,t,y∼pµ [w∗(x, t)`f (x, t, y)] = Rπ(f) . (2) Hence, with access to w∗, an unbiased estimator of Rπ(f) may be obtained by re-weighting the (factual) empirical risk under µ, R̂w ∗ µ (f) := 1 n n∑ i=1 w∗(xi, ti)`f (xi, ti, yi) . (3) Unfortunately, importance sampling weights can be very large when pπ is large and pµ small, resulting in large variance in R̂w ∗ µ (f) (Swaminathan & Joachims, 2015). More importantly, pµ(x, t) is rarely known in practice, and neither is w∗. In principle, however, any re-weighting function w with the following property yields a valid risk under the re-weighted distribution pwµ . Definition 1. A function w : X × T → R+ is a valid re-weighting of pµ if Ex,t∼pµ [w(x, t)] = 1 and pµ(x, t) > 0⇒ w(x, t) > 0. We denote the re-weighted density pwµ (x, t) := w(x, t)pµ(x, t). A natural candidate in place of w∗ is an estimate ŵ∗ based on estimating densities pπ(x, t) and pµ(x, t). In this work, we adopt a different strategy, learning parameteric re-weighting functions w from observational data, that minimize an upper bound on the risk under pπ .An important special case of our setting is when treatments are binary, T ∈ {0, 1}, often interpreted as treating (T = 1) or not treating (T = 0) a unit, and the domain is fixed across designs, pµ(X) = pπ(X). This is the classical setting for estimating treatment effects—the effect of choosing one intervention over another (Morgan & Winship, 2014).2 The effect of an intervention T = 1 in context X , is measured by the conditional average treatment effect (CATE), τ(x) = E [Y (1)− Y (0) | X = x]. Predicting τ for unobserved units typically involves prediction of both potential outcomes3. In a clinical setting, knowledge of τ is necessary to assess which medication should be administered to a certain individual. Historically, the (population) average treatment effect, ATE = Ex∼p[τ(x)], has received comparatively much more attention (Rosenbaum & Rubin, 1983), but is inadequate for personalized decision making. Using predictors f(x, t) of potential outcomes Y (t) in contextsX = x, we can estimate the CATE by τ̂(x) = f(x, 1)−f(x, 0) and measure the quality using the mean squared error (MSE), MSE(τ̂) = Ep [ (τ̂(x)− τ(x))2 ] (4) In Section 4, we argue that estimating CATE from observational data requires overcoming distributional shift with respect to the treat-all and treat-none policies, in predicting each respective potential outcome, and show how this can be used to derive generalization bounds for CATE.A large body of work has shown that under assumptions of ignorability and having a wellspecified model, various regression methods for counterfactual estimation are asymptotically consistent (Chernozhukov et al., 2017; Athey & Imbens, 2016; Belloni et al., 2014). However, consistency results like these provide little insight into the case of model misspecification. Under model misspecification, regression methods may suffer from additional bias when generalizing across designs due to distributional shift. A common way to alleviate this is importance sampling, see Section 2. This idea is used in propensity-score methods (Austin, 2011), that use treatment assignment probabilities (propensities) to re-weight samples for causal effect estimation, and more generally in re-weighted regression, see e.g. (Swaminathan & Joachims, 2015). A major drawback of these methods is the assumption that the design density is known. To address this, others (Gretton et al., 2009; Kallus, 2016), have proposed learning sample weights w to minimize the distributional distance between samples under pπ and pwµ , but rely on specifying the data representation a priori, without regard for which aspects of the data actually matter for outcome prediction and policy estimation. On the other hand, Johansson et al. (2016); Shalit et al. (2017) proposed learning representations for counterfactual inference, inspired by work in unsupervised domain adaptation (Mansour et al., 2009). The drawback of this line of work is that the generalization bounds of Shalit et al. (2017) and Long et al. (2015) are loose—even if infinite samples are available, they are not guaranteed to converge to the lowest possible error. Moreover, these approaches do not make use of important information that can be estimated from data: the treatment/domain assignment probabilities.We give a bound on the risk in predicting outcomes under a target design pπ(T,X) based on unlabeled samples from pπ and labeled samples from a source design pµ(T,X). Our result combines representation learning, distribution matching and re-weighting, resulting in a tighter bound 2Notions of causal effects exist also for the non-binary case, but these are not considered here. 3This is sufficient but not necessary. than the closest related work, Shalit et al. (2017). The predictors we consider are compositions f(x, t) = h(Φ(x), t) where Φ is a representation of x and h an hypothesis. We first give an upper bound on the risk in the general design shift setting, then show how this result can be used to bound the error in prediction of treatment effects. In Section 5 we give a result about the asymptotic properties of the minimizers of this upper bound. Risk under distributional shift Our bounds on the risk under a target design capture the intuition that if either a) the target design π and source design µ are close, or b) the true outcome is a simple function of x and t, the gap between the target risk and the re-weighted source risk is small. These notions can be formalized using integral probability metrics (IPM) (Sriperumbudur et al., 2009) that measure distance between distributions w.r.t. a normed vector space of functionsH. Definition 2. The integral probability metric (IPM) distance, associated with a normed vector space of functionsH, between distributions p and q is, IPMH(p, q) := suph∈H |Ep[h]− Eq[h]|. Important examples of IPMs include the Wasserstein distance, for whichH is the family of functions with Lipschitz constant at most 1, and the Maximum Mean Discrepancy for which H are functions in the norm-1 ball in a reproducing kernel Hilbert space. Using definitions 1–2, and the definition of re-weighted risk, see (2), we can state the following result (see Appendix A.2 for a proof). Lemma 1. For hypotheses f with loss `f such that `f/‖`f‖H ∈ H, and pµ, pπ with common support, there exists a valid re-weighting w of pµ, see Definition 1, such that, Rπ(f) ≤ Rwµ (f) + ‖`f‖HIPMH(pπ, pwµ ) ≤ Rµ(f) + ‖`f‖HIPMH(pπ, pµ) . (5) The first inequality is tight for importance sampling weights, w(x, t) = pπ(x, t)/pµ(x, t). The second inequality is not tight for general f , even if `f/‖`f‖H ∈ H, unless pπ = pµ. The bound of Lemma 1 is tighter if pµ and pπ are close (the IPM is smaller), and if the loss lives in a small family of functions H (the supremum is taken over a smaller set). Lemma 1 also implies that there exist weighting functions w(x, t) that achieve a tighter bound than the uniform weighting w(x, t) = 1, implicitly used by Shalit et al. (2017). While importance sampling weights result in a tight bound in expectation, neither the design densities nor their ratio are known in general. Moreover, exact importance weights often result in large variance in finite samples (Cortes et al., 2010). Here, we will search for a weighting function w, that minimizes a finite-sample version of (5), trading off bias and variance. We examine the empirical value of this idea alone in Section 6.1. We now introduce the notion of representation learning to combat distributional shift. Representation learning The idea of learning representations that reduce distributional shift in the induced space, and thus the source-target error gap, has been applied in domain adaptation (Ajakan et al., 2014), algorithmic fairness (Zemel et al., 2013) and counterfactual prediction (Shalit et al., 2017). The hope of these approaches is to learn predictors that predominantly exploit information that is common to both source and target distributions. For example, a face detector should be able to recognize the structure of human features even under highly variable environment conditions, by ignoring background, lighting etc. We argue that re-weighting (e.g. importance sampling) should also only be done with respect to features that are predictive of the outcome. Hence, in Section 5, we propose using re-weightings that are functions of learned representations. We follow the setup of Shalit et al. (2017), and consider learning twice-differentiable, invertible representations Φ : X → Z , where Z is the representation space, and Ψ : Z → X is the inverse representation, such that Ψ(Φ(x)) = x for all x. Let E denote space of such representation functions. For a design π, we let pπ,Φ(z, t) be the distribution induced by Φ over Z × T , with pwπ,Φ(z, t) := pπ,Φ(z, t)w(Ψ(z), t) its re-weighted form and p̂wπ,Φ its re-weighted empirical form, following our previous notation. Finally, we let G ⊆ {h : Z × T → Y} denote a set of hypotheses h(Φ, t) operating on the representation Φ and let F the space of all compositions, F = {f = h(Φ(x), t) : h ∈ G,Φ ∈ E}. We can now relate the expected target risk Rπ(f) to the re-weighted empirical source risk R̂wµ (f). Theorem 1. Given is a labeled sample (x1, t1, y1), ..., (xn, tn, yn) from pµ, and an unlabeled sample (x′1, t ′ 1), ..., (x ′ m, t ′ m) from pπ , with corresponding empirical measures p̂µ and p̂π . Suppose that Φ is a twice-differentiable, invertible representation, that h(Φ, t) is an hypothesis, and f = h(Φ(x), t) ∈ F . Define mt(x) = EY [Y | X = x, T = t], let `h,Φ(Ψ(z), t) := L(h(z, t),mt(Ψ(z))) where L is the squared loss, L(y, y′) = (y − y′)2, and assume that there exists a constant BΦ > 0 such that `h,Φ/BΦ ∈ H ⊆ {h : Z × T → Y}, where H is a reproducing kernel Hilbert space of a kernel, k such that k((z, t), (z, t)) < ∞. Finally, let w be a valid re-weighting of pµ,Φ. Then with probability at least 1− 2δ, Rπ(f) ≤ R̂wµ (f) +BΦIPMH(p̂π,Φ, p̂wµ,Φ) + Vµ(w, `f ) CFn,δ n3/8 +DΦ,Hδ ( 1√ m + 1√ n ) + σ2Y (6) where CFn,δ is a function of the pseudo-dimension of F , DHm,n,δ a function of the kernel norm of H, both only with logarithmic dependence on n and m, σ2Y is the expected variance in Y , and Vµ(w, `f ) = max (√ Epµ [w2(x, t)`2f (x, t)], √ Ep̂µ [w2(x, t)`2f (x, t)] ) . A similar bound exists where H is the family of functions Lipschitz constant at most 1, and IPMH the Wasserstein distance, but with worse sample complexity. See Appendix A.2 for a proof of Theorem 1 that involves applying finite-sample generalization bounds to Lemma 1, as well as moving to the space induced by the representation Φ. Theorem 1 has several implications: non-identity feature representations, non-uniform sample weights, and variance control of these weights can all contribute to a lower bound. Using uniform weights w(x, t) = 1 in (6), results in a bound similar to that of Shalit et al. (2017) and Long et al. (2015). When π 6= µ, minimizing uniform-weight bounds results in biased hypotheses, even in the asymptotical limit, as the IPM term does not vanish when the sample size increases. This is an undesirable property, as even k-nearest-neighbor classifiers are consistent in the limit of infinite samples. We consider minimizing (6) with respect to w, improving the tightness of the bound. Theorem 1 indicates that even though importance sampling weights w∗ yield estimators with small bias, they can suffer from high variance, as captured by the factor Vµ(w, `f ). The factor BΦ is not known in general as it depends on the true outcome, and is determined by ‖`f‖H as well as the determinant of the Jacobian of Ψ, see Appendix A.2. Qualitatively, BΦ measures the joint complexity of Φ and `f and is sensitive to the scale of Φ—as the scale of Φ vanishes, BΦ blows up. To prevent this in practice, we normalize Φ. As BΦ is unknown, Shalit et al. (2017) substituted a hyperparameter α for BΦ, but discussed the difficulties of selecting its value without access to counterfactual labels. In our experiments, we explore a heuristic for adaptively choosing α, based on measures of complexity of the observed held-out loss as a function of the input. Finally, the term CFn,δ follows from standard learning theory results (Cortes et al., 2010) and F , and DΦ,Hδ from concentration results for estimating IPMs (Sriperumbudur et al., 2012), see Appendix A.2. Theorem 1 is immediately applicable to the case of unsupervised domain adaptation in which there is only a single potential outcome of interest, T = {0}. In this case, pµ(T | X) = pπ(T | X) = 1. Conditional average treatment effects A simple argument shows that the error in predicting the conditional average treatment effect, MSE(τ̂) can be bounded by the sum of risks under the constant treat-all and treat-none policies. As in Section 2.2, we consider the case of a fixed domain pπ(X) = pµ(X) and binary treatment T = {0, 1}. Let Rπt(f) denote the risk under the constant policy πt such that ∀x ∈ X : pπt(T = t | X = x) = 1. Proposition 1. We have with MSE(τ̂) as in (4) and Rπt(f) the risk under the constant policy πt, MSE(τ̂) ≤ 2(Rπ1(f) +Rπ0(f))− 4σ2 (7) where σ is such that ∀t ∈ T , x ∈ X , σY (x, t) ≥ σ and σY (x, t) is standard deviation of Y (t) conditioned on X = x. The proof involves the relaxed triangle inequality and the law of total probability. By Proposition 1, we can apply Theorem 1 to Rπ1 and Rπ0 separately, to obtain a bound on MSE(τ). For brevity, we refrain from stating the full result, but emphasize that it follows from Theorem 1. In Section 6.2, we evaluate our framework in treatment effect estimation, minimizing this bound.Motivated by the theoretical insights of Section 4, we propose to jointly learn a representation Φ(x), a re-weighting w(x, t) and an hypothesis h(Φ, t) by minimizing a bound on the risk under the target design, see (6). This approach improves on previous work in two ways: it alleviates the bias of Shalit et al. (2017) when sample sizes are large, see Section 4, and it increases the flexibility of the balancing method of Gretton et al. (2009) by learning the representation to balance. For notational brevity, we let wi = w(xi, ti). Recall that p̂wπ,Φ is the re-weighted empirical distribution of representations Φ under pπ . The training objective of our algorithm is the RHS of (6), with hyperparameters β = (α, λh, λw) substituted for model (and representation) complexity terms, Lπ(h,Φ, w;β) = 1 n n∑ i=1 wi`h(Φ(xi), ti) + λh√ n R(h)︸ ︷︷ ︸ Lhπ(h,Φ,w;D,α,λh) +α IPMG(p̂π,Φ, p̂wµ,Φ) + λw ‖w‖2 n︸ ︷︷ ︸ Lwπ (Φ,w;D,α,λw) (8) whereR(h) is a regularizer of h, such as `2-regularization. We can show the following result. Theorem 2. Suppose H is a reproducing kernel Hilbert space given by a bounded kernel. Suppose weak overlap holds in that E[(pπ(x, t)/pµ(x, t))2] <∞. Then, min h,Φ,w Lπ(h,Φ, w;β) ≤ min f∈F Rπ(f) +Op(1/ √ n+ 1/ √ m) . Consequently, under the assumptions of Thm. 1, for sufficiently large α and λw, Rπ(f̂n) ≤ min f∈F Rπ(f) +Op(1/n 3/8 + 1/ √ m). In words, the minimizers of (8) converge to the representation and hypothesis that minimize the counterfactual risk, in the limit of infinite samples. Implementation Minimization of Lπ(h,Φ, w;β) over h,Φ and w is, while motivated by Theorem 2, a difficult optimization problem to solve in practice. For example, adjusting w to minimize the empirical risk term may result in overemphasizing “easy” training examples, resulting in a poor local minimum. Perhaps more importantly, ensuring invertibility of Φ is challenging for many representation learning frameworks, such as deep neural networks. In our implementation, we deviate from theory on these points, by fitting the re-weighting w based only on imbalance and variance terms, and don’t explicitly enforce invertibility. As a heuristic, we split the objective, see (8), in two and use only the IPM term and regularizer to learn w. In short, we adopt the following alternating procedure. hk,Φk = arg min h,Φ Lhπ(h,Φ, w;D,α, λh), wk+1 = arg min w Lwπ (Φk, w;D,α, λw) (9) The re-weighting function w(x, t) could be represented by one free parameter per training point, as it is only used to learn the model, not for prediction. However, we propose to let w be a parametric function of Φ(x). Doing so ensures that information predictive of the outcome is used for balancing, and lets us compute weights on a hold-out set, to perform early stopping or select hyperparameters. This is not possible with existing re-weighting methods such as Gretton et al. (2009); Kallus (2016). An example architecture for the treatment effect estimation setting is presented in Figure 1. By Proposition 1, estimating treatment effects involves predicting under the two constant policies— treat-everyone and treat-no-one. In Section 6, we evaluate our method in this task. As noted by Shalit et al. (2017), choosing hyperparameters for counterfactual prediction is fundamentally difficult, as we cannot observe ground truth for counterfactuals. In this work, we explore setting the balance parameter α adaptively. α is used in (8) in place of BΦ, a factor measuring the complexity of the loss and representation function as functions of the input, a quantity that changes during training. As a heuristic, we use an approximation of the Lipschitz constant of `f , with f = h(Φ(x), t), based on observed examples: αh,Φ = maxi,j∈[n] |`f (xi, ti, yi) − `f (xj , tj , yj)|/‖xi − xj‖2. We use a moving average over batches to improve stability.We create a synthetic domain adaptation experiment to highlight the benefit of using a learned reweighting function to minimize weighted risk over using importance sampling weights w∗(x) = pπ(x)/pµ(x) for small sample sizes. We observe n labeled source samples, distributed according to pµ(x) = N (x;mµ, Id) and predict for n unlabeled target samples drawn according to pπ(x) = N (x;mπ, Id) where Id is the d-dimensional identity matrix, mµ = b1d/2, mπ = −b1d/2 and 1d is the d-dimensional vector of all 1:s, here with b = 1, d = 10. We let β ∼ N (0d, 1.5Id) and c ∼ N (0, 1) and let y = σ(β>x + c) where σ(z) = 1/(1 + e−z). Importance sampling weights, w∗(x) = pπ(x)/pµ(x), are known. In experiments, we vary n from 10 to 600. We fit (misspecified) linear models4 f(x) = β>x + γ to the logistic outcome, and compare minimizing a weighted source risk by a) parameterizing sample weights as a small feed-forward neural network to minimize (8) (ours) b) using importance sampling weights (baseline), both using gradient descent. For our method, we add a small variance penalty, λw = 10−3, to the learned weights, use MMD with an RBF-kernel of σ = 1.0 as IPM, and let α = 10. We compare to exact importance sampling weights (IS) as well as clipped IS weights (ISC), wM (x) = min(w(x),M) for M ∈ {5, 10}, a common way of reducing variance of re-weighting methods (Swaminathan & Joachims, 2015). In Figure 2a, we see that our proposed method behaves well at small sample sizes compared to importance sampling methods. The poor performance of exact IS weights is expected at smaller samples, as single samples are given very large weight, resulting in hypotheses that are highly sensitive to the training set. While clipped weights alleviates this issue, they do not preserve relevance ordering of high-weight samples, as many are given the truncation value M , in contrast to the reweighting learned by our method. True domain densities are known only to IS methods.We evaluate our framework in the CATE estimation setting, see Section 2.2. Our task is to predict the expected difference between potential outcomes conditioned on pre-treatment variables, for a held-out sample of the population. We compare our results to ordinary least squares (OLS) (with one regressor per outcome), OLS-IPW (re-weighted OLS according to a logistic regression estimate of propensities), Random Forests, Causal Forests (Wager & Athey, 2017), BART (Chipman et al., 2010), and CFRW (Shalit et al., 2017) (with Wasserstein penalty). Finally, we use as baseline (IPMWNN): first weights are found by IPM minimization in the input space (Gretton et al., 2009; Kallus, 2016), then used in a re-weighted neural net regression, with the same architecture as our method. Our implementation, dubbed RCFR for Re-weighted CounterFactual Regression, parameterizes representations Φ(x), weighting functions w(Φ, t) and hypotheses h(Φ, t) using neural networks, trained by minimizing (8). We use the RBF-kernel maximum mean discrepancy as the IPM (Gretton et al., 2012). For a description of the architecture, training procedure and hyperparameters, see Appendix B. We compare results using uniform w = 1 and learned weights, setting the balance parameter α either fixed, by an oracle (test-set error), or adaptively using the heuristic described in Section 5. To pick other hyperparameters, we split training sets into one part used for function fitting and one used for early stopping and hyperparameter selection. Hyperparameters for regularization are chosen based on the empirical loss on a held-out source (factual) sample. 4The identity representation Φ(x) = x is used for both approaches. The Infant Health and Development Program (IHDP) dataset is a semi-synthetic binary-treatment benchmark (Hill, 2011), split into training and test sets by Shalit et al. (2017). IHDP has a set of d = 25 real-world continuous and binary features describing n = 747 children and their mothers, a real-world binary treatment made non-randomized through biased subsampling by Hill (2011), and a synthesized continuous outcome that can be used to compute the ground-truth CATE error. Average results over 100 different realizations/settings of the outcome are presented in Table 1. We see that our proposed method achieves state-of-the-art results, and that adaptively choosing α does not hurt performance much. Furthermore, we see a substantial improvement from using nonuniform sample weights. In Figure 2b we take a closer look at the behavior of our model as we vary its hyperparameters on the IHDP dataset. Between the two plots we can draw the following conclusions: a) For moderate to large α ∈ [10, 100], we observe a marginal gain from using the IPM penalty. This is consistent with the observations of Shalit et al. (2017). b) For large α ∈ [10, 1000], we see a large gain from using a non-uniform re-weighting (small λw). c) While large α makes the factual error more representative of the counterfactual error, using it without re-weighting results in higher absolute error. We believe that the moderate sample size of this dataset is one of the reasons for the usefulness of our method. See Appendix C.2 for a complementary view of these results.We have proposed a theory and an algorithmic framework for learning to predict outcomes of interventions under shifts in design—changes in both intervention policy and feature domain. The framework combines representation learning and sample re-weighting to balance source and target designs, emphasizing information from the source sample relevant for the target. Existing reweighting methods either use pre-defined weights or learn weights based on a measure of distributional distance in the input space. These approaches are highly sensitive to the choice of metric used to measure balance, as the input may be high-dimensional and contain information that is not predictive of the outcome. In contrast, by learning weights to achieve balance in representation space, we base our re-weighting only on information that is predictive of the outcome. In this work, we apply this framework to causal effect estimation, but emphasize that joint representation learning and re-weighting is a general idea that could be applied in many applications with design shift. Our work suggests that distributional shift should be measured and adjusted for in a representation space relevant to the task at hand. Joint learning of this space and the associated re-weighting is attractive, but several challenges remain, including optimization of the full objective and relaxing the invertibility constraint on representations. For example, variable selection methods are not covered by our current theory, as they induce a non-ivertible representation, but a similar intuition holds there—only predictive attributes should be used when measuring imbalance. We believe that addressing these limitations is a fruitful path forward for future work.Definition 1 (Restated). A function w : X × T → R+ is a valid re-weighting of pµ if Ex,t∼pµ [w(x, t)] = 1 and pµ(x, t) > 0⇒ w(x, t) > 0. We denote the re-weighted density pwµ (x, t) := w(x, t)pµ(x, t). Expected & empirical risk We let the (expected) risk of f measured by `h under pµ be denoted Rµ(h) = Epµ [lh(x, t)] where lh is an appropriate loss function, and the empirical risk over a sample Dµ = {(x1, t1, y1)..., (xn, tn, yn) from pµ R̂µ(f) = 1 n n∑ i=1 lf (xi, ti, yi) . We use the superscript w to denote the re-weighted risks Rwµ (f) = E[w(x, t)lf (x, t)] R̂wµ (f) = 1 n n∑ i=1 w(xi, ti)lh(xi, ti, yi) Definition A1 (Importance sampling). For two distributions p, q on Z , of common support, ∀z ∈ Z : p(z) > 0 ⇐⇒ q(z) > 0, we call wIS(z) := q(z) p(z) the importance sampling weights of p and q. Definition 2 (Restated). The integral probability metric (IPM) distance, associated with the function familyH, between distributions p and q is defined by IPMH(p, q) := sup h:‖h‖H=1 |Ep[h]− Eq[h]|We begin by bounding the expected risk under a distribution pπ in terms of the expected risk under pµ and a measure of the discrepancy between pπ and pµ. Using definition 2 we can show the following result. Lemma 1 (Restated). For hypotheses f with loss `f such that `f/‖`f‖H ∈ H, and pµ, pπ with common support, there exists a valid re-weighting w of pµ, see Definition 1, such that, Rπ(f) ≤ Rwµ (f) + ‖`f‖HIPMH(pπ, pwµ ) ≤ Rµ(f) + ‖`f‖HIPMH(pπ, pµ) . (10) The first inequality is tight for importance sampling weights, w(x, t) = pπ(x, t)/pµ(x, t). The second inequality is not tight for general f , even if `f ∈ H, unless pπ = pµ. Proof. The results follows immediately from the definition of IPM. Rπ(f)−Rwµ (f) = Eπ[`f (x, t)]− Eµ[w(x, t)`f (x, t)] ≤ sup h∈H` |Eπ[h(x, t)]− Eµ[w(x, t)h(x, t)]| = IPMH`(pπ, p w µ ) Further, for importance sampling weights wIS(x, t) = π(t;x) µ(t;x) , for any h ∈ H, Eπ[h(x, t)]− Eµ[wIS(x, t)h(x, t)] = Eπ[h(x, t)]− Eµ[ π(t;x) µ(t;x) h(x, t)] = 0 and the LHS is tight. We could apply Lemma 1 to bound the loss under a distribution q based on the weighted loss under p. Unfortunately, bounding the expected risk in terms of another expectation is not enough to reason about generalization from an empirical sample. To do that we use Corollary 2 of Cortes et al. (2010), restated as a Theorem below. Theorem A1 (Generalization error of re-weighted loss (Cortes et al., 2010)). For a loss function `h of any hypothesis h ∈ H ⊆ {h′ : X → R}, such that d = Pdim({`h : h ∈ H}) where Pdim is the pseudo-dimension, and a weighting function w(x) such that Ep[w] = 1, with probability 1− δ over a sample (x1, ..., xn), with empirical distribution p̂, Rwp (h) ≤ R̂wp (h) + 25/4Vp,p̂[w(x)l(x)] ( d log 2ned + log 4 δ n )3/8 with Vp,p̂[w(x)l(x)] = max( √ Ep[w2(x)`2h(x)], √ Ep̂[w2(x)`2h(x)]). With CHn = 25/4 ( d log 2ne d + log 4 δ )3/8 we get the simpler form Rwp (h) ≤ R̂wp (h) + Vp,p̂[w(x)l(x)] CHn n3/8 . We will also need the following result about estimating IPMs from finite samples from Sriperumbudur et al. (2009). Theorem A2 (Estimation of IPMs from empirical samples (Sriperumbudur et al., 2009)). Let M be a measurable space. Suppose k is measurable kernel such that supx∈M k(x, x) ≤ C ≤ ∞ and H the reproducing kernel Hilbert space induced by k, with ν := supx∈M,f∈H f(x) < ∞. Then, with p̂, q̂ the empirical distributions of p, q from m and n samples respectively, and with probability at least 1− δ, |IPMH(p, q)− IPMH(p̂, q̂)| ≤ √ 18ν2 log 4 δ C ( 1√ m + 1√ n ) We consider learning twice-differentiable, invertible representations Φ : X → Z , where Z is the representation space, and Ψ : Z → X is the inverse representation, such that Ψ(Φ(x)) = x for all x. Let E denote space of such representation functions. For a design π, we let pπ,Φ(z, t) be the distribution induced by Φ over Z × T , with pwπ,Φ(z, t) := pπ,Φ(z, t)w(Ψ(z), t) its re-weighted form and p̂wπ,Φ its re-weighted empirical form, following our previous notation. Note that we do not include t in the representation itself, although this could be done in principle. Let G ⊆ {h : Z ×T → Y} denote a set of hypotheses h(Φ, t) operating on the representation Φ and let F denote the space of all compositions, F = {f = h(Φ(x), t) : h ∈ G,Φ ∈ E}. We now restate and prove Theorem 1. Theorem 1 (Restated). Given is a labeled sample Dµ = {(x1, t1, y1), ..., (xn, tn, yn)} from pµ, and an unlabeled sample Dπ = {(x′1, t′1), ..., (x′m, t′m)} from pπ , with corresponding empirical measures p̂µ and p̂π . Suppose that Φ is a twice-differentiable, invertible representation, that h(Φ, t) is an hypothesis, and f = h(Φ(x), t) ∈ F . Define mt(x) = EY [Y | X = x, T = t], let `h,Φ(Ψ(z), t) := L(h(z, t),mt(Ψ(z))) where L is the squared loss, L(y, y′) = (y − y′)2, and assume that there exists a constant BΦ > 0 such that `h,Φ/BΦ ∈ H ⊆ {h : Z × T → Y}, where H is a reproducing kernel Hilbert space of a kernel, k such that k((z, t), (z, t)) <∞. Finally, let w be a valid re-weighting of pµ,Φ. Then with probability at least 1− 2δ, Rπ(f) ≤ R̂wµ (f) +BΦIPMH(p̂π,Φ, p̂wµ,Φ) + Vµ(w, `f ) CFn,δ n3/8 +DΦ,Hδ ( 1√ m + 1√ n ) + σ2Y (11) where CFn,δ measures the capacity ofF and has only logarithmic dependence on n,DHm,n,δ measures the capacity ofH, σ2Y is the expected variance in potential outcomes, and Vµ(w, `f ) = max( √ Epµ [w2(x, t)`2f (x, t)], √ Ep̂µ [w2(x, t)`2f (x, t)]) . A similar bound exists whereH is the family of functions Lipschitz constant at most 1, but with worse sample complexity. Proof. We have by definition Rπ(f)−Rwµ (f) = Eπ[`f (x, t, y)]− Eµ[w(x, t)`f (x, t, y)] = ∫ x,t,y `f (x, t, y)p(y | t, x)(pπ(x, t)− pwµ (x, t))dxdtdy Define `h,Φ(x, t) = L(h(Φ(x), t),mt(x)) where mt(x) := E[Y | T = t,X = x]). Then, with L, the squared loss, L(y, y′) = (y − y′)2, we have, Eπ[`h,Φ(x, t, y)] = Eπ[`h,Φ(x, t)] + σ2π where σ2π = Epπ [(Y −mt(x))2], and analogously for µ. We get that Rπ(f)−Rwµ (f) = ∫ x∈X ,t∈T `h,Φ(x, t)(pπ(x, t)− pwµ (x, t))dxdt+ σ2π + σ2µ = ∫ z∈Z,t∈T `h,Φ(Ψ(z), t)(pπ,Φ(z, t)− pwµ,Φ(z, t))|JΨ(z)|dzdt+ σ2π + σ2µ ≤ AΦ ∫ z∈Z,t∈T `h,Φ(Ψ(z), t)(pπ(z, t)− pwµ (z, t))dzdt+ σ2π + σ2µ ≤ σ2π + σ2µ +AΦ‖`h,Φ‖H sup h∈H ∣∣∣∣∣ ∫ z∈Z t∈T h(Ψ(z), t) ( pπ,Φ(rz, t)− pwµ,Φ(z, t) ) dzdt ∣∣∣∣∣ = BΦ · IPMH(pπ,Φ, pwµ,Φ) + σ2π + σ2µ where JΨ(z) is the Jacobian matrix of Ψ evaluated at z and AΦ ≥ |JΨ(z)| for all z ∈ Z , where |J | is the absolute determinant of J . By application of Theorem A1 we have with probability at least 1− δ, Rwµ (f) ≤ R̂wµ (f) + Vµ(w, `) CHn,δ n3/8 . and by applying Theorem A2, we have with probability at least 1− δ, ∣∣IPMH(pπ,Φ, pwµ,Φ)− IPMH(p̂π,Φ, p̂wµ,Φ)∣∣ ≤√18ν2 log 4δC ( 1√ m + 1√ n ) We let σ2Y = σ 2 π + σ 2 µ and DΦ,Hδ := BΦ √ 18ν2 log 4 δ C Combining these results, observing that (1− δ)2 ≥ 1− 2δ, we obtain the desired result.Theorem 2 (Restated). SupposeH is a reproducing kernel Hilbert space given by a bounded kernel. Suppose weak overlap holds in that E[(pπ(x, t)/pµ(x, t))2] <∞. Then, min h,Φ,w Lπ(h,Φ, w;β)] ≤ min f∈F Rπ(f) +O(1/ √ n+ 1/ √ m) . Proof. Let f∗ = Φ∗ ◦ h∗ ∈ arg minf∈F Rπ(f) and let w∗(x, t) = pπ,Φ(Φ∗(x), t)/pµ,Φ(Φ∗(x), t). Since minh,Φ,w Lπ(h,Φ, w;β) ≤ Lπ(h∗,Φ∗, w∗;β), it suffices to show that Lπ(h∗,Φ∗, w∗;β) = Rπ(f ∗) +O(1/ √ n+ 1/ √ m). We will work term by term: Lπ(h∗,Φ∗, w∗;β) = 1 n n∑ i=1 wi`h(Φ(xi), ti)︸ ︷︷ ︸ A +λh R(h)√ n︸ ︷︷ ︸ B +α IPMG(q̂Φ, p̂w k Φ )︸ ︷︷ ︸ C +λw ‖w‖2 n︸ ︷︷ ︸ D . For term D , letting w∗i = w ∗(xi, ti), we have that by weak overlap D 2 = 1 n × 1 n n∑ i=1 (w∗i ) 2 = Op(1/n), so that D = Op(1/ √ n). For term A , under ignorability, each term in the sum in the first term has expectation equal to Rπ(f∗) and so, so by weak overlap and bounded second moments of loss, we have A = Rπ(f∗) +Op(1/ √ n). For term B , since h∗ is fixed we have deterministically that B = O(1/ √ n). Finally, we address term C , which when expanded can be written as sup ‖h‖≤1 ( 1 m m∑ i=1 h(Φ∗(x′i), t ′ i)− 1 n n∑ i=1 w∗i h(Φ ∗(xi), ti)). Let x′′i , t ′′ i for i = 1, . . . ,m and x ′′′ i , t ′′′ i for i = 1, . . . , n be new iid replicates of x ′ 1, t ′ 1, i.e., new ghost samples drawn from the target design. By Jensen’s inequality, E[ C 2] = E[ sup ‖h‖≤1 ( 1 m m∑ i=1 h(Φ∗(x′i), t ′ i)− 1 n n∑ i=1 w∗i h(Φ ∗(xi), ti)) 2] = E[ sup ‖h‖≤1 ( 1 m m∑ i=1 (h(Φ∗(x′i), t ′ i)− E[h(Φ∗(x′′i ), t′′i )]) − 1 n n∑ i=1 (w∗i h(Φ ∗(xi), ti)− E[h(Φ∗(x′′′i ), t′′′i )]))2] ≤ E[ sup ‖h‖≤1 ( 1 m m∑ i=1 (h(Φ∗(x′i), t ′ i)− h(Φ∗(x′′i ), t′′i )) − 1 n n∑ i=1 (w∗i h(Φ ∗(xi), ti)− h(Φ∗(x′′′i ), t′′′i )))2] ≤ 2E[ sup ‖h‖≤1 ( 1 m m∑ i=1 (h(Φ∗(x′i), t ′ i)− h(Φ∗(x′′i ), t′′i )))2] + 2E[ sup ‖h‖≤1 ( 1 n n∑ i=1 (w∗i h(Φ ∗(xi), ti)− h(Φ∗(x′′′i ), t′′′i )))2] Let ξi(h) = h(Φ∗(x′i), t ′ i) − h(Φ∗(X ′qi ) and let ζi(h) = w∗i h(Φ∗(xi), ti) − h(Φ∗(x′′′i ), t′′′i ). Note that for every h, E[ζi(h)] = E[ξi(h)] = 0. Moreover, E[‖ζi‖2] ≤ 4E[K(Φ∗(x′i), t′i,Φ∗(x′i), t′i)] ≤ M . Similarly, E[‖ξi‖2] ≤ 2E[(w∗i )2]M + 2M ≤ M ′ < ∞ because of weak overlap. Let ζ ′i for i = 1, . . . , n be iid replicates of ζi (ghost sample) and let i be iid Rademacher random variables. Because H is a Hilbert space, we have that sup‖h‖≤1(A(h))2 = ‖A‖2 = 〈A,A〉. Therefore, by Jensen’s inequality, E[ sup ‖h‖≤1 ( 1 n n∑ i=1 (w∗i h(Φ ∗(xi), ti)− h(Φ∗(x′′′i ), t′′′i )))2] = E[ sup ‖h‖≤1 ( 1 n n∑ i=1 ζi(h)) 2] = E[ sup ‖h‖≤1 ( 1 n n∑ i=1 (ζi(h)− E[ζ ′i(h)]))2] ≤ E[ sup ‖h‖≤1 ( 1 n n∑ i=1 (ζi(h)− ζ ′i(h)))2] = E[ sup ‖h‖≤1 ( 1 n n∑ i=1 i(ζi(h)− ζ ′i(h)))2] ≤ 4 n2 E[ sup ‖h‖≤1 ( n∑ i=1 iζi(h)) 2] = 4 n2 E[‖ n∑ i=1 iζi‖2] = 4 n2 E[ n∑ i,j=1 i j 〈ζi, ζj〉] = 4 n2 E[ n∑ i=1 ‖ζi‖2] = 4 n2 n∑ i=1 E[‖ζi‖2] ≤ 4M ′ n An analogous argument can be made of ξi’s, showing that E[ C 2 ] = O(1/n) and hence C = O(1/ √ n) by Markov’s inequality. B IMPLEMENTATION We implemented all neural network models (IPM-WNN, RCFR) in TensorFlow as feed-forward fully-connected networks with ELU activations. All architectures have a representation with two hidden layers of 32 and 16 hidden units, and hypotheses (one for each outcome) of 1 layer of 16 hidden units. The networks were trained using stochastic gradient descent with the ADAM optimizer with a learning rate of 10−3. The batch size was 128. Representations were normalized by dividing by the norm. Weight functions were implemented as 2 hidden layers of 32 units each, as functions of the representation Φ. σ in the RBF kernel was set to 1.0. λw was set to 0.1 and λh to 10−4.We use a two-layer MLP with ELU units and layer sizes 10, 10 as parameterization of the sample weights. Weights are normalized by dividing by the mean. C.2 IHDP In Figure 3, we see two different views of the IHDP results.
Retrosynthesis planning is the procedure of identifying a series of reactions that lead to the synthesis of target product. It is first formalized by E. J. Corey [1] and now becomes one of the fundamental problems in organic chemistry. Such problem of “working backwards from the target” is challenging, due to the size of the search space–the vast numbers of theoretically-possible transformations–and thus requires the skill and creativity from experienced chemists. Recently, various computer algorithms [2] work in assistance to experienced chemists and save them tremendous time and effort. The simplest formulation of retrosynthesis is to take the target product as input and predict possible reactants 1. It is essentially the “reverse problem” of reaction prediction. In reaction prediction, the reactants (sometimes reagents as well) are given as the input and the desired outputs are possible products. In this case, atoms of desired products are the subset of reactants atoms, since the side products are often ignored (see Fig 1). Thus models are essentially designed to identify this subset in reactant atoms and reassemble them to be the product. This can be treated as a deductive reasoning process. In sharp contrast, retrosynthesis is to identify the superset of atoms in target products, and thus is an abductive reasoning process and requires “creativity” to be solved, making it a harder problem. Although recent advances in graph neural networks have led to superior performance in reaction prediction [3, 4, 5], such advances do not transfer to retrosynthesis. Computer-aided retrosynthesis designs have been deployed over the past years since [6]. Some of them are completely rule-based systems [7] and do not scale well due to high computation cost and ⇤Work done while Hanjun was at Georgia Institute of Technology 1We will focus on this “single step” version of retrosynthesis in our paper. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. incomplete coverage of the rules, especially when rules are expert-defined and not algorithmically extracted [2]. Despite these limitations, they are very useful for encoding chemical transformations and easy to interpret. Based on this, the retrosim [8] uses molecule and reaction fingerprint similarities to select the rules to apply for retrosynthesis. Other approaches have used neural classification models for this selection task [9]. On the other hand, recently there have also been attempts to use the sequence-to-sequence model to directly predict SMILES 2 representation of reactants [10, 11] (and for the forward prediction problem, products [12, 13]). Albeit simple and expressive, these approaches ignore the rich chemistry knowledge and thus require huge amount of training. Also such models lack interpretable reasoning behind their predictions. The current landscape of computer-aided synthesis planning motivated us to pursue an algorithm that shares the interpretability of template-based methods while taking advantage of the scalability and expressiveness of neural networks to learn when such rules apply. In this paper, we propose Conditional Graph Logic Network towards this direction, where chemistry knowledge about reaction templates are treated as logic rules and a conditional graphical model is introduced to tolerate the noise in these rules. In this model, the variables are molecules while the synthetic relationships to be inferred are defined among groups of molecules. Furthermore, to handle the potentially infinite number of possible molecule entities, we exploit the neural graph embedding in this model. Our contribution can be summarized as follows: 1) We propose a new graphical model for the challenging retrosynthesis task. Our model brings both the benefit of the capacity from neural embeddings, and the interpretability from tight integration of probabilistic models and chemical rules. 2) We propose an efficient hierarchical sampling method for approximate learning by exploiting the structure of rules. Such algorithm not only makes the training feasible, but also provides interpretations for predictions. 3) Experiments on the benchmark datasets show a significant 8.1% improvement over existing state-of-the-art methods in top-one accuracy. Other related work: Recently there have been works using machine learning to enhance the rule systems. Most of them treat the rule selection as multi-class classification [9] or hierarchical classification [14] where similar rules are grouped into subcategories. One potential issue is that the model size grows with the number of rules. Our work directly models the conditional joint probability of both rules and the reactants using embeddings, where the model size is invariant to the rules. On the other hand, researchers have tried to tackle the even harder problem of multi-step retrosynthesis [15, 16] using single-step retrosynthesis as a subroutine. So our improvement in single-step retrosynthesis could directly transfer into improvement of multi-step retrosynthesis [8].A chemical reaction can be seen as a transformation from set of N reactant molecules {Ri}Ni=1 to an outcome molecule O. Without loss of generality, we work with single-outcome reactions in this paper, as this is a standard formulation of the retrosynthetic problem and multi-outcome reactions can be split into multiple single-outcome ones. We refer to the set of atoms changed (e.g., bond being added or deleted) during the reaction as reaction centers. Given a reaction, the corresponding 2https://www.daylight.com/dayhtml/doc/theory/theory.smiles.html. retrosynthesis template T is represented by a subgraph pattern rewriting rule 3 T := oT ! rT1 + rT2 + . . .+ rTN(T ), (1) where N(·) represents the number of reactant subgraphs in the template, as illustrated in Figure. 1. Generally we can treat the subgraph pattern oT as the extracted reaction center from O, and rT i , i 2 1, 2, . . . , N(T ) as the corresponding pattern inside i-th reactant, though practically this will include neighboring structures of reaction centers as well. We first introduce the notations to represent these chemical entities: • Subgraph patterns: we use lower case letters to represent the subgraph patterns. • Molecule: we use capital letters to represent the molecule graphs. By default, we use O for an outcome molecule, and R for a reactant molecule, or M for any molecule in general. • Set: sets are represented by calligraphic letters. We use M to denote the full set of possible molecules, T to denote all extracted retrosynthetic templates, and F to denote all the subgraph patterns that are involved in the known templates. We further use Fo to denote the subgraphs appearing in reaction outcomes, and Fr to denote those appearing in reactants, with F = Fo S Fr. Task: Given a production or target molecule O, the goal of a one-step retrosynthetic analysis is to identify a set of reactant molecules R 2 P(M) that can be used to synthesize the target O. Here P(M) is the power set of all molecules M.Let I[m ✓ M ] : F ⇥M 7! {0, 1} be the predicate that indicates whether subgraph pattern m is a subgraph inside molecule M . This can be checked via subgraph matching. Then the use of a retrosynthetic template T : oT ! rT1 + rT2 + . . . + rTN(T ) for reasoning about a reaction can be decomposed into two-step logic. First, I. Match template: O(T ) := I[oT ✓ O] · I[T 2 T ], (2) where the subgraph pattern oT from the reaction template T is matched against the product O, i.e., o T is a subgraph of the product O. Second, II. Match reactants: O,T (R) := O(T ) · I[|R| = N(T )] · Q N(T ) i=1 I[rTi ✓ R⇡(i)], (3) where the set of subgraph patterns {r1, . . . , rN(T )} from the reaction template are matched against the set of reactants R. The logic is that the size of the set of reactant R has to match the number of patterns in the reaction template T , and there exists a permutation ⇡(·) of the elements in the reactant set R such that each reactant matches a corresponding subgraph pattern in the template. Since there will still be uncertainty in whether the reaction is possible from a chemical perspective even when the template matches, we want to capture such uncertainty by allowing each template/or logic reasoning rule to have a different confidence score. More specifically, we will use a template score function w1(T,O) given the product O, and the reactant score function w2(R, T, O) given the template T and the product O. Thus the overall probabilistic models for the reaction template T and the set of molecules R are designed as I. Match template: p(T |O) / exp (w1(T,O)) · O(T ), (4) II. Match reactants: p(R|T,O) / exp (w2(R, T, O)) · O,T (R). (5) Given the above two step probabilistic reasoning models, the joint probability of a single-step retrosythetic proposal using reaction template T and reactant set R can be written as p (R, T |O) / exp (w1 (T,O) + w2 (R, T, O)) · O (T ) O,T (R) , (6) In this energy-based model, whether the graphical model (GM) is directed or undirected is a design choice. We will present our directed GM design and the corresponding partition function in Sec 4 shortly. We name our model as Conditional Graph Logic Network (GLN) (Fig. 2), as it is a conditional graphical model defined with logic rules, where the logic variables are graph structures (i.e., molecules, subgraph patterns, etc.). In this model, we assume that satisfying the templates is a necessary condition for the retrosynthesis, i.e., p (R, T |O) 6= 0 only if O (T ) and O,T (R) are nonzero. Such restriction provides sparse structures into the model, and makes this abductive type of reasoning feasible. 3Commonly encoded using SMARTS/SMIRKS patterns Reaction type conditional model: In some situations when performing the retrosynthetic analysis, the human expert may already have a certain type c of reaction in mind. In this case, our model can be easily adapted to incorporate this as well: p(R, T |O, c) / exp (w1 (T,O) + w2 (R, T, O)) · O (T ) O,T (R) I[T 2 Tc] (7) where Tc is the set of retrosynthesis templates that belong to reaction type c. GLN is related but significantly different from Markov Logic Network (MLN, which also uses graphical model to model uncertainty in logic rules). MLN treats the predicates of logic rules as latent variables, and the inference task is to get the posterior for them. While in GLN, the task is the structured prediction, and the predicates are implemented with subgraph matching. We show more details on this connection in Appendix A.Although the model we defined so far has some nice properties, the design of the components plays a critical role in capturing the uncertainty in the retrosynthesis. We first describe a decomposable design of p(T |O) in Sec. 4.1, for learning and sampling efficiency consideration; then in Sec. 4.2 we describe the parameterization of the scoring functions w1, w2 in detail. 4.1 Decomposable design of p(T |O) Depending on how specific the reaction rules are, the template set T could be as large as the total number of reactions in extreme case. Thus directly model p(T |O) can lead to difficulties in learning and inference. By revisiting the logic rule defined in Eq. (2), we can see the subgraph pattern oT plays a critical role in choosing the template. Since we represent the templates as T = (oT ! r T i N(T ) i=1 ), it is natural to decompose the energy function w1(T,O) in Eq. (4) as w1(T,O) = v1 o T , O + v2 ⇣ r T i N(T ) i=1 , O ⌘ . Meanwhile, recall the template matching rule is also decomposable, so we obtain the resulting template probability model as: p(T |O) = p(oT , r T i N(T ) i=1 |O) (8) = 1 Z(O) exp v1(oT , O) · I ⇥ o T 2 O ⇤ ⇣ exp ⇣ v2 ⇣ r T i N(T ) i=1 , O ⌘⌘ · I[(oT ! r T i N(T ) i=1 ) 2 T ] ⌘ , where the partition function Z (O) is defined as: Z (O) = P o2F exp (v1(o,O)) · I [o 2 O] · ⇣P {r}2P(F) exp (v2 ({r} , O)) · I[(o ! {r}) 2 T ] ⌘ (9) Here we abuse the notation a bit to denote the set of subgraph patterns as {r}. With such decomposition, we can further speed up both the training and inference for p(T |O), since the number of valid reaction centers per molecule and number of templates per reaction center are much smaller than total number of templates. Specifically, we can sample T ⇠ p(T |O) by first sampling reaction center p(o|O) / exp (v1(o,O)) · I [o 2 O] and then choosing the subgraph patterns for reactants p({r} |O, o) / exp (v2 ({r} , O) · I[(o ! {r}) 2 T ]). In the end we obtain the templated represented as (o ! {r}). In the literature there have been several attempts for modeling and learning p(T |O), e.g., multi-class classification [9] or multiscale model with human defined template hierarchy [14]. The proposed decomposable design follows the template specification naturally, and thus has nice graph structure parameterization and interpretation as will be covered in the next subsection. Finally the directed graphical model design of Eq. (6) is written as p(R, T |O) = 1 Z(O)Z(T,O) exp ⇣⇣ v1 o T , O + v2 ⇣ r T i N(T ) i=1 ⌘ + w2 (R, T, O) ⌘⌘ · O (T ) O,T (R) (10) where Z(T,O) = P R2P(M) exp (w2(R, T, O)) · O,T (R) sums over all subsets of molecules. 4.2 Graph Neuralization for v1, v2 and w2 Since the arguments of the energy functions w1, w2 are molecules, which can be represented by graphs, one natural choice is to design the parameterization based on the recent advances in graph neural networks (GNN) [17, 18, 19, 20, 21, 22]. Here we first present a brief review of the general form of GNNs, and then explain how we can utilize them to design the energy functions. The graph embedding is a function g : M S F 7! Rd that maps a graph into d-dimensional vector. We denote G = (VG, EG) as the graph representation of some molecule or subgraph pattern, where VG = {vi}|V G| i=1 is the set of atoms (nodes) and EG = ei = (e1i , e 2 i ) |EG| i=1 is the set of bonds (edges). We represent each undirected bond as two directional edges. Generally, the embedding of the graph is computed through the node embeddings hvi that are computed in an iterative fashion. Specifically, let h0 vi = xvi initially, where xvi is a vector of node features, like the atomic number, aromaticity, etc. of the corresponding atom. Then the following update operator is applied recursively: h l+1 v = F (xv, (hl u , xu!v u2N (v)) where xu!v is the feature of edge u ! v. (11) This procedure repeats for L steps. While there are many design choices for the so-called message passing operator F , we use the structure2vec [21] due to its simplicity and efficient c++ binding with RDKit. Finally we have the parameterization h l+1 v = (✓1xv + ✓2 X u2N (v) h l u + ✓3 X u2N (v) (✓4xu!v)) (12) where (·) is some nonlinear activation function, e.g., relu or tanh, and ✓ = {✓1, . . . , ✓4} are the learnable parameters. Let the node embedding hv = hLv be the last output of F , then the final graph embedding is obtained via averaging over node embeddings: g(G) = 1|VG| P v2VG hv. Note that attention [23] or other order invariant aggregation can also be used for such aggregation. With the knowledge of GNN, we introduce the concrete parametrization for each component: • Parameterizing v1: Given a molecule O, v1 can be viewed as a scoring function of possible reaction centers inside O. Since the subgraph pattern o is also a graph, we parameterize it with inner product, i.e., v1(o,O) = g1(o)>g2(O). Such form can be treated as computing the compatibility between o and O. Note that due to our design choice, v1(o,O) can be written as v1(o,O) =P v2VO h > v g1(o). Such form allows us to see the contribution of compatibility from each atom in O. • Parameterizing v2: The size of set of subgraph patterns r T i N(T ) i=1 varies for different template T . Inspired by the DeepSet [24], we use average pooling over the embeddings of each subgraph pattern to represent this set. Specifically, v2( r T i N(T ) i=1 , O) = g3(O) > 0 @ 1 N(T ) N(T )X i=1 g4(r T i )) 1 A (13) • Parameterizing w2: This energy function also needs to take the set as input. Following the same design as v2, we have w2(R, T, O) = g5(O)> 1 |R| X R2R g6(R) ! . (14) Note that our GLN framework isn’t limited to the specific parameterization above and is compatible with other parametrizations. For example, one can use condensed graph of reaction [25] to represent R as a single graph. Other chemistry specialized GNNs [3, 26] can also be easily applied here. For the ablation study on these design choices, please refer to Appendix C.1.Given dataset D = {(Oi, Ti,Ri)}|D|i=1 with |D| reactions, we denote the parameters in w1 (T,O) , w2 (T,R, O) as ⇥ = (✓1, ✓2), respectively. The maximum log-likelihood estimation (MLE) is a natural choice for parameter estimation. Since 8 (O, T,R) ⇠ D, O (T ) = 1 and O,T (R) = 1, we have the MLE optimization as max ⇥ ` (⇥) := bED [log p (R|T,O) p (T |O)] (15) = bED [w1 (T,O) + w2 (R, T, O) logZ (O) logZ (O, T )] , The gradient of ` (⇥) w.r.t. ⇥ can be derived4 as r⇥` (⇥) = bED [r⇥w1 (T,O)] bEOET |O [r⇥w1 (T,O)] (16) +bED [r⇥w2 (R, T, O)] bEO,TER|T,O [r⇥w2 (R, T, O)] , where ET |O [·] and ER|O,T [·] stand for the expectation w.r.t. current model p (T |O) and p (R, T |O), respectively. With the gradient estimator (16), we can apply the stochastic gradient descent (SGD) algorithm for optimizing (15). Efficient inference for gradient approximation: Since R 2 P(M) is a combinatorial space, generally the expensive MCMC algorithm is required for sampling from p (R|T,O) to approximate (16). However, this can be largely accelerated by scrutinizing the logic property in the proposed model. Recall that the matching between template and reactants is the necessary condition for p (R, T |O) 0 by design. On the other hand, given O, only a few templates T with reactants R have nonzero O (T ) and O,T (R). Then, we can sample T and R by importance sampling on restricted supported templates instead of MCMC over P (M). Rigorously, given O, we denote the matched templates as TO and the matched reactants based on T as RT,O, where TO = {T : O (T ) 6= 0, 8T 2 T } and RT,O = {R : O,T (R) 6= 0, 8R 2 P (M)} (17) Algorithm 1 Importance Sampling for br⇥` (⇥) 1: Input (R, T, O) ⇠ D, p (R|T,O) and p (T |O). 2: Construct TO according to O (T ). 3: Sample T̃ / exp (w1 (T,O)) , 8T 2 TO in hierar- chical way, as in Sec. 4.1. 4: Construct RT,O according to O,T (R). 5: Sample R̃ / exp (w2 (R, T, O)). 6: Compute stochastic approximation br⇥` (⇥) with sample ⇣ R, T, R̃, T̃ , O ⌘ by (16). Then, the importance sampling leads to an unbiased gradient approximation br⇥` (⇥) as illustrated in Algorithm 1. To make the algorithm more efficient in practice, we have adopted the following accelerations: • 1) Decomposable modeling of p(T |O) as described in Sec. 4.1; • 2) Cache the computed TO and R (T,O) in advance. In a dataset with 5 ⇥ 104 reactions, |TO| is about 80 and |RT,O| is roughly 10 on average. Therefore, we reduce the actual computational cost to a manageable constant. We further reduce the computation cost of sampling by generating the T and R uniformly from the support. Although these samples only cover the support of the model, we avoid the calculation of the forward pass of neural networks, achieving better computational complexity. In our experiment, such an approximation already achieves state-ofthe-art results. We would expect recent advances in energy based models would further boost the performance, which we leave as future work to investigate. Remark on RT,O: Note that to get all possible sets of reactants that match the reaction template T and product O, we can efficiently use graph edit tools without limiting the reactants to be known in the dataset. This procedure works as follows: given a template T = oT ! rT1 + . . .+ rTN , 1) Enumerate all matches between subgraph pattern oT and target product O. 2) Instantiate a copy of the reactant atoms according to rT1 , . . . , rTN for each match. 3) Copy over all of the connected atoms and atom properties from O. 4We adopt the conventions 0 log 0 = 0 [27], which is justified by continuity since x log x ! 0 as x ! 0. This process is a routine in most Cheminformatics packages. In our paper we use runReactants from RDKit with the improvement of stereochemistry handling 5 to realize this. Further acceleration via beam search: Given a product O, the prediction involves finding the pair (R, T ) that maximizes p(R, T |O). One possibility is to first enumerate T 2 T (O) and then R 2 RT,O. This is acceptable by exploiting the sparse support property induced by logic rules. A more efficient way is to use beam search with size k. Firstly we find k reaction centers {oi}ki=1 with top v1(o,O). Next for each o 2 {oi}ki=1 we score the corresponding v2({r} , O) · I [(o ! {r}) 2 T ]. In this stage the top k pairs {(oTj , {r Tj i })}k j=1 (i.e., the templates) that maximize v1(o|O) + v2({r} , O) are kept. Finally using these templates, we choose the best R 2 S k j=1 RTj ,O that maximizes total score w1 (T,O) + w2 (R, T, O). Fig. 2 provides a visual explanation.Dataset: We mainly evaluate our method on a benchmark dataset named USPTO-50k, which contains 50k reactions of 10 different types in the US patent literature. We use exactly the same training/validation/test splits as Coley et al. [8], which contain 80%/10%/10% of the total 50k reactions. Table 1 contains the detailed information about the benchmark. Additionally, we also build a dataset from the entire USPTO 1976-2016 to verify the scalability of our method. Baselines: Baseline algorithms consist of rule-based ones and neural network-based ones, or both. The expertSys is an expert system based on retrosynthetic reaction rules, where the rule is selected according to the popularity of the corresponding reaction type. The seq2seq [10] and transformer [11] are neural sequence-to-sequence-based learning model [28] implemented with LSTM [29] or Transformer [30]. These models encode the canonicalized SMILES representation of the target compound as input, and directly output canonical SMILES of reactants. We also include some data-driven template-based models. The retrosim [8] uses direct calculation of molecular similarities to rank the rules and resulting reactants. The neuralsym [9] models p(T |O) as multi-class classification using MLP. All the results except neuralsym are obtained from their original reports, since we have the same experiment setting. Since neuralsym is not open-source, we reimplemented it using their best reported ELU512 model with the same method for parameter tuning. Evaluation metric: The evaluation metric we used is the top-k exact match accuracy, which is commonly used in the literature. This metric compares whether the predicted set of reactants are exactly the same as ground truth reactants. The comparison is performed between canonical SMILES strings generated by RDKit. Setup of GLN: We use rdchiral [31] to extract the retrosynthesis templates from the training set. After removing duplicates, we obtained 11,647 unique template rules in total for USPTO-50k. These rules represent 93.3% coverage of the test set. That is to say, for each test instance we try to apply these rules and see if any of the rules gives exact match. Thus this is the theoretical upper bound of the rule-based approach using this particular degree of specificity, which is high enough for now. For more information about the statistics of these rules, please refer to Table 2. We train our model for up to 150k updates with batch size of 64. It takes about 12 hours to train with a single GTX 1080Ti GPU. We tune embedding sizes in {128, 256}, GNN layers {3, 4, 5} and GNN aggregation in {max, mean, sum} using validation set. Our code is released at https://github.com/HanjunDai/GLN. More details are included in Appendix B.We present the top-k exact match accuracy in Table 3, where k ranges from {1, 3, 5, 10, 20, 50}. We evaluate both the reaction class unknown and class conditional settings. Using the reaction class as prior knowledge represents some situations where the chemists already have an idea of how they would like to synthesize the product. In all settings, our proposed GLN outperforms the baseline algorithms. And particularly for top-1 accuracy, our model performs significantly better than the second best method, with 8.1% higher accuracy with unknown reaction class, and 8.9% higher with reaction class given. This demonstrates the advantage of our method in this difficult setting and potential applicability in reality. 5https://github.com/connorcoley/rdchiral. USPTO 50k # train 40,008 # val 5,001 # test 5,007 # rules 11,647 # reaction types 10 Table 1: Dataset information. Rule coverage 93.3% # unique centers 9,078 Avg. # centers per mol 29.31 Avg. # rules per mol 83.85 Avg. # reactants 1.71 Top-k accuracy % methods 1 3 5 10 20 50 Reaction class unknown transformer[11] 37.9 57.3 62.7 / / / retrosim[8] 37.3 54.7 63.3 74.1 82.0 85.3 neuralsym[9] 44.4 65.3 72.4 78.9 82.2 83.1 GLN 52.5 69.0 75.6 83.7 89.0 92.4 Reaction class given as prior expertSys[10] 35.4 52.3 59.1 65.1 68.6 69.5 seq2seq[10] 37.4 52.4 57.0 61.7 65.9 70.7 retrosim[8] 52.9 73.8 81.2 88.1 91.8 92.9 neuralsym[9] 55.3 76.0 81.4 85.1 86.5 86.9 GLN 64.2 79.1 85.2 90.0 92.3 93.2 Table 3: Top-k exact match accuracy. Ground truth Similarity=0.82 Similarity=0.87 Similarity=0.82 N NH F NH S O O F O N S O I N NH F NH S O O F O N S O O N N NH F NH S O O F O N S O S O O NH F O OH F N NH2 N N S O O N NH F NH S O O F O N S O O NH O F NH S O O F N N S O O O NH N NH F NH S O O F O N S O S O O NH F O OH F N NH2 N N S O O Figure 4: Example failed predictions. Moreover, our performance in the reaction class unknown setting even outperforms expertSys and seq2seq in the reaction conditional setting. Since the transformer paper didn’t report top-k performance for k > 10, we leave it as blank. Meanwhile, Karpov et al. [11] also reports the result when training using training+validation set and tuning on the test set. With this extra priviledge, the top-1 accuracy of transformer is 42.7% which is still worse than our performance. This shows the benefit of our logic powered deep neural network model comparing to purely neural models, especially when the amount of data is limited. Since the theoretical upper bound of this rule-based implementation is 93.3%, the top-50 accuracy for our method in each setting is quite close to this limit. This shows the probabilistic model we built matches the actual retrosynthesis target well.Visualizing the predicted synthesis: In Fig 3 and 4, we visualize the ground truth reaction and the top 3 predicted reactions (see Appendix C.6 for high resolution figures). For each reaction, we also highlight the corresponding reaction cores (i.e., the set of atoms get changed). This is done by matching the subgraphs from predicted retrosynthesis template with the target compound and generated reactants, respectively. Fig 3 shows that our correct prediction also gets almost the same reaction cores predicted as the ground truth. In this particular case, the explanation of our prediction aligns with the existing reaction knowledge. Fig 4 shows a failure mode where none of the top-3 prediction matches. In this case we calculated the similarity between predicted reactants and ground truth ones using Dice similarity from RDKit. We find these are still similar in the molecule fingerprint level, which suggests that these predictions could be the potentially valid but unknown ones in the literature. Visualizing the reaction center prediction: Here we visualize the prediction of probabilistic modeling of reaction center. This is done by calculating the inner product of each atom embedding in target molecule with the subgraph pattern embedding. Fig 5 shows the visualization of scores on the atoms that are part of the reaction center. The top-1 prediction assigns positive scores to these atoms (red ones), while the bottom-1 prediction (i.e., prediction with least probability) assigns large negative scores (blue ones). Note that although the reaction center in molecule and the corresponding subgraph pattern have the same structure, the matching scores differ a lot. This suggests that the model has learned to predict the activity of substructures inside molecule graphs.In addition to the overall numbers in Table 3, we provide detailed study of the performances. This includes per-category performance, the accuracy of each module in hierarchical sampling and also the effect of the beam size. Due to the space limit, please refer to Appendix C. 6.4 Large scale experiments on USPTO-full retrosim neuralsym GLN top-1 32.8 35.8 39.3 top-10 56.1 60.8 63.7 Table 4: Top-k accuracy on USPTO-full. To see how this method scales up with the dataset size, we create a large dataset from the entire set of reactions from USPTO 1976-2016. There are 1,808,937 raw reactions in total. For the reactions with multiple products, we duplicate them into multiple ones with one product each. After removing the duplications and reactions with wrong atom mappings, we obtain roughly 1M unique reactions, which are further divided into train/valid/test sets with size 800k/100k/100k. We train on single GPU for 3 days and report with the model having best validation accuracy. The results are presented in Table 4. We compare with the best two baselines from previous sections. Despite the noisiness of the full USPTO set relative to the clean USPTO-50k, our method still outperforms the two best baselines in top-k accuracies.Evaluation: Retrosynthesis usually does not have a single right answer. Evaluation in this work is to reproduce what is reported for single-step retrosynthesis. This is a good, but imperfect benchmark, since there are potentially many reasonable ways to synthesize a single product. Limitations: We share the limitations of all template-based methods. In our method, the template designs, more specifically, their specificities, remain as a design art and are hard to decide beforehand. Also, the scalability is still an issue since we rely on subgraph isomorphism during preprocessing. Future work: The subgraph isomorphism part can potentially be replaced with predictive model, while during inference the fast inner product search [32] can be used to reduce computation cost. Also actively building templates or even inducing new ones could enhance the capacity and robustness.We would like to thank anonymous reviewers for providing constructive feedbacks. This project was supported in part by NSF grants CDS&E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983 to L.S.
Autoencoders are a class of machine learning models that have been used for various purposes such as dimensionality reduction, representation learning, or unsupervised pretraining (see, e.g., Hinton & Salakhutdinov (2006); Bengio (2009); Erhan et al. (2010); Goodfellow et al. (2016)). In a nutshell, autoencoders are feed-forward neural networks which encode the given data in a latent, fixed-size representation, and subsequently try to reconstruct the input data in their output variables using a decoder function. This basic mechanism of encoding and decoding is applicable to a wide variety of input distributions. Recently, researchers have proposed a sequence autoencoder (Dai & Le, 2015), a model that is able to handle sequences of inputs by using a recurrent encoder and decoder. Furthermore, there has been growing interest to tackle sets of elements with similar recurrent architectures (Vinyals et al., 2015a; 2016; Xu et al., 2016). In this paper, we propose the set autoencoder – a model that learns to embed a set of elements in a permutation-invariant, fixed-size representation using unlabeled training data only. The basic architecture of our model corresponds to that of current sequence-to-sequence models (Sutskever et al., 2014; Chan et al., 2016; Vinyals et al., 2015c): It consists of a recurrent encoder that takes a set of inputs and creates a fixed-length embedding, and a recurrent decoder that uses the fixedlength embedding and outputs another set. As encoder, we use an LSTM network with an attention mechanism as in (Vinyals et al., 2015a). This ensures that the embedding is permutation-invariant in the input. Since we want the loss of the model to be permutation-invariant in the decoder output, we re-order the output and align it to the input elements, using a stable matching algorithm that calculates a permutation matrix. This approach yields a loss which is differentiable with respect to the model’s parameters. The proposed model can be trained in an unsupervised fashion, i.e., without having a labeled data set for a specific task. In a series of experiments, we analyze the properties of the embedding. For example, we show that the learned embedding is to some extent distance-preserving, i.e., the distance between two sets of elements correlates with the distances of their embeddings. Also, the embedding is smooth, i.e., small changes in the input set lead to small changes of the respective embedding. Furthermore, we show g o <start> a l l a l l e e r encoder decoder Figure 1: Example of a sequence-to-sequence translation model. The encoder receives the input characters ["g","o"]. Its internal state is passed to the decoder, which outputs the translation, i.e., the characters of the word "aller". that pretraining in an unsupervised fashion can help to increase the performance on supervised tasks when using the fixed-size embedding as input to a classification or regression model, especially if training data is limited. The rest of the paper is organized as follows. Section 2 introduces the preliminaries and briefly discusses related work. In Section 3, we present the details of the set autoencoder. Section 4 presents experimental setup and results. We discuss the results and conclude the paper in Section 5.Sequence-to-sequence models have been applied very successfully in various domains such as automatic translation (Sutskever et al., 2014), speech recognition (Chan et al., 2016), or image captioning (Vinyals et al., 2015c). In all these domains, the task is to model P (Y |X), i.e., to predict an output sequence Y = (y1, y2, . . . , ym) given an input sequence X = (x1, x2, . . . , xn). Figure 1 shows the basic architecture of a sequence-to-sequence model. It consists of an encoder and a decoder, both of which are usually recurrent neural networks (RNNs). In the figure, the sequence-to-sequence model translates the input sequence X = (g, o) to the output sequence Y = (a, l, l, e, r). One by one, the elements of the input sequence are passed to the encoder as inputs. The encoder always updates its internal state given the previous state and the new input. Now, the last internal state of the encoder represents a fixed-size embedding of the input sequence (and is sometimes referred to as the thought vector). The decoder network’s internal state is now initialized with the thought vector, and a special "start" token is passed as the input. One by one, the decoder will now output the tokens of the output sequence, each of which is used as input in the next decoder step. By calculating a loss on the output sequence, the complete sequence-to-sequence model can be trained using backpropagation. A special case of a sequence-to-sequence model is the sequence autoencoder (Dai & Le, 2015), where the task is to reconstruct the input in the output. For a more formal description of sequence-to-sequence models, please refer to (Sutskever et al., 2014).Researchers have tackled sets of elements directly with neural networks, without using explicit but lossy set representations such as the popular bag-of-words-model (Harris, 1954). Vinyals et al. raise the question of how the sequence-to-sequence architecture can be adapted to handle sets. They propose an encoder that achieves the required permutation-invariance to the input elements by using a content-based attention mechanism. Using a pointer network (Vinyals et al., 2015b) as decoder, the model can then be trained to sort input sets and outperforms a model without a permutation-invariant encoder. The proposed attention-based encoder has been used successfully in other tasks such as one-shot or few-shot learning (Vinyals et al., 2016; Xu et al., 2016). Another approach (Ravanbakhsh et al., 2016) tackles sets of fixed size by introducing a permutation-equivariant1 layer in standard neural networks. For sets containing more than a few elements, the proposed layer helps to solve problems like point cloud classification, calculating sums of images depicting numbers, or set anomaly detection. The proposed models can fulfill complex supervised tasks and operate on sets of elements by exploiting permutation equi- or invariance. However, they do not make use of unlabeled data.The objective of the set autoencoder is very similar to that of the sequence autoencoder (Dai & Le, 2015): to create a fixed-size, permutation-invariant embedding for an input set X = 1A function g is permutation equivariant, if π(g(x)) = g(π(x)), for all permutations π. However, we are more interested in permutation invariant functions g, such that g(x) = g(π(x)), ∀π {x1, x2, . . . , xn}, xi ∈ Rd by using unsupervised learning, i.e., unlabeled data. The motivation is that unlabeled data is much easier to come by, and can be used to pretrain representations, which facilitate subsequent supervised learning on a specific task (Erhan et al., 2010). In contrast to the sequence autoencoder, the set autoencoder needs to be permutation invariant, both in the input and the output set. The first can be achieved directly by employing a recurrent encoder architecture using content-based attention similar to the one proposed by (Vinyals et al., 2015a) (see Section 3.1). Achieving permutation invariance in the output set is not straightforward. When training a set autoencoder, we need to provide the desired outputs Y in some order. By definition, all possible orders should be equally good, as long as all elements of the input set and no surplus elements are present. In theory, the order in which the outputs are presented to the model (or, more specifically: to the loss function) should be irrelevant: by using a chain rule-based model, the RNN can, in principle, model any joint probability distribution, and we could simply enlarge the data set by including all permutations. Since the number of permutations grows exponentially in the number of elements, this is not a feasible way: The data set quickly becomes huge, and the model has to learn to create every possible permutation of output sets. Therefore, we need to tackle the problem of random permutations in the outputs differently, while maintaining a differentiable architecture (see Section 3.2).2The encoder takes the input set and embeds it into the fixed-sized latent representation. This representation should be permutation invariant to the order of the inputs. We use an architecture with content-based attention mechanism similar to the one proposed in (Vinyals et al., 2015a) (see Figure 2): i First, each element xi of the input set X is mapped to a memory slot mi ∈ Rl using a mapping function f inp (Eq. 1). We use a linear mapping as f inp, the same for all i3 . Then, an LSTM network (Hochreiter & Schmidhuber, 1997; Gers & Schmidhuber, 2000) with l cells performs n steps of calculation. In each step, it calculates its new cell state ct ∈ Rl and hidden state ht ∈ Rl using the previous cell- and hidden state ct−1 and ht−1, as well as the previous read vector rt−1, all of which are initialized to zero in the first step. The new read vector rt is then calculated as weighted combination of all memory locations, using the attention mechanism (Eq. 5). For each memory location i, the attention mechanism calculates a scalar value ei,t which increases based on the similarity between the memory value mi and the hidden state ht, which is interpreted as a query to the memory (Eq. 3). We set f dot to be a dot product. Then, the normalized weighting ai for all memory locations is calculated using a softmax (Eq. 4). After n steps, the concatenation of cn,hn and rn can be seen as a fixed-size embedding of X4. Note that permuting the elements of X has no effect on the embedding, since the memory locations mi are weighted by content, and the sum in Eq. 5 is commutative. 2Note that, like the encoder, the decoder is implemented as a recurrent architecture. This is to accommodate for the fact that the set size is not specified – using a simple feed-forward architecture is not straight forward in this case. 3(Vinyals et al., 2015a) used a "small neural network" for this task 4Preliminary experiments showed that the quality of the embedding (based on decoder performance) was relatively robust to the usage of different combinations of cn, hn, and rn. We chose to include all three values, since it is not our goal to create the most compact embedding possible.Section 3.1 defined the forward propagation from the input set X to the fixed-size embedding [ct, ht, rt]. We now define the output of the set autoencoder that allows us to train the model using a loss function L. Like in the original sequence-to-sequence model, the core of the decoder is an LSTM network (see Figure 3): LSTM r̂t ĉt, ĥt ĥt f eosf out ot ωt In each step, the decoder LSTM calculates its internal cell state ĉt and its hidden state ĥt (Eq. 6). The cell- and hidden state are initialized with the cell- and hidden state from the embedding, produced by the encoder (Eq. 7 and Eq. 8). In the first step, the decoder also gets an additional input r̂0, which is set to be the last read vector of the encoder (Eq. 9). In all following steps, r̂t is a vector of all zeros. We calculate the decoder’s output ot at step t by using the linear function f out (Eq. 10). Each output element ot is of the same dimensionality as the input elements xi. The underlying idea is that f out is the “reverse” operation to f inp, such that encoder and decoder LSTM can operate on similar representations. Furthermore, in each step, the function f eos calculates ωt (Eq. 11), which we interpret as the probability that ot is an element of the set. In other words, if ωt = 0, we can stop sampling. In principle, we could use the LSTM’s output sequence O = (o1, o2, . . . , om) directly as elements of the output set. However, this does not take into account the following issues: First, the number m of outputs produced by the decoder should be equal to the size n of the input set X . This can be achieved by learning to output the correct ωt (see Eq. 12 below). Second, the order of the set elements should be irrelevant, i.e., the loss function should be permutation-invariant in the outputs. To address the latter point, we introduce an additional mapping layer D = (d0, d1, . . . , dn) between the decoder output and the loss function. The mapping rearranges the first n decoder outputs in the order of the inputs X . That is, it should make sure that the distance between di and xi is small for all i. The mapping is defined as: di = ∑n j=1 ojwij Here, wij are the elements of a permutation matrix W of size n× n with the properties wij ∈ {0, 1} ∀i, j ∈ 1 . . . n ∑ i wij = 1 ∀j ∑ j wij = 1 ∀i In other words, each output oi is mapped to exactly one dj , and vice versa. For now, we assume that W has been parametrized such that the re-ordered elements in D match the elements in input set X well (see Section 3.3). Then, the set autoencoder loss function can be calculated as L = n∑ i=1 L(xi, di) + m∑ i=1 Leos(ωi, ω ∗ i ) (12) The function L(xi, di) is small if xi and di are similar, and large if xi and di are dissimilar. In other words, for each element in the input set, the distance to a matching output element (as mapped by W ) will be decreased by minimizing L. For discrete elements, L can be calculated as the cross entropy loss L(x, d) = − ∑ i xi log di. For elements that are vectors of real numbers, L is a norm of these vectors, e.g., L(x, d) = ||x− d||. The function Leos calculates the cross-entropy loss between ωi and ω∗t , where ω ∗ i indicates if an i th element should be present in the output, i.e., ω∗i = 1 if i <= n, 0 else (recall that the decoder can produce m outputs, and m is not necessarily equal to n). Since the whole set autoencoder is differentiable, we train all weights except W using gradient descent.Re-ordering the decoder outputs resembles a point cloud correspondence problem from the domain of computer vision (Jahne, 2000; Sonka et al., 2014). Methods like the iterative closest points algorithm Algorithm 1 Gale-Shapely algorithm for stable matching Initialize all m ∈ M and w ∈ W to free while ∃ free man m who still has a woman w to propose to do −0.5 0.5 −0.5 0.5 −0.5 0.5 −0.5 0.5 −0.5 0.5 −0.5 0.5 −0.5 0.5 −0.5 0.5 (Besl & McKay, 1992) find closest points between two sets, and find a transformation that aligns the second set to the first. Since we are only interested in the correspondence step, we notice its similarity to the stable marriage problem (Gusfield & Irving, 1989): We want to find matching pairs P i = {mani,womani} of two sets of men and women, such that there are no two pairs P i, P j where element mani prefers womanj over womani, and, at the same time, womanj prefers mani over manj .5 To solve this problem greedily, we can use the Gale-Shapely algorithm (Gale & Shapley, 1962), which has a run time complexity of O(n2) (see Algorithm 1)6. Since its solution is permutation invariant in the set that proposes first (Gusfield & Irving, 1989)(p. 10), we consider the input elements xi to be the men, and let them propose first. After the stable marriage step, wij = 1 if xi is “engaged” to oj .We use a number of synthetic data sets of point clouds for the unsupervised experiments. Each data set consists of sets of up to k items with d dimensions. In the random data sets, the k values of each element are randomly drawn from a uniform distribution between -0.5 and 0.5. In the following experiments, we set k = 16 and d ∈ {1, 2, 3}. In other words, we consider sets of up to 16 elements that are randomly distributed along a zero-centered 1d-line, 2d-plane, or 3-d cube with side length 1. We choose random distributions to evaluate the architecture’s capability of reconstructing elements from sets, rather than learning common structure within those sets. In the shapes data set, we create point clouds with up to k = 32 elements of d = 2 dimensions. In each set, the points form either a circle, a square, or a cross (see Figure 4). The shapes can occur in different positions and vary in size. To convey enough information, each set consists of at least 10 points. Each data set contains 500k examples in the training set, 100k examples in the validation set, and another 500k examples in the test set. For each of the data sets we train a set autoencoder to minimize the reconstruction error of the sets, i.e., to minimize Eq. 12 (please refer to the appendix for details of the training procedure, including all hyperparameters). Figure 5 shows the mean euclidean distance of the reconstructed elements for the three random data sets (left-hand side) and the shapes data set (right-hand side), for different set sizes. For the random data sets, the mean error increases with the number of dimensions d of the elements, and with the number of elements within a set. This is to be expected, since all values are completely uncorrelated. For the shapes data set, the average error is lower than the errors for the 2d-random data set with more than two elements. Furthermore, the error decreases with the number of elements in the set. We hypothesize that with a growing number of points, the points become more evenly distributed along the outlines of the shapes, and it is therefore easier for the model to reconstruct them (visual inspection of the reconstructions suggests that the model tends to distribute points more uniformly on the shapes’ outlines). We now take a closer look at the embeddings of the set autoencoder (i.e., the vector [ct, ht, rt]) 5Note that this is a relaxation of the correspondence problem, since the concept of preference in the stable marriage problem is ordinal rather than cardinal, i.e., we do not consider the exact distances between elements, but ranks. 6This complexity could restrict the applicability of the proposed algorithm to smaller sets. However, there is a range of problems where small set sizes are relevant, e.g. when an agent interacts with an environment where one or multiple instances of an object can be present (as opposed to point cloud representations of objects). 1 3 5 7 9 11 13 15 Number of elements 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 E u cl id e a n d is ta n ce 1d elements 2d elements 3d elements (a) Random data sets with d ∈ {1, 2, 3} 11 13 15 17 19 21 23 25 27 29 31 Number of elements 0.031 0.032 0.033 0.034 0.035 0.036 0.037 E u cl id e a n d is ta n ce 2d elements (b) Shapes data set (d = 2) Figure 5: Mean reconstruction error of reconstructed elements in sets of different size. for random sets. Some of the embedding variables have a very strong correlation with the set size (Pearson correlation coefficients of > 0.97 and <-0.985, respectively). In other words, the size of the set is encoded almost explicitly in the embedding. The embeddings seem to be reasonably smooth (see Figure 6). We take a set with a single 2d-element and calculate its embeddings (leftmost points in Figure 6). We then move this element smoothly in the 2d-plane and observe the resulting embeddings. Most of the time, the embeddings change smoothly as well. The discontinuities occur when the element crosses the center region of the 2d plane. Apart from this center region, the embedding preserves a notion of distance of two item sets. This becomes apparent when looking at the correlations between the euclidean distances of two sets X1 and X2 and their corresponding embeddings enc(X1) and enc(X2).7 The correlation coefficients for random sets of size one to four are 0.81, 0.71, 0.67, and 0.64. In other words, similar sets tend to yield similar embeddings. Vinyals et al. show that order matters for sequence-to-sequence and set-to-set models. This is the case both for the order of input sequences – specific orders improve the performance of the model’s task – as well as for the order of the output sets, i.e., the order in which the elements of the set are processed to calculate the loss function. Recall that the proposed set autoencoder is invariant to the order of the elements both in the input set (using the attention mechanism) and the target set (by reordering the outputs before calculating the loss). Nevertheless, we observe that the decoder learns to output elements in a particular order: We now consider sets with exactly 8 random 2-dimensional elements. We use a pretrained set autoencoder from above, encode over 6,000 sets, and subsequently reconstruct the sets using the decoder. Figure 7 shows heat maps of the 2d-coordinates of the i’th reconstructed element. The first reconstructed element tends to be in the center right area. Accordingly, the second element tends to be in the lower-right region, the third element in the center-bottom region, and so on. The decoder therefore has learned to output the elements within a set in a particular order. Note that most of the distributions put some mass in every area, since the decoder must be able to reproduce sets where the items are not distributed equally in all areas (e.g., in a set where all elements are in the top right corner, the first element must be in the top right corner as well). Figure 8 shows the effect of the set size n on the distribution of the first element. If there is only one element (n = 1), it can be anywhere in the 2d plane. The more elements there are, the more likely it is that at least one of them will be in the center right region, so the distribution of the first element gets more peaked. We speculate that the roughly circular arrangement of the element distributions (which occurs for other set sizes as well) could be an implicit result of using the cosine similarity f dot in the attention mechanism of the encoder. Also, this behavior is likely to be the reason for the discontinuity in Figure 6 around [0, 0]. 7We align the elements of X1 and X2 using the Gale-Shapely algorithm before calculating distances. n=1 n=6 n=11 n=16 Figure 8: Heat maps of the location of the first element in sets of various sizes n with 2delements (decoder output). Darker shadings indicate more points at these coordinates.We derive a number of classification and regression tasks based on the data sets in Section 4.1. On the random data sets, we define a number of binary classification tasks. The 1d-, 2d- or 3d- area is partitioned into two, four, or eight areas of equal sizes. Then, two classes of sets are defined: A set is of class 1, if all of its elements are within i of the j defined areas. All other sets are of class two. For example, if d = 2, i = 2, and j = 4, a set is of class 1 if all its elements are in the top left or bottom left area, or any other combination of two areas.8 Furthermore, we define two regression tasks on the random data sets: The target for the first one is the maximum distance between any two elements in the set, the second one is the volume of the d-dimensional bounding box of all elements. On the shapes data set, the three-class classification problem is to infer the prototypical shape represented by the elements in the set. In the following, we use a set autoencoder as defined above, add a standard two-layer neural network f supervised(enc(X)) on top of the embedding, and use an appropriate loss function for the task (for implementation details see supplementary material). We compare the results of the set autoencoder (referred to as set-AE) to those of two vanilla sequence autoencoders, which ignore the fact that the elements form a set. The first autoencoder (seq-AE) gets the same input as the set model, the input for the second autoencoder has been ordered along the first element dimensions (seq-AE (ordered)). Furthermore, we consider three training fashions: For direct training, we train the respective model in a purely supervised fashion on the available training data (in other words: the models are not trained as autoencoders). In the pretrained fashion (pre), we initialize the encoder weights using unsupervised pretraining on the respective (unlabeled) 500k training set, and subsequently train the parameters of f supervised, holding the encoder weights fixed. In the fine tuning setting (fine), we continue training from the pretrained model, and fine-tune all weights. Tables 1(a) and (b) show the accuracy on the test set for the i-of-j-areas classification tasks, for 1,000 and 10,000 training examples.The plain sequence autoencoder is only competitive for the most simple task (first row). For the simple and moderately complex tasks (first three rows), the ordered sequence autoencoder and the set autoencoder reach high accuracies close to 100%, both 8We create new labeled data sets with the same number of examples per class for the small and the large training set. When the task gets more difficult (higher i, j, or d), the set autoencoder clearly outperforms both other models. For the small training set, the pre and fine training modes of the set autoencoder usually lead to better results than direct training. In other words, the unsupervised pretraining of the encoder weights leads to a representation which can be used to master the classification tasks with a low number of labeled examples. For the larger training set, unsupervised pretraining is still very useful for the more complicated classification tasks. On the other hand, unsupervised pretraining only helps in a few rare cases if the elements are treated as a sequence – the representation learned by the sequence autoencoders does not seem to be useful for the particular classification tasks.9 The results or the regression task are shown in Tables 2 (a) and (b). Again, the ordered sequence autoencoder shows good results for small d (recall that the first element dimension is the one that has been ordered), but fails to compete with the set-aware model in the higher dimensions. However, unsupervised pretraining helps the set model in the regression task only for small d. Tables 3 (a) and (b) show the results for the shapes classification task. Here, the ordered sequence autoencoder with fine tuning clearly dominates both other models. The set model is unable to capitalize on the proper handling of permutation invariance. In sum, the results show that unsupervised pretraining of the set autoencoder creates representations that can be useful for subsequent supervised tasks. This is primarily the case, if the supervised task requires knowledge of the individual locations of the elements, as in the i-of-j-areas classification task. If the precise locations of a subset of the elements are required (as in the bounding box or maximum distance regression tasks), direct training yields better results. We hypothesize that failure of the set-aware model on the shapes classification is due to the linear mapping functions f inp and f out: They might be too simple to capture the strong, but non-linear structures in the data.We presented the set autoencoder, a model that can be trained to reconstruct sets of elements using a fixed-size latent representation. The model achieves permutation invariance in the inputs by using a content-based attention mechanism, and permutation invariance in the outputs, by reordering the outputs using a stable marriage algorithm during training. The fixed-size representation possesses a number of interesting attributes, such as distance preservation. We show that, despite the output permutation invariance, the model learns to output elements in a particular order. A series of experiments show that the set autoencoder learns representations that can be useful for tasks that require information about each set element, especially if the tasks are more difficult, and few labeled training examples are present. There are a number of directions for future research. The most obvious is to use non-linear functions for f inp and f out to enable the set autoencoder to capture non-linear structures in the input set, and test the performance on point clouds of 3d data sets such as ShapeNet (Chang et al., 2015). Also, changes to the structure of the encoder/decoder (e.g., which variables are 9This is despite the fact that the reconstruction error after the unsupervised training is much lower for the sequence autoencoders than for the set autoencoder (not in the results tables). interpreted as query or embedding) and alternative methods for aligning the decoder outputs to the inputs can be investigated. Furthermore, more research is necessary to get a better understanding for which tasks the permutation invariance property is helpful, and unsupervised pretraining can be advantageous.We use Tensorflow v0.12 (Abadi et al., 2016) to implement all models. For the implementation and experiments, we made the following design choices:• Both the encoder and the decoder LSTMs are have peephole connections (Gers & Schmidhuber, 2000). We use the LSTM implementation of Tensorflow 10 • The input mapping f inp and output mapping f out functions are simple linear functions. Note that we can not re-use f inp on the decoder side to transform the supervised labels in a “backwards” fashion: In this case, learning could parametrize f inp such that all set elements are mapped to a the same value, and the decoder learns to output this element only. • For the supervised experiments (classification and regression), we add a simple two-layer neural network f supervised on top of the embedding. The hidden layer of this network has the same number of units as the embedding, and uses ReLu activations (Nair & Hinton, 2010). For the two-class problems (i of j areas), we use a single output neuron and a cross-entropy loss, for the multi-class problems (object shapes) we use three output neurons and a cross-entropy loss. For the regression problems (bounding box and maximum distance), we optimize the mean squared error. • All parameters are initialized using Xavier initialization (Glorot & Bengio, 2010) • Batch handling: For efficiency, we use minibatches. Therefore, within a batch, there can be different set sizes ni for each example i in the set. For simplicity, the encoder keeps processing all sets in a batch, i.e., it always performs n = k steps, where k is the maximum set size. Preliminary experiments showed only minor variations in the performance when processing is stopped after ni steps, where ni corresponds to the actual size of set i in the minibatch.• The number l of LSTM cells is automatically determined by the dimensionality d and maximum set size k of the input. We set l = k ∗ d, therefore ct, ht, ĉt, ĥt ∈ Rl. As a consequence, the embedding for all models (set- and sequence autoencoder) could, in principle, comprise the complete information of the set (recall that the goal was not to find the most compact or efficient embedding) • For simplicity, the dimensionality of each the memory cell mi and the read vector ri is equal to the number of LSTM cells, i.e., mi, ri ∈ Rl (in principle, the memory could have any other dimensionality, but this would require an additional mapping step, since the query needs to be of the same dimensionality as the memory).We use Adam (Kingma & Ba, 2014) to optimize all parameters. We keep Adam’s hyperparameters (except for the learning rate) at Tensorflow ’s default values (β1 = 0.9, β2 = 0.999, = 1e−08). We use minibatch training with a batch size of 100. We keep track of the optimization objective during 10https://github.com/tensorflow/tensorflow/blob/r0.12/tensorflow/python/ ops/rnn_cell.py#L363 training and reduce the learning rate by 33% / stop training once there has been no improvement for a defined number of epochs, depending on the training mode (see Table 4). For the classification tasks, we couple the learning rate decrease/early stopping mechanism to the missclassification error (1− accuracy) rather than the loss function.• The values of stalled-epochs-before-X are much higher for the supervised learning scenarios, since the training sets are much smaller (e.g., when using 1,000 examples and a batch size of 100, a single epoch only results in 10 gradient update steps). • It is possible that the results for supervised training with fine tuning improve if the encoder weights are regularized as well (the weights are prone to overfitting, since we use a low number of training examples).
This paper studies sparse linear regression problems of the form y = Φx0 + w, where x0 ∈ Rn is the unknown vector to estimate, supposed to be non-zero and sparse, w ∈ Rm is some additive noise and the design matrix Φm×n is in general rank deficient corresponding to a noisy underdetermined linear system of equations, i.e., typically in the high-dimensional regime where m n. This can also be understood as an inverse problem in imaging sciences, a particular instance of which being the compressed sensing problem [3], where the matrix Φ is drawn from some appropriate random matrix ensemble. In order to recover a sparse vector x0, a popular regularization is the `1-norm, in which case we consider the following constrained sparsity-promoting optimization problem min x∈Rn {||x||1 s.t. ||Φx− y||α 6 τ} , (Pτα(y)) 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. where for α ∈ [1,+∞], ||u||α def.= ( ∑ i |ui|α) 1/α denotes the `α-norm, and the constraint size τ > 0 should be adapted to the noise level. To avoid trivialities, through the paper, we assume that problem (Pτα(y)) is feasible, which is of course the case if τ > ||w||α. In the special situation where there is no noise, i.e., w = 0, it makes sense to consider τ = 0 and solve the so-called Lasso [14] or Basis-Pursuit problem [4], which is independent of α, and reads min x {||x||1 s.t. Φx = Φx0} . (P0(Φx0)) The case α = 2 corresponds to the usual `2 loss function, which entails a smooth constraint set, and has been studied in depth in the literature (see Section 1.6 for an overview). In contrast, the cases α ∈ {1,+∞} correspond to very different setups, where the loss function || · ||α is polyhedral and non-smooth. They are expected to lead to significantly different estimation results and require to develop novel theoretical results, which is the focus of this paper. The case α = 1 corresponds to a “robust” loss function, and is important to cope with impulse noise or outliers contaminating the data (see for instance [11, 13, 9]). At the extreme opposite, the case α = +∞ is typically used to handle uniform noise such as in quantization (see for instance [10]). This paper studies the stability of the support supp(xτ ) of minimizers xτ of (Pτα(y)). In particular, we provide a sharp analysis for the polyhedral cases α ∈ {1,+∞} that allows one to control the deviation of supp(xτ ) from supp(x0) if ||w||α is not too large and τ is chosen proportionally to ||w||α. The general case is studied numerically in a compressed sensing experiment where we compare supp(xτ ) and supp(x0) for α ∈ [1,+∞].The support of x0 is noted I def. = supp(x0) where supp(u) def. = {i | ui 6= 0}. The saturation support of a vector is defined as sat(u) def.= {i | |ui| = ||u||∞ }. The sub-differential of a convex function f is denoted ∂f . The subspace parallel to a nonempty convex set C is par(C) def.= R(C − C). A∗ is the transpose of a matrix A and A+ is the Moore-Penrose pseudo-inverse of A. Id is the identity matrix and δi the canonical vector of index i. For a subspace V ⊂ Rn, PV is the orthogonal projector onto V . For sets of indices S and I , we denote ΦS,I the submatrix of Φ restricted to the rows indexed by S and the columns indexed by I . When all rows or all columns are kept, a dot replaces the corresponding index set (e.g., Φ·,I ). We denote Φ∗S,I def. = (ΦS,I) ∗, i.e. the transposition is applied after the restriction.Before diving into our theoretical contributions, we first give important definitions. Let Dx0 be the set of dual certificates (see, e.g., [17]) defined by Dx0 def. = {p ∈ Rm | Φ∗p ∈ ∂||x0||1 } = { p ∈ Rm ∣∣ Φ∗·,Ip = sign(x0,I), ||Φ∗p||∞ 6 1} . (1) The first order optimality condition (see, e.g., [12]) states that x0 is a solution of (P0(Φx0)) if and only if Dx0 6= ∅. Assuming this is the case, our main theoretical finding (Theorem 1) states that the stability (and instability) of the support of x0 is characterized by the following specific subset of certificates pβ ∈ Argmin p∈Dx0 ||p||β where 1α + 1β = 1. (2) We call such a certificate pβ a minimum norm certificate. Note that for 1 < α < +∞, this pβ is actually unique but that for α ∈ {1,∞} it might not be the case. Associated to such a minimal norm certificate, we define the extended support as J def. = sat(Φ∗pβ) = {i ∈ {1, . . . , n} | |(Φ∗pβ)i| = 1} . (3) When the certificate pβ from which J is computed is unclear from the context, we write it explicitly as an index Jpβ . Note that, from the definition of Dx0 , one always has I ⊆ J . Intuitively, J indicates the set of indexes that will be activated in the signal estimate when a small noise w is added to the observation, and thus the situation when I = J corresponds to the case where the support of x0 is stable.In the case of noiseless observations (w = 0) and when τ > 0, the following general lemma whose proof can be found in Section 2 associate to a given dual certificate pβ an explicit solution of (Pτα(Φx0)). This formula depends on a so-called Lagrange multiplier vector vβ ∈ Rn, which will be instrumental to state our main contribution (Theorem 1). Note that this lemma is valid for any α ∈ [1,∞]. Even though this goes beyond the scope of our main result, one can use the same lemma for an arbitrary `α-norm for α ∈ [1,∞] (see Section 3) or for even more general loss functions. Lemma 1 (Noiseless solution). We assume that x0 is identifiable, i.e. it is a solution to (P0(Φx0)), and consider τ > 0. Then there exists a vβ ∈ Rn supported on J such that Φ·,Jvβ,J ∈ ∂||pβ ||β and − sign(vβ,J̃) = Φ∗·,J̃pβ where we denoted J̃ def.= J\I . If τ is such that 0 < τ < x||vβ,I ||∞ , with x = mini∈I |x0,I |, then a solution x̄τ of (Pτα(Φx0)) with support equal to J is given by x̄τ,J = x0,J − τvβ,J . Moreover, its entries have the same sign as those of x0 on its support I , i.e., sign(x̄τ,I) = sign(x0,I). An important question that arises is whether vβ can be computed explicitly. For this, let us define the model tangent subspace Tβ def. = par(∂||pβ ||β)⊥, i.e., Tβ is the orthogonal to the subspace parallel to ∂||pβ ||β , which uniquely defines the model vector, eβ def.= PTβ∂||pβ ||β , as shown on Figure 1 (see [17] for details). Using this notation, vβ,J is uniquely defined and expressed in closed-form as vβ,J = (PTβΦ·,J) +eβ (4) if and only if the following restricted injectivity condition holds Ker(PTβΦ·,J) = {0}. (INJα) For the special case (α, β) = (∞, 1), the following lemma, proved in Section 2, gives easily verifiable sufficient conditions, which ensure that (INJ∞) holds. The notation S def. = supp(p1) is used. Lemma 2 (Restricted injectivity for α =∞). Assume x0 is identifiable and ΦS,J has full rank. If sJ /∈ Im(Φ∗S′,J) ∀S′ ⊆ {1, . . . ,m}, |S′| < |J | and qS /∈ Im(ΦS,J ′) ∀J ′ ⊆ {1, . . . , n}, |J ′| < |S|, where sJ = Φ∗·,Jp1 ∈ {−1, 1}|J|, and qS = sign(p1,S) ∈ {−1, 1}|S|, then, |S| = |J | and ΦS,J is invertible, i.e., since PT1Φ·,J = Id·,SΦS,J , (INJ∞) holds. Remark 1. If Φ is randomly drawn from a continuous distribution with i.i.d. entries, e.g., Gaussian, then as soon as x0 is identifiable, the conditions of Lemma 2 hold with probability 1 over the distribution of Φ. For (α, β) = (1,∞), we define Z def.= sat(p∞), Θ def. = [ IdZc,· sign(p∗∞,Z)IdZ,· ] and Φ̃ def.= ΘΦ·,J . Following similar reasoning as in Lemma 2 and Remark 1, we can reasonably assume that |Zc|+ 1 = |J | and Φ̃ is invertible. In that case, (INJ1) holds as Ker(PT∞Φ·,J) = Ker(Φ̃). Table 1 summarizes for the three specific cases α ∈ {1, 2,+∞} the quantities introduced here.Our main contribution is Theorem 1 below. A similar result is known to hold in the case of the smooth `2 loss (α = 2, see Section 1.6). Our paper extends it to the more challenging case of non-smooth losses α ∈ {1,+∞}. The proof for α = +∞ is detailed in Section 2. It is important to emphasize that the proof strategy is significantly different from the classical approach developed for α = 2, mainly because of the lack of smoothness of the loss function. The proof for α = 1 follows a similar structure, and due to space limitation, it can be found in the supplementary material. Theorem 1. Let α ∈ {1, 2,+∞}. Suppose that x0 is identifiable, and let pβ be a minimal norm certificate (see (2)) with associated extended support J (see (3)). Suppose that the restricted injectivity condition (INJα) is satisfied so that vβ,J can be explicitly computed (see (4)). Then there exist constants c1, c2 > 0 depending only on Φ and pβ such that, for any (w, τ) satisfying ||w||α < c1τ and τ 6 c2x where x def.= min i∈I |x0,I |, (5) a solution xτ of (Pτα(Φx0 + w)) with support equal to J is given by xτ,J def. = x0,J + (PTβΦ·,J) +w − τvβ,J . (6) This theorem shows that if the signal-to-noise ratio is large enough and τ is chosen in proportion to the noise level ||w||α , then there is a solution supported exactly in the extended support J . Note in particular that this solution (6) has the correct sign pattern sign(xτ,I) = sign(x0,I), but might exhibit outliers if J̃ def.= J\I 6= ∅. The special case I = J characterizes the exact support stability (“sparsistency”), and in the case α = 2, the assumptions involving the dual certificate correspond to a condition often referred to as “irrepresentable condition” in the literature (see Section 1.6). In Section 3, we propose numerical simulations to illustrate our theoretical findings on a compressed sensing (CS) scenario. Using Theorem 1, we are able to numerically assess the degree of support instability of CS recovery using `α fidelity. As a prelude to shed light on this result, we show on Figure 2, a smaller simulated CS example for (α, β) = (∞, 1). The parameters are n = 20, m = 10 and |I| = 4 and x0 and Φ are generated as in the experiment of Section 3 and we use CVX/MOSEK [8, 7] at best precision to solve the optimization programs. First, we observe that x0 is indeed identifiable by solving (P0(Φx0)). Then we solve (2) to compute pβ and predict the extended support J . Finally, we add uniformly distributed noise w with wi ∼i.i.d. U(−δ, δ) and δ chosen appropriately to ensure that the hypotheses hold and we solve (Pτα(y)). Observe that as we increase τ , new non-zero entries appear in xτ but because w and τ are small enough, as predicted, we have supp(xτ ) = J . Let us now comment on the limitations of our analysis. First, this result does not trivially extend to the general case α ∈ [1,+∞] as there is, in general, no simple closed form for xτ . A generalization would require more material and is out of the scope of this paper. Nevertheless, our simulations in Section 3 stand for arbitrary α ∈ [1,+∞] which is why the general formulation was presented. Second, larger noise regime, though interesting, is also out of the scope. Let us note that no other results in the literature (even for `2) provide any insight about sparsistency in the large noise regime. In that case, we are only able to provide bounds on the distance between x0 and the recovered vector but this is the subject of a forthcoming paper. Finally our work is agnostic with respect to the noise models. Being able to distinguish between different noise models would require further analysis of the constant involved and some additional constraint on Φ. However, our result is a big step towards the understanding of the solutions behavior and can be used in this analysis.To the best of our knowledge, Theorem 1 is the first to study the support stability guarantees by minimizing the `1-norm with non-smooth loss function, and in particular here the `1 and `∞ losses. The smooth case α = 2 is however much more studied, and in particular, the associated support stability results we state here are now well understood. Note that most of the corresponding literature studies in general the penalized form, i.e., minx 12 ||Φx − y||2 + λ||x||1 instead of our constrained formulation (Pτα(y)). In the case α = 2, since the loss is smooth, this distinction is minor and the proof is almost the same for both settings. However, for α ∈ {1,+∞}, it is crucial to study the constrained problems to be able to state our results. The support stability (also called “sparsistency”, corresponding to the special case I = J of our result) of (Pτα(y)) in the case α = 2 has been proved by several authors in slightly different setups. In the signal processing literature, this result can be traced back to the early work of J-J. Fuchs [6] who showed Theorem 1 when α = 2 and I = J . In the statistics literature, sparsistency is also proved in [19] in the case where Φ is random, the result of support stability being then claimed with high probability. The condition that I = J , i.e., that the minimal norm certificate pβ (for α = β = 2) is saturating only on the support, is often coined the “irrepresentable condition” in the statistics and machine learning literature. These results have been extended recently in [5] to the case where the support I is not stable, i.e. I ( J . One could also cite [15], whose results are somewhat connected but are restricted to the `2 loss and do not hold in our case. Note that “sparsistency”-like results have been proved for many “low-complexity” regularizers beyond the `1-norm. Let us quote among others: the group-lasso [1], the nuclear norm [2], the total variation [16] and a very general class of “partly-smooth” regularizers [17]. Let us also point out that one of the main sources of application of these results is the analysis of the performance of compressed sensing problems, where the randomness of Φ allows to derive sharp sample complexity bounds as a function of the sparsity of x0 and n, see for instance [18]. Let us also stress that these support recovery results are different from those obtained using tools such as the Restricted Isometry Property and alike (see for instance [3]) in many respects. For instance, the guarantees they provide are uniform (i.e., they hold for any sparse enough vector x0), though they usually lead to quite pessimistic worst-case bounds, and the stability is measured in `2 sense.In this section, we prove the main result of this paper. For the sake of brevity, when part of the proof will become specific to a particular choice of α, we will only write the details for α =∞. The details of the proof for α = 1 can be found in the supplementary material. It can be shown that the Fenchel-Rockafellar dual problem to (Pτα(y)) is [12] min p∈Rm {−〈y, p〉+ τ ||p||β s.t. ||Φ∗p||∞ 6 1} . (Dτβ(y)) From the corresponding (primal-dual) extremality relations, one can deduce that (x̂, p̂) is an optimal primal-dual Kuhn-Tucker pair if, and only if, Φ∗·,Î p̂ = sign(x̂Î) and ||Φ ∗p̂||∞ 6 1. (7) where Î = supp(x̂), and y − Φx̂ τ ∈ ∂||p̂||β . (8) The first relationship comes from the sub-differential of the `1 regularization term while the second is specific to a particular choice of α for the `α-norm data fidelity constraint. We start by proving the Lemma 1 and Lemma 2. Proof of Lemma 1 Let us rewrite the problem (2) by introducing the auxiliary variable η = Φ∗p as min p,η {||p||β + ιB∞(η) | η = Φ∗p, ηI = sign(x0,I)} , (9) where ιB∞ is the indicator function of the unit `∞ ball. Define the Lagrange multipliers v and zI and the associated Lagrangian function L(p, η, v, zI) = ||p||β + ιB∞(η) + 〈v, η − Φ∗p〉+ 〈zI , ηI − sign(x0,I)〉. Defining zIc = 0, the first order optimality conditions (generalized KKT conditions) for p and η read Φv ∈ ∂||p||β and − v − z ∈ ∂ιB∞(η), From the normal cone of the B∞ at η on its boundary, the second condition is −v − z ∈ {u | uJc = 0, sign(uJ) = ηJ } , where J = sat(η) = sat(Φ∗p). Since I ⊆ J , v is supported on J . Moreover, on J̃ = J\I , we have − sign(vJ̃) = ηJ̃ . As pβ is a solution to (9), we can define a corresponding vector of Lagrange multipliers vβ supported on J such that − sign(vβ,J̃) = Φ∗·,J̃pβ and Φ·,Jvβ,J ∈ ∂||pβ ||β . To prove the lemma, it remains to show that x̄τ is indeed a solution to (Pτα(y)), i.e., it obeys (7) and (8) for some dual variable p̂. We will show that this is the case with p̂ = pβ . Observe that pβ 6= 0 as otherwise, it would mean that x0 = 0, which contradicts our initial assumption of non-zero x0. We can then directly see that (8) is satisfied. Indeed, noting y0 def. = Φx0, we can write y0 − Φ·,J x̄τ,J = τΦ·,Jvβ,J ∈ τ∂||pβ ||β . By definition of pβ , we have ||Φ∗pβ ||∞ 6 1. In addition, it must satisfy Φ∗·,Jpβ = sign(x̄τ,J).Outside I , the condition is always satisfied since − sign(vβ,J̃) = Φ∗·,J̃pβ . On I , we know that Φ ∗ ·,Ipβ = sign(x0,I). The condition on τ is thus |x0,i| > τ |vβ,i| ,∀i ∈ I , or equivalently, τ < x||vβ,I ||∞ . Proof of Lemma 2 As established by Lemma 1, the existence of p1 and of v1 are implied by the identifiability of x0. We have the following, ∃p1 ⇒ ∃pS ,Φ∗S,JpS = sJ ⇔ Φ∗S,J is surjective⇔ |S| > |J | ∃v1 ⇒ ∃vJ ,ΦS,JvJ = qS ⇔ ΦS,J is surjective⇔ |J | > |S|, To clarify, we detail the first line. Since Φ∗S,J is full rank, |S| > |J | is equivalent to surjectivity. Assume Φ∗S,J is not surjective so that |S| < |J |, then sJ /∈ Im(Φ∗S,J) and the over-determined system Φ∗S,JpS = sJ has no solution in pS , which contradicts the existence of p1. Now assume Φ ∗ S,J is surjective, then we can take pS = Φ ∗,† S,JsJ as a solution where Φ ∗,† S,J is any right-inverse of Φ ∗ S,J . This proves that ΦS,J is invertible. We are now ready to prove the main result in the particular case α =∞. Proof of Theorem 1 (α =∞) Our proof consists in constructing a vector supported on J , obeying the implicit relationship (6) and which is indeed a solution to (Pτ∞(Φx0 + w)) for an appropriate regime of the parameters (τ, ||w||α). Note that we assume that the hypothesis of Lemma 2 on Φ holds and in particular, ΦS,J is invertible. When (α, β) = (∞, 1), the first order condition (8), which holds for any optimal primal-dual pair (x, p), reads, with Sp def. = supp(p), ySp − ΦSp,·x = τ sign(pSp) and ||y − Φx||∞ 6 τ. (10) One should then look for a candidate primal-dual pair (x̂, p̂) such that supp(x̂) = J and satisfying ySp̂ − ΦSp̂,J x̂J = τ sign(p̂Sp̂). (11) We now need to show that the first order conditions (7) and (10) hold for some p = p̂ solution of the “perturbed” dual problem (Dτ1 (Φx0 + w)) with x = x̂. Actually, we will show that under the conditions of the theorem, this holds for p̂ = p1, i.e., p1 is solution of (Dτ1 (Φx0 + w)) so that x̂J = Φ −1 S,JyS − τΦ−1S,J sign(p1,S) = x0,J + Φ−1S,JwS − τv1,J . Let us start by proving the equality part of (7), Φ∗S,J p̂S = sign(x̂J). Since ΦS,J is invertible, we have p̂S = p1,S if and only if sign(x̂J) = Φ∗S,Jp1,S . Noting IdI,J the restriction from J to I , we have sign ( x0,I + IdI,JΦ −1 S,JwS − τv1,I ) = sign (x0,I) as soon as ∣∣∣(Φ−1S,JwS) i − τv1,i ∣∣∣ < |x0,I | ∀i ∈ I. It is sufficient to require ||IdI,JΦ−1S,JwS − τv1,I ||∞ < x ||Φ−1S,J ||∞,∞||w||∞ + τ ||v1,I ||∞ < x, with x = mini∈I |x0,I |. Injecting the fact that ||w||∞ < c1τ (the value of c1 will be derived later), we get the condition τ (bc1 + ν) 6 x, with b = ||Φ−1S,J ||∞,∞ and ν = ||v1||∞ 6 b. Rearranging the terms, we obtain τ 6 x bc1 + ν = c2x, which guarantees sign(x̂I) = sign(x0,I). Outside I , defining IdJ̃,J as the restriction from J to J̃ , we must have Φ∗ S,J̃ p1,S = sign ( IdJ̃,JΦ −1 S,JwS − τv1,J̃ ) . From Lemma 1, we know that − sign(v1,J̃) = Φ∗S,J̃p1,S , so that the condition is satisfied as soon as∣∣∣∣(Φ−1S,JwS) j ∣∣∣∣ < τ |v1,j | ∀j ∈ J̃ . Noting v = minj∈J̃ |v1,j |, we get the sufficient condition for (7), ||Φ−1S,JwS ||∞ < τv, ||w||∞ < τ v b . (c1a) We can now verify (10). From (11) we see that the equality part is satisfied on S. Outside S, we have ySc − ΦSc,·x̂ = wSc − ΦSc,JΦ−1S,JwS + τΦSc,Jv1,J , which must be smaller than τ , i.e., ||wSc − ΦSc,JΦ−1S,JwS + τΦSc,Jv1,J ||∞ 6 τ. It is thus sufficient to have (1 + ||ΦSc,JΦ−1S,J ||∞,∞)||w||∞ + τµ 6 τ, with µ def.= ||ΦSc,Jv1,J ||∞. Noting a = ||ΦSc,JΦ−1S,J ||∞,∞, we get ||w||∞ 6 1− µ 1 + a τ. (c1b) (c1a) and (c1b) together give the value of c1. This ensures that the inequality part of (10) is satisfied for x̂ and with that, that x̂ is solution to (Pτ∞(Φx0 + w)) and p1 solution to (Dτ1 (Φx0 + w)), which concludes the proof. Remark 2. From Lemma 1, we know that in all generality µ 6 1. If the inequality was saturated, it would mean that c1 = 0 and no noise would be allowed. Fortunately, it is easy to prove that under a mild assumption on Φ, similar to the one of Lemma 2 (which holds with probability 1 for Gaussian matrices), the inequality is strict, i.e., µ < 1.In order to illustrate support stability in Lemma 1 and Theorem 1, we address numerically the problem of comparing supp(xτ ) and supp(x0) in a compressed sensing setting. Theorem 1 shows that supp(xτ ) does not depend on w (as long as it is small enough); simulations thus do not involve noise. All computations are done in Matlab, using CVX [8, 7], with the MOSEK solver at “best” precision setting to solve the convex problems. We set n = 1000, m = 900 and generate 200 times a random sensing matrix Φ ∈ Rm×n with Φij ∼i.i.d N (0, 1). For each sensing matrix, we generate 60 different k-sparse vectors x0 with support I where k def. = |I| varies from 10 to 600. The non-zero entries of x0 are randomly picked in {±1} with equal probability. Note that this choice does not impact the result because the definition of Jpβ only depends on sign(x0) (see (1)). It will only affect the bounds in (5). For each case, we verify that x0 is identifiable and for α ∈ {1, 2,∞} (which correspond to β ∈ {∞, 2, 1}), we compute the minimum `β-norm certificate pβ , solution to (2) and in particular, the support excess J̃pβ def. = sat(Φ∗pβ)\I . It is important to emphasize that there is no noise in these simulations. As long as the hypotheses of the theorem are satisfied, we can predict that supp(xτ ) = Jpβ ⊂ I without actually computing xτ , or choosing τ , or generating w. We define a support excess threshold se ∈ N varying from 0 to∞. On Figure 3 we plot the probability that x0 is identifiable and |J̃pβ |, the cardinality of the predicted support excess, is smaller or equal to se. It is interesting to note that the probability that |J̃p1 | = 0 (the bluest horizontal curve on the right plot) is 0, which means that even for extreme sparsity (k = 10) and a relatively high m/n rate of 0.9, the support is never predicted as perfectly stable for α =∞ in this experiment. We can observe as a rule of thumb, that a support excess of |J̃p1 | ≈ k is much more likely. In comparison, `2 recovery provides a much more likely perfect support stability for k not too large and the expected size of J̃p2 increases slower with k. Finally, we can comment that the support stability with `1 data fidelity is in between. It is possible to recover the support perfectly but the requirement on k is a bit more restrictive than with `2 fidelity. As previously noted, Lemma 1 and its proof remain valid for smooth loss functions such as the `α-norm when α ∈ (1,∞). Therefore, it makes sense to compare the results with the ones obtained for α ∈ (1,∞) . On Figure 4 we display the result of the same experiment but with 1/α as the vertical axis. To realize the figure, we compute pβ and J̃pβ for β corresponding to 41 equispaced values of 1/α ∈ [0, 1]. The probability that |J̃pβ | 6 se is represented by the color intensity. The three different plots correspond to three different values for se. On this figure, the yellow to blue transition can be interpreted as the maximal k to ensure, with high probability, that |J̃pβ | does not exceeds se. It is always (for all se) further to the right at α = 2. It means that the `2 data fidelity constraint provides the highest support stability. Interestingly, we can observe that this maximal k decreases gracefully as α moves away from 2 in one way or the other. Finally, as already observed on Figure 3, we see that, especially when se is small, the `1 loss function has a small advantage over the `∞ loss.In this paper, we provided sharp theoretical guarantees for stable support recovery under small enough noise by `1 minimization with non-smooth loss functions. Unlike the classical setting where the data loss is smooth, our analysis reveals the difficulties arising from non-smoothness, which necessitated a novel proof strategy. Though we focused here on the case of `α data loss functions, for α ∈ {1, 2,∞}, our analysis can be extended to more general non-smooth losses, including coercive gauges. This will be our next milestone.KD and LJ are funded by the Belgian F.R.S.-FNRS. JF is partly supported by Institut Universitaire de France. GP is supported by the European Research Council (ERC project SIGMA-Vision).
Recent machine learning models have achieved great success in applications such as self-driving cars (Bojarski et al., 2016) and medical diagnosis (Litjens et al., 2017). In a classification problem, we sometimes face the situation where the misclassification of a specific label can result in serious accidents. This problem arises in various situations, and below we provide examples. Example 1 (Object Detection for Self-Driving Cars). For self-driving cars, object detection is crucial. For protecting the life of pedestrians, self-driving cars need to be able to detect human or suspend decision making if it detects an object that might be a human. Example 2 (Medical Diagnosis). Cancer is one of the leading causes of death globally, and machine learning algorithms for detecting cancer from images have been gaining attention. This is done by detecting abnormalities in an image that might indicate cancer. As shown above, we want the labeling of data to be conservative for certain situations; for example, in autonomous driving, it is better to misclassify a non-human object as a human than to risk misclassifying a human. In this paper, we introduce two novel frameworks called learning with rejection under adversarial attacks and learning with protection to protect a specific class from critical misclassification. Our algorithm is based on the techniques of learning with rejection and adversarial examples. Learning with rejection under adversarial attacks is a naive extension of learning with rejection to hold a decision when we face a suspicious sample that is vulnerable to adversarial attack. Learning with protection is an application of learning with rejection under adversarial attack. The purpose is to protect a specific class from misclassification. Both frameworks are based on the existing work of learning with rejection and adversarial examples. Learning with rejection is a classification scenario where the learner is given the option to reject an instance instead of predicting its label. The purpose of this framework is to prevent critical misclassification, where rejecting labeling a datum incurs a lower cost than misclassification. In this field, considerable theoretical and empirical analysis has been conducted (Bartlett & Wegkamp, 2008; Cortes et al., 2016a;b; Grandvalet et al., 2009; Herbei & Wegkamp, 2006; Yuan & Wegkamp, 2010). Learning with rejection is one ultimate solution against data with high uncertainty. However, the existing frameworks have not considered adversarial examples, which refers to a well-designed attack to mislead people to misclassification. In the industrial application of machine learning, it has been observed that many machine learning models are vulnerable to adversarial attacks. Threat model of adversarial attacks specifies the capabilities of the adversary and classifies adversary attacks to two types of attacks, white-box attack and black-box attack. Adversarial inputs that result in machine learning models returning incorrect outputs are called adversarial examples. Several previous studies have artificially generated adversarial examples by adding small perturbations that are imperceptible to humans (Goodfellow et al., 2015; Gu & Rigazio, 2014; Huang et al., 2015; Carlini & Wagner, 2017). Methods for protecting against these adversarial examples are also being proposed. Among them, adversarial training is one of the most effective defenses (Szegedy et al., 2014; Goodfellow et al., 2015; Shaham et al., 2015; Carlini & Wagner, 2016; Papernot et al., 2016; Xu et al., 2017; Madry et al., 2018; Buckman et al., 2018; Kannan et al., 2018; Pang et al., 2018; Wong & Kolter, 2018; Tramèr et al., 2018). However, most of the defense method fails to avoid misclassification under adversarial attacks. Carlini & Wagner (2017) defeated representative methods for detection of adversarial examples. Athalye et al. (2018) reports that several defense algorithms for white-box setting fail when the attacker uses a carefully designed gradient-based method. Besides, Shafahi et al. (2019); Gilmer et al. (2019) shows that adversarial examples are inevitable in some cases. However, we sometimes face situations in which misclassification can result in fatal accidents. For that purpose, we propose learning with rejection option under adversarial attack. The main idea of our method is to hold a decision when we get suspicious samples. As an application of learning with a rejection option, we propose a method for preventing the misclassification of data for a specific label under adversarial examples. This method considers situations where we must protect our model from adversarial examples by any means. In addition, It should be noted that our method can be combined with other existing methods such as adversarial training. In the following section, we describe the problem setting. In Sections 3 and 4, we propose and describe our algorithm, and in Section 5, we show the experimental results.We use the standard settings for binary classification problems, i.e., training and test data points are i.i.d. and sampled from some unknown distribution D over X × {−1,+1}, where X denotes the input space. The goal is to learn a classifier h : X → {−1,+1} that assigns a label ŷ(x) to a datum x as ŷ(x) = sign(h(x)). Let L be a loss function L : R× {−1,+1} → R. The optimal classifier h∗ is given by h∗ = argminh∈F E[L(h(X), Y )], where F is a set of measurable functions. When we train a classifier with training samples, we can naively replace the expectation with the corresponding sample averages. For a hypothesis set H, let us define an estimator of the optimal classifier h∗ as ĥ = argminh∈H Ê[L(h(X), Y )], where Ê denotes the averaging operator over training data. Although an estimator ĥ converges to h∗ in many cases with infinite samples, an estimator ĥ might return result that differ greatly from h∗ in a case with finite samples. In addition, when h(x) is not smooth at around x, it might become difficult to estimate the function. These can cause serious problems in real-world applications, such as traffic accidents arising from misclassification by the algorithms used in self-driving cars. To make our algorithm’s inference more robust, we allow a learner to reject such suspicious samples. Let R© denote rejection. For any given instance x ∈ X , the learner has the option of abstaining from assigning a label or rejecting that instance and returning the symbol R©, or assigning the label ŷ ∈ {−1,+1}. If the learner rejects an instance, then they incur some loss c(x) ∈ R; if it does not reject but assigns an incorrect label, then it incurs a cost of one; otherwise, it suffers no loss. Thus, the learner outputs a rejection function r : X → R, that determines the points x ∈ X to be rejected according to r(x) ≤ 0. Let us denote a loss function with rejection option L R© : R× {−1,+1} → R. For example, a loss function can be defined as follows: L R©(h(x), y) = 1yh(x)≤01r(x)≥0 + c(x)1r(x)≤0. There are two frameworks used in the learning with rejection model: the confidence-based and separation-based approaches Cortes et al. (2016a). The separation-based approach is first formulated by Cortes et al. (2016a). This formulation is a generalization of confidence-based approach (Bartlett & Wegkamp, 2008). In the separation-based approach, we train a classifier and a rejection function simultaneously. In confidence-based approach, given the conditional probability p(y = +1|x) and a cost function c(x), we only train a classifier and obtain a rejection function automatically after training a classifier. Although the confidence-based approach is a special case of the separation-based approach, the method of confidence-based approach is easy to implement. In this paper, we follow the confidence-based approach and correspond the cost function to vulnerability for adversarial examples. Our goal is to obtain the optimal classifier h∗, which minimizes the classification risk with rejection option, i.e., h∗ = argminh∈H E[L R©(h(X), Y )].In this section, we describe a method to hold a decision against suspicious samples.To reject suspicious samples, we define which samples are suspicious. We introduce a concept of a p-norm suspicious sample. Let ‖ · ‖p be `p-norm. We refer a sample x as suspicious based on the value of `p-norm if there exits a sample x′ such that ‖x − x′‖p ≤ ε and ĥ(x′) returns a different class from ĥ(x). If a sample is suspicious, we refrain from assigning a class label. Suppose that we have a estimator f̂(x) of the class conditional probability η(x) = p(y = +1|x) and a classifier ĥ(x) = sign ( f̂(x)− 12 ) . Let B∞x ( ) denote the `p ball centered at x ∈ X with radius , i.e., Bpx( ) = {x′ ∈ X : ‖x′ − x‖p ≤ }. We calculate z∗(x) ∈ Bpx( ) as follows: z∗(x) = argmax x′∈Bpx( ) L̃(ĥ(x′),x). By defining z∗(x) as in the above equation, we can easily calculate it with the techniques used in adversarial examples. If ĥ(z∗(x))ĥ(x) ≤ 0 for a sample x, we regard the sample as suspicious one. We illustrate the concept in Figure 1.To interpret the above strategy, we consider the relationship between the strategy and learning with rejection. First, we consider the ideal situation, where we know the true value of the conditional class probability η(x) = p(y = +1|x). In confidence-based approach, we can determine the rejection function r(x) corresponding to the rejection cost as shown by Cortes et al. (2016a). It is known that the classifier h∗ defined for any x ∈ X by h∗(x) = η(x) − 12 is optimal. For any x ∈ X , the misclassification cost for h∗ is E [ 1yh(x)≤0|x ] = min{η(x), 1− η(x)}. The optimal rejection function r∗(x) should therefore be defined such that r∗(x) ≤ 0 if and only if min{η(x), 1− η(x)} ≥ c(x)⇔ 1−max{η(x), 1− η(x)} ≥ c(x)⇔ |h(x)| ≤ 1 2 − c(x). Therefore, the optimal rejection function r∗(x) is given as r∗(x) = |η(x)− 12 | − ( 1 2 − c(x) ) . Thus, in confidence-based approach, we can determine the optimal rejection function r∗(x) if we know the conditional probability η(x) and the rejection cost c(x). To reject suspicious samples, we relate the rejection cost c(x) with uncertainty of data x. One naive idea is to set c(x) as a monotonically decreasing function representing the smoothness of η(x). When |η(x)− η(x′)| is zero for x,x′ ∈ Rd, c(x) takes its largest value; when max |f(x)− f(x′)| is large, c(x) is small. This means that, if a sample x is suspicious, then the cost of rejection is low, and we can reject the sample easily; conversely, if a sample x is not suspicious, then the cost of rejection is high. When the cost is high, we hesitate to reject the sample. We assumed this condition could be met when designing our algorithm. As we show later, we can easily find such c(x) and reject suspicious samples without expending much energy in determining the value of c(x). In our strategy, we reject a sample x such that ( η(z∗(x))− 1 2 )( η(x)− 1 2 ) ≤ 0. Therefore, r∗(x) ≤ 0 if and only if( η(z∗(x)) ≤ 1 2 and η(x) ≥ 1 2 ) or ( η(z∗(x)) ≥ 1 2 and η(x) ≤ 1 2 ) ⇔ ( η(x)− η(x) + η(z∗(x)) ≤ 1 2 and η(x) ≥ 1 2 ) or ( η(x)− η(x) + η(z∗(x)) ≥ 1 2 and η(x) ≤ 1 2 ) ⇔ ( 0 ≤ η(x)− 1 2 ≤ η(x)− η(z∗(x)) ) or ( η(x)− η(z∗(x)) ≤ η(x)− 1 2 ≤ 0 ) (1) Let us assume that η(x) − η(z∗(x)) ≥ 0 when η(x) − 12 ≥ 0 and η(x) − η(z ∗(x)) ≤ 0 when η(x)− 12 ≤ 0. Then, (1) insists that r ∗(x) ≤ 0 if and only if |h(x)| ≤ |η(x)−η(z∗(x))|. Therefore, the following relationship holds for a sample x such that |η(x)− η(z∗(x))| ≤ 12 : c(x) = 1 2 − |η(x)− η(z∗(x))|. Thus, in our strategy, the cost function is a decreasing function of the smoothness of η(x). Next, we show a relationship between the cost function and the degree of perturbation ε. Let us assume η(x) is Lipschitz continuous, with `p-norm, and |η(x)− η(x′)| ≤ λ‖x− x′‖p = λε, wherex ∈ X , x′ ∈ Bpx( ), and λ > 0 is a positive constant. From Lipschitz continuity, c(x) ≥ 12−λε holds. This means that the cost of rejection will be small when ε is a large value. Therefore, a learner can reject a sample with low cost when the learner is afraid of the misclassification and set the possible perturbation of x with `p-norm large.Based on the above idea, we develop an algorithm for rejecting suspicious samples. To discuss learning the rejection in the framework of confidence-based approach, we construct an estimator f̂ of η(x) = p(y = +1|x) and a classifier ĥ(x) = f̂(x) − 12 . Then, we define pseudo loss function L̃ for two samples x ∈ X and x′ ∈ Bpx( ) as a binary loss function for a pair of (x′, ŷ(x), where ŷ(x) = sign(ĥ(x)). For example, if we assume logistic loss, it can be defined as L̃(f̂(x′), ŷ(x)) = −1ŷ(x)=−1 log ( f̂(x) ) − 1ŷ(x)=+1 log ( 1− f̂(x) ) . Using the pseudo loss, we calculate z∗(x) for a sample x as follows: z∗(x) = argmax x′∈Bpx( ) L̃(ĥ(x′), ŷ(x)). Because this optimization is hard to calculate, we apply techniques of adversarial examples as a heuristics. Then, we reject a sample x if and only if ĥ(z∗(x))ĥ(x) ≤ 0. Calculation of Adversarial Examples: Here, we describe the two simple first-order methods for calculating adversarial examples used in this paper. Although various types of adversarial examples have been proposed, hereinafter adversarial examples refer to perturbation-based methods. One of the simplest methods for generating adversarial examples is the fast gradient sign method (FGSM) Goodfellow et al. (2015), which is a fast single-step attack that maximizes the loss function in the linear approximation. The perturbation under the FGSM is calculated as follows: δ = ε · sign(∇xL(h(x), y)). The projected gradient descent method (PGD) Madry et al. (2018) is an iterative variant of the FGSM. The perturbation under the PGD at time step s+ 1 is calculated as follows: δ(s+1) = Pε(δ(s) + α · sign(∇xL(h(x+ δ(s)), t))), where α denotes a single step and Pε(·) denotes the projection onto the `p-ball with radius ε.In this section, we propose a practical and easily implemented algorithm as an application of learning with rejection under adversarial attack. In many cases, we want to avoid misclassification for a specific class. In self-driving cars, we must be able to detect human beings to avoid serious accidents. In medical diagnosis, we should not miss a dangerous disease. We thus propose an algorithm to protect a specific class from misclassification in a multi-class classification problem. We extend the previous problem setting of binary classification to that of multi-class classification. Let X and Y be the feature and label spaces, respectively, and suppose that there is an unknown distributionD overX×Y . We assume that the label space hasK ≥ 2 labels, i.e., Y = {1, 2, 3, ...,K}. We define the class for which we want to avoid misclassification as the defense target class t ∈ Y . For example, if we want to avoid misclassification of 1, we designate the defense target class as t = 1. We call the algorithm for protecting a defense target class based on the following idea learning with protection.Let F be the set of measurable functions from X × Y to R. For a function f ∈ F , we define a classifier hf : X × Y → R as follows: yf (x) = argmax k∈[K] f(x, y = k). Let L(f ;x, y) be the loss function of a function f ∈ F for data (x, y). The goal of the learning problem is to find f ∈ F such that the population riskR(f) := E(x,y)∈D[L(f ;x, y)] is minimized. This formulation is the standard setting of multi-class classification problem. From the original formulation of the multi-class classification problem, we construct a binary classification problem to protect the defense target class t from misclassification. Let us define a function ybf : X → {−1,+1} for a function f to transform the prediction of multi-class classification to that of binary classification as ybf (x) = { +1 yf (x) = t −1 yf (x) 6= t . For a pair (x, ybf (x)), we calculate a new feature z ∗(x) as follows: z∗(x) = argmax x′∈Bpx( ) L̃(f ;x′, ybf (x)), (2) where L̃f (f ;x′, ybf (x)) is a binary loss function for a pair (x, ybf (x)). This loss function corresponds to the pseudo loss function introduced in the previous section. For example, we can define 0-1 loss as follows: L̃(f ;x′, ybf (x)) = 1ybf (x)=−11ybf (x′)=+1 + 1ybf (x)=+11ybf (x′)=−1. Then, for this binary classification problem derived from multi-class classification problem, we apply an algorithm for rejection. If yf (z∗(x)) = t for a sample x such that yf (x) 6= t, then we reject the sample because the sample x predicted as a class that is not t might be a class t in neighborhood of x with `p-norm. We call algorithms based on the abeve idea learning with protection. Algorithm 1 Learning with Protection Input: Trained classier ĥ, defense target class t ∈ [K], and test dataset {xi}ni=1. Construct a pseudo loss function (3) for test dataset {xi}ni=1. Compute z∗(xi) for a sample xi by (2) by method of adversarial example. Reject a sample xi if ĥ(xi) 6= t and ĥ(z∗(xi)) = tVarious algorithms can be used to implement the above framework. In this section, we consider a cross-entropy loss function and define an algorithm for protecting the defense target class. LetH be a hypothesis set. First, we train a model f ∈ H with the cross-entropy loss by any of the suitable methods for classification problems such as adversarial training. After training g, we obtain f̂ , the function that minimizes the cross-entropy loss. The trained model is an estimator of the risk minimizer f∗ on population. Second, we use the logistic loss for L̃, i.e., L̃(f ;x′, ybf (x)) =− 1ybf (x)=−1 log ( exp (f(x′, y = t))∑ k∈[K] exp (f(x ′, y = k)) ) − 1ybf (x)=+1 log ( 1− exp (f(x ′, y = t))∑ k∈[K] exp (f(x ′, y = k)) ) . (3) The intuition of this loss function is as follows. Let g∗(x′, t) = exp(f∗(x′,y=t))∑ k∈[K] exp(f ∗(x′,y=k)) . Under a cross entropy loss function, we can interpret g∗(x′, k) as p(y = k|x′) with g∗. Therefore, we can apply the log loss function function with g∗(x′, t) = p(y = t|z) and p(y 6= t|z) = 1−p(y = t|z) = 1− g∗(x′, t). As mentioned above, to find a feature z∗ from (2), we can apply methods of adversarial examples as a heuristics. We show the pseudo algorithm in Algorithm 1.To demonstrate the effectiveness of proposed method, we conducted experiments using benchmark data. For the benchmark data, we use the CIFAR-101) (Krizhevsky, 2009) and ImageNet-1002) (Kang et al., 2019) dataset. The CIFAR-10 dataset consists of 32×32 color images in 10 classes, with 6, 000 images per class. There are 50, 000 training images and 10, 000 test images. The ImageNet-100 dataset consists of the 100-class subset of ImageNet-1K (Deng et al., 2009; Russakovsky et al., 2015) containing every 10th class by WordNet ID order. As we explained above, learning with protection rejects suspicious samples after we train a model f̂ to protect defense target class. For CIFAR-10, we set airplane class as defense target class. For ImageNet-A, we set the first 10 classes in the order of WordNet ID as defense target class. Our method allow various training method to estimate f∗. When we train the model f̂ , we used both standard training method and adversarial training (Madry et al., 2018). After trainning a model, we adopt 30-step `∞ bounded PGD attack to find the feature z∗. For each experiment, we output the accuracy, precision, recall, rejection rate, and true rejection. Let TP, TN, FP, and FN be the numbers of true positive, true negative, false positive, and false negative of a classification result. Then, accuracy, precision, and recall are defined as accuracy = TP+TNTP+TN+FP+FN , precision = TP TP+FP , and recall = TP TP+FN . Let true accept (TA) be an outcome where the rejection function correctly accepts data such that a sample x does not belong to the defense target class, i.e., h(x) 6= t. Let true reject (FR) be an outcome where the rejection function correctly rejects data such that a sample x belongs to the defense target class, i.e., h(x) = t. FA and FR can be defined similarly (Ni et al., 2019). Then, rejection rate and precision of rejection are defined as rejection rate = TR+FRTR+TA+FR+FA and precision of rejection = TR TR+FR . 1)See https://www.cs.toronto.edu/~kriz/cifar.html. 2)See http://image-net.org/index. Neural network model: For CIFAR-10, we use the ResNet-56 (He et al., 2015) architecture. For ImageNet-100, we use the ResNet-50 (He et al., 2015) architecture with 224 × 224 resolution as implemented in torchvision. We describe training hyperparameters in Appendix A. Adversarial training: We use `∞ bounded projected gradient descent (PGD) method to generate training samples. We select a attack target class for each image uniformly at random from the set of incorrect classes. For distortion size ε, we set as ε = 8/255 and apply random scaling Uniform(0, ε) to improve performance against smaller distortions. We use 10 optimization steps and step size ε/ √ steps as described in (Kang et al., 2019). We update the model by stochastic gradient descent (SGD) method using only the adversarial images (no clean images).We evaluate our learning with protection method on the CIFAR-10 and ImageNet-100 validation sets against artificial adversarial examples with the defense target class. protection against expected attack: We distort the inputs by 50-step `∞ bounded PGD attack and apply learning with protection method. The single step size of PGD is calculated by ε/ √ steps. The results are shown in Table 1 and Appendix B.1. protection against unexpected attack: Although in our learning with protection method, a specific adversarial attack method is used to find the feature z∗(xi) for a sample xi (we adopt `∞ bounded PGD in this experiment), the input might be distorted by different attack methods. In order to evaluate the performance against such unexpected adversaries, we conducted experiments against 50-step l2 bounded PGD attack. The results are shown in Table 2 and Appendix B.1. We can see from the table that our method achieves 100% in the recall, i.e., we can reject all samples such that y = t and h(x) 6= t. As shown in Appendix B.1, models with adversarial training outperforms models without adversarial training. It is because our method only reduce the false negative, and it is difficult to improve the performance of a model when the number of the true positive is small. When we use adversarial training, we can increase the recall with the small number of rejection. For example, in Table 2, when attack is `2 bounded PGD (ε = 80) for CIFAR-10, recall increases about 20% by only reject 9.14% of all samples. On the other hand, On the other hand, when we reject too many samples, the precision of rejection dramatically drops. Therefore, we need to control the perturbation ε not to reject samples unnecessarily.Natural adversarial examples which are introduced in (Hendrycks et al., 2019) are defined as unmodified and naturally occurring examples that cause classifier accuracy to degrade drastically. In order to evaluate the proposed method for such real hard samples, we conduct experiments on ImageNet-A3) 3)See https://github.com/hendrycks/natural-adv-examples. dataset. ImageNet-A contains 7,500 natural adversarial example images in 200 classes that are a subset of ImageNet-1K’s 1,000 classes. We use the ResNet-50 architecture with 224× 224 resolution. We first trained the model on the training set of ImageNet-1K dataset and evaluate the performance on ImageNet-A with our learning with protection method at test time. Due to the difference in the number of classes, only 200 of its 1,000 logits are used in test time. We set the first 20 classes in the order of WordNet ID as the defense target class. The results are shown in Table 3. As shown in Table 3 and Appendix B.2, the proposed method works well for natural adversarial samples. For example, when the distortion size is 2/255, the recall increases about 20% by rejecting only 30% of all samples. On the other hand, the rejection rate of the models without adversarial training is high even when the size of the perturbation ε is small.In this paper, we proposed learning with rejection under adversarial attacks for avoiding the misclassification. The proposed method suspend decision making when a sample is `p-norm suspicious. By extending the strategy, we proposed an easily implementable algorithm called learning with protection. This algorithm prevents a defense target class from misclassification. We showed the performance of learning with protection using benchmark datasets. From the results, we confirmed that our method successfully caught samples that would have misclassified if we did not reject it.For CIFAR-10, we trained on a single NVIDIA V100 GPU for 200 epochs with batch size 32, initial learning rate 0.1, momentum 0.9, and weight decay 10−4. We decayed the learning rate at epochs 100 and 150. For ImageNet-100, we trained on machines with 8 NVIDIA V100 GPUs using standard data augmentation used in (He et al., 2015). We ran synchronized SGD (Goyal et al., 2017) for 90 epochs with batch size 32× 8 and a learning rate schedule with 5 “warm-up” epochs and a decay at epochs 30, 60, and 80 by a factor of 10.B.1 PROTECTION AGAINST ARTIFICIAL ADVERSARIAL EXAMPLES B.2 PROTECTION AGAINST NATURAL ADVERSARIAL EXAMPLES
1 2 (0, 1), and “light" coins with mean ✓ 0 2 (0, ✓ 1 ), where heavy coins are drawn from the bag with proportion ↵ 2 (0, 1/2). When ↵, ✓ 0 , ✓ 1 are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters ✓ 0 , ✓ 1 ,↵, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples. In characterizing this gap between adaptive and nonadaptive strategies, we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.The trade-off between exploration and exploitation has been an ever-present trope in the online learning literature. In contrast, this paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. Consider a bag that contains an infinite number of two kinds of biased coins: “heavy” coins with mean ✓ 1 2 (0, 1) and “light” coins with mean ✓ 0 2 (0, ✓ 1 ). When a player picks a coin from the bag, with probability ↵ the coin is “heavy” and with probability (1 ↵) the coin is “light.” The player can flip any coin she picks from the bag as many times as she wants, and the goal is to identify a heavy coin using as few total flips as possible. When ↵, ✓ 0 , ✓ 1 are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. That is, how does one balance flipping an individual coin many times to better estimate its mean against considering many new coins to maximize the probability of observing a heavy one. Previous work has only proposed solutions that rely on some or full knowledge ↵, ✓ 0 , ✓ 1 , limiting their applicability. In this work we propose the first algorithm that requires no knowledge of ↵, ✓ 0 , ✓ 1 , is guaranteed to return a heavy coin with probability at least 1 , and flips a total number of coins, in expectation, that nearly matches known lower bounds. Moreover, our fully adaptive algorithm supports more general sub-Gaussian sources in addition to just coins, and only ever has one “coin” outside the bag at a given time, a constraint of practical importance to some applications. In addition, we connect the most biased coin problem to anomaly detection and prove novel lower bounds on the difficulty of detecting the presence of a mixture versus just a single component of a known family of distributions (e.g. X ⇠ (1 ↵)g ✓ 0 + ↵g ✓ 1 versus X ⇠ g ✓ for some ✓). We show that in detecting the presence of a mixture distribution, there is a stark difference of difficulty 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain. between when the underlying distribution parameters are known (e.g. ↵, ✓ 0 , ✓ 1 ) and when they are not. The most biased coin problem can be viewed as an online, adaptive mixture detection problem where source distributions arrive one at a time that are either g ✓ 0 with probability (1 ↵) or g ✓ 1 with probability ↵ (e.g. null or anomolous) and the player adaptively chooses how many samples to take from each distribution (to increase the signal-to-noise ratio) with the goal of identifying an anomolous distribution f ✓ 1 using as few total number of samples as possible. This work draws a contrast between the power of an adaptive versus non-adaptive (e.g. taking the same number of samples each time) approaches to this problem, specifically when ↵, ✓ 0 , ✓ 1 are unknown.The most biased coin problem characterizes the inherent difficulty of real-world problems including anomaly and intrusion detection and discovery of vacant frequencies in the radio spectrum. Our interest in the problem stemmed from automated hiring of crowd workers: data labeling for machine learning applications is often performed by humans, and recent work in the crowdsourcing literature accelerates labeling by organizing workers into pools of labelers and paying them to wait for incoming data [4, 12]. Workers hired on marketplaces such as Amazon’s Mechanical Turk [16] vary widely in skill, and identifying high-quality workers as quickly as possible is an important challenge. We can model each worker’s performance (e.g. accuracy or speed) as a random variable so that selecting a good worker is equivalent to identifying a worker with a high mean. Since we do not observe a worker’s expected performance directly, we must give them tasks from which we estimate it (like repeatedly flipping a biased coin). Arlotto et al. [3] proposed a strategy with some guarantees for a related problem but did not characterize the sample complexity of the problem, the focus of our work. The most biased coin problem was first proposed by Chandrasekaran and Karp [8]. In that work, it was shown that if ↵, ✓ 0 , ✓ 1 were known then there exists an algorithm based on the sequential probability ratio test (SPRT) that is optimal in that it minimizes the expected number of total flips to find a “heavy” coin whose posterior probability of being heavy is at least 1 , and the expected sample complexity of this algorithm was upper-bounded by 16 (✓ 1 ✓ 0 ) 2 ✓ 1 ↵ ↵ + log ✓ (1 ↵)(1 ) ↵ ◆◆ . (1) However, the practicality of the proposed algorithm is severely limited as it relies critically on knowing ↵, ✓ 0 , and ✓ 1 exactly. In addition, the algorithm returns to coins it has previously flipped and thus requires more than one coin to be outside the bag at a time, ruling out some applications. Malloy et al. [15] addressed some of the shortcomings of [9] (a preprint of [8]) by considering both an alternative SPRT procedure and a sequential thresholding procedure. Both of these proposed algorithms only ever have one coin out of the bag at a time. However, the former requires knowledge of all relevant parameters ↵, ✓ 0 , ✓ 1 , and the latter requires knowledge of ↵, ✓ 0 . Moreover, these results are only presented for the asymptotic case where ! 0. The most biased coin problem can be viewed through the lens of multi-armed bandits. In the best-arm identification problem, the player has access to K distributions (arms) such that if arm i 2 [K] is sampled (pulled), an iid random variable with mean µ i is observed; the objective is to identify the arm associated with the highest mean with probability at least 1 using as few pulls as possible (see [14] for a short survey). In the infinite armed bandit problem, the player is not confined to K arms but an infinite reservoir of arms such that a draw from this reservoir results in an arm with a mean µ drawn from some distribution; the objective is to identify the highest mean possible after n total pulls for any n > 0 with probability 1 (see [7]). The most biased coin problem is an instance of this latter game with the arm reservoir distribution of means µ defined as P(µ ✓ 1 ✏) = ↵1 ✏ 0 + (1 ↵)1✏ ✓ 1 ✓ 0 for all ✏. Previous work has focused on an alternative arm distribution reservoir that satisfies E✏  P(µ µ⇤ ✏)  E0✏ for some µ⇤ 2 [0, 1] where E,E0 are constants and is known [5, 21, 6, 7]. Because neither arm distribution reservoir can be written in terms of the other, neither work subsumes the other. Note that one can always apply an algorithm designed for the infinite armed bandit problem to any finite K-armed bandit problem by defining the arm reservoir as placing a uniform distribution over the K arms. This is appealing when K is very large and one wishes to guarantee nontrivial performance when the number of pulls is much less than K1. The most biased problem is a special case of the K-armed reservoir distribution where one arm has mean ✓ 1 and K 1 arms have mean ✓ 0 with ↵ = 1 K . 1All algorithms for K-armed bandit problem known to these authors begins by sampling each arm once so that until the number of pulls exceeds K, performance is no better than random selection. Given that [8] and [15] are provably optimal algorithms for the most biased coin problem given knowledge of ↵, ✓ 0 , ✓ 1 , it is natural to consider a procedure that first estimates these unknown parameters first and then uses these estimates in the algorithms of [8] or [15]. Indeed, in the - parameterized arm reservoir setting discussed above, this is exactly what Carpentier and Valko [7] propose to do, suggesting a particular estimator for given a lower bound b  . They show that this estimator is sufficient to obtain the same sample complexity result up to log factors as when was known. Sadly, through upper and lower bounds we show that for the most biased coin problem this estimate-then-explore approach requires quadratically more flips than our proposed algorithm that adapts to these unknown parameters. Specifically, we show that when ✓ 1 ✓ 0 is sufficiently small one cannot use a static estimation step to determine whether ↵ = 0 or ↵ > 0 unless a number of samples quadratic in the optimal sample complexity are taken. Our contributions to the most biased coin problem include a novel algorithm that never has more than one coin outside the bag at a time, has no knowledge of the distribution parameters, supports distributions on [0, 1] rather than just “coins,” and comes within log factors of the known informationtheoretic lower bound and Equation 1 which is achieved by an algorithm that knows the parameters. See Table 1 for an overview of the upper and lower bounds proved in this work for this problem. We believe that our algorithm is the first solution to the most biased coin problem that does not require prior knowledge of the problem parameters and that the same approach can be reworked to solve more general instances of the infinite-armed bandit problem, including the -parameterized and K-armed reservoir cases described of above. Finally, if an algorithm is desired for arbitrary arm reservoir distributions, this work rules out an estimate-then-explore approach.Let ✓ 2 ⇥ index a family of single-parameter probability density functions g ✓ and fix ✓ 0 , ✓ 1 2 ⇥, ↵ 2 [0, 1/2]. For any ✓ 2 ⇥ assume that g ✓ is known to the procedure. Note that in the most biased coin problem, g ✓ =Bernoulli(✓), but in general it is arbitrary (e.g. N (✓, 1)). Consider a sequence of iid Bernoulli random variables ⇠ i 2 {0, 1} for i = 1, 2, . . . where each P(⇠ i = 1) = 1 P(⇠ i = 0) = ↵. Let X i,j for j = 1, 2, . . . be a sequence of random variables drawn from g ✓ 1 if ⇠ i = 1 and g ✓ 0 otherwise, and let {{X i,j }Mi j=1 }N i=1 represent the sampling history generated by a procedure for some N 2 N and (M 1 , . . . ,M N ) 2 NN . Any valid procedure behaves accordingly: Algorithm 1 The most biased coin problem definition. Only the last distribution drawn may be sampled or declared heavy, enforcing the rule that only one coin may be outside the bag at a time. Initialize an empty history (N = 1,M = (0, 0, . . . )). Repeat until heavy distribution declared: Choose one of 1. draw a sample from distribution N , M N M N + 1 2. draw a sample from the (N + 1)st distribution, M N+1 = 1, N N + 1 3. declare distribution N as heavy Definition 1 We say a strategy for the most biased coin problem is -probably correct if for all (↵, ✓ 0 , ✓ 1 ) it identifies a “heavy” g ✓ 1 distribution with probability at least 1 . Definition 2 (Strategies for the most biased coin problem) An estimate-then-explore strategy is a strategy that, for any fixed m 2 N, begins by sampling each successive coin exactly m times for a number of coins that is at least the minimum necessary for any test to determine that ↵ 6= 0 with probability at least 1 , then optionally continues sampling with an arbitrary strategy that declares a heavy coin. An adaptive strategy is any strategy that is not an estimate-then-explore strategy. We study the estimate-then-explore strategy because there exist optimal algorithms [8, 15] for the most biased coin problem if ↵, ✓ 0 , ✓ 1 are known, so it is natural to consider estimating these quantities then using one of these algorithms. Note that the algorithm of [7] for the -parameterized infinite armed bandit problem discussed above can be considered an estimate-then-explore strategy since it first estimates by sampling a fixed number of samples from a set of arms, and then uses this estimate to draw a fixed number of arms and applies a UCB-style algorithm to these arms. A contribution of this work is showing that such a strategy is infeasible for the most biased coin problem. For all strategies that are -probably correct and follow the interface of Algorithm 1, our goal is to provide lower and upper bounds on the quantity E[T ] := E[ P N i=1 M i ] for any (↵, ✓ 0 , ✓ 1 ) if N denotes the final number of coins considered.Addressing the most biased coin problem, [15] analyzes perhaps the most natural strategy: fix an m 2 N and flip each successive coin exactly m times. The relevant questions are how large does m have to be in order to guarantee correctness with probability 1 , and for a given m how long must one wait to declare a “heavy” coin? The authors partially answer these questions and we improve upon them (see Section 3.2.1) which leads us to our study of the difficulty of detecting the presence of a mixture distribution. As an example of the kind of lower bounds shown in this work, if we observe a sequence of random variables X 1 , . . . , X n , consider the following hypothesis test: H 0 : 8i X 1 , . . . , X n ⇠ N (✓, 2) for some ✓ 2 R, H 1 : 8i X 1 , . . . , X n ⇠ (1 ↵)N (✓ 0 , 2) + ↵ N (✓ 1 , 2) (P1) which will henceforth be referred to as Problem P1 or just (P1). We can show that if ✓ 0 , ✓ 1 ,↵ are known and ✓ = ✓ 0 , then it is sufficient to observe just max{1/↵, 2 ↵ 2 (✓ 1 ✓ 0 ) 2 log(1/ )} samples to determine the correct hypothesis with probability at least 1 . However, if ✓ 0 , ✓ 1 ,↵ are unknown then it is necessary to observe at least max 1/↵, 2 ↵(✓ 1 ✓ 0 ) 2 2 log(1/ ) samples in expectation whenever (✓1 ✓0) 2 2  1 and max{1/↵, 2 ↵ 2 (✓ 1 ✓ 0 ) 2 log(1/ )} otherwise (see Appendix C). Recognizing (✓1 ✓0) 2 2 as the KL divergence between two Gaussians of H 1 , we observe startling consequences for anomaly detection when the parameters of the underlying distributions are unknown: if the anomalous distribution is well separated from the null distribution, then detecting an anomalous component is only about as hard as observing just one anomalous sample (i.e. 1/↵) multiplied by the inverse KL divergence between the null and anomalous distributions. However, when the two distributions are not well separated then the necessary sample complexity explodes to this latter quantity squared. In Section 4 we will investigate adaptive methods for dramatically decreasing this sample complexity. Our lower bounds are based on the detection of the presence of a mixture of two distributions of an exponential family versus just a single distribution of the same family. There has been extensive work in the estimation of mixture distributions [13, 11] but this literature often assumes that the mixture coefficient ↵ is bounded away from 0 and 1 to ensure a sufficient number of samples from each distribution. In contrast, we highlight the regime when ↵ is arbitrarily small, as is the case in statistical anomaly detection [10, 20, 2]. Property testing, e.g. unimodality, [1] is relevant but can lack interpetability or strength in favor of generality. Considering the exponential family allowing us to make interpretable statements about the relevant problem parameters in different regimes. Preliminaries Let P and Q be two probability distributions with densities p and q, respectively. For simplicity, assume p and q have the same support. Define the KL Divergence between P and Q as KL(P,Q) = R log ⇣ p(x) q(x) ⌘ dp(x). Define the 2 Divergence between P and Q as 2(P,Q) = R ⇣ p(x) q(x) 1 ⌘ 2 dq(x) = R (p(x) q(x))2 q(x) dx. Note that by Jensen’s inequality KL(P,Q) = E p ⇥ log p q ⇤  log E p ⇥ p q ⇤ = log 2(P,Q) + 1  2(P,Q). (2) Examples: If P = N (✓ 1 , 2) and Q = N (✓ 0 , 2) then KL(P,Q) = (✓1 ✓0) 2 2 2 and 2(P,Q) = e (✓ 1 ✓ 0 ) 2 2 1. If P = Bernoulli(✓ 1 ) and Q = Bernoulli(✓ 0 ) then KL(P,Q) = ✓ 1 log( ✓ 1 ✓ 0 ) + (1 ✓ 1 ) log( 1 ✓ 1 1 ✓ 0 )  (✓1 ✓0) 2 /2 ✓ 0 (1 ✓ 0 ) [(✓ 1 ✓ 0 )(2✓ 0 1)] + and 2(P,Q) = (✓1 ✓0) 2 ✓ 0 (1 ✓ 0 ) . All proofs appear in the appendix.We present lower bounds on the sample complexity of -probably correct strategies for the most biased coin problem that follow the interface of Algorithm 1. Lower bounds are stated for any adaptive strategy in Section 3.1, non-adaptive strategies that may have knowledge of the parameters but sample each distribution the same number of times in Section 3.2.1, and estimate-then-explore strategies that do not have prior knowledge of the parameters in Section 3.2.2. Our lower bounds, with the exception of the adaptive strategy, are based on the difficulty of detecting the presence of a mixture distribution, and this reduction is explained in Section 3.2.The following theorem, reproduced from [15], describes the sample complexity of any -probably correct algorithm for the most biased coin identification problem. Note that this lower bound holds for any procedure even if it returns to previously seen distributions to draw additional samples and even if it knows ↵, ✓ 0 , ✓ 1 . Theorem 1 [15, Theorem 2] Fix 2 (0, 1). Let T be the total number of samples taken of any procedure that is -probably correct in identifying a heavy distribution. Then E[T ] c 1 max ⇢ 1 ↵ , (1 ) ↵KL(g ✓ 0 |g ✓ 1 ) whenever ↵  c 2 where c 1 , c 2 2 (0, 1) are absolute constants. The above theorem is directly applicable to the special case where g ✓ is a Bernoulli distribution, implying a lower bound of max 1 ↵ , 2min{✓0(1 ✓0),✓1(1 ✓1)} ↵(✓ 1 ✓ 0 ) 2 for the most biased coin problem. The upper bounds of our proposed procedures for the most biased coin problem presented later will be compared to this benchmark.First observe that identifying a specific distribution i  N as heavy (i.e. ⇠ i = 1) or determining that ↵ is strictly greater than 0, is at least as hard as detecting that any of the distributions up to distribution N is heavy. Thus, a lower bound on the total expected number of samples of all considered distributions for this strictly easier detection problem is also a lower bound for the estimate-then-explore strategy for the most biased coin identification problem. The estimate-then-explore strategy fixes an m 2 N prior to starting the game and then samples each distribution exactly m times, i.e. M i = m for all i  N for some N . To simplify notation let f ✓ denote the distribution of the sufficient statistics of these m samples. In general f ✓ is a product distribution, but when g ✓ is a Bernoulli distribution, as in the biased coin problem, we can take f ✓ to be a Binomial distribution with parameters (m, ✓). Now our problem is more succinctly described as: H 0 : 8i X i ⇠ f ✓ for some ✓ 2 e⇥ ✓ ⇥, H 1 : 8i ⇠ i ⇠ Bernoulli(↵), 8i X i ⇠ ⇢ f ✓ 0 if ⇠ i = 0 f ✓ 1 if ⇠ i = 1 (P2) If ✓ 0 and ✓ 1 are close to each other, or if ↵ is very small, it can be very difficult to decide between H 0 and H 1 even if ↵, ✓ 0 , ✓ 1 are known a priori. Note that when the parameters are known, one can take e ⇥ = {✓ 0 }. However, when the parameters are unknown, one takes e⇥ = ⇥ to prove a lower bound on the sample complexity of the estimate-then-explore algorithm, which is tasked with deciding whether or not samples are coming from a mixture of distributions or just a single distribution within the family. That is, lower bounds on the sample complexity when the parameters are known and unknown follow by analyzing a simple binary and composite hypothesis test, respectively. In what follows, for any event A, let P i (A) and E i [A] denote probability and expectation of A under hypothesis H i for i 2 {0, 1} (the specific value of ✓ in H 0 will be clear from context). The next claim is instrumental in our ability to prove lower bounds on the difficulty of the hypothesis tests. Claim 1 Any procedure that is -probably correct also satisfies P 0 (N <1)  whenever ↵ = 0.Theorem 2 Fix 2 (0, 1). Consider the hypothesis test of Problem P2 for any fixed ✓ 2 e⇥ ✓ ⇥. Let N be the random number of distributions considered before stopping and declaring a hypothesis. If a procedure satisfies P 0 (N < 1)  and P 1 ([N i=1 {⇠ i = 1}) 1 , then E 1 [N ] max n 1 ↵ , log(1/ ) KL(P 1 |P 0 ) o max n 1 ↵ , log(1/ ) 2 (P 1 |P 0 ) o . In particular, if e⇥ = {✓ 0 } then E 1 [N ] max n 1 ↵ , log(1/ ) ↵2 2(f ✓ 1 |f ✓ 0 ) o . The next corollary relates Theorem 2 to the most biased coin problem and is related to Malloy et al. [15, Theorem 4] that considers the limit as ↵! 0 and assumes m is sufficiently large (specifically, large enough for the Chernoff-Stein lemma to apply). In contrast, our result holds for all finite ,↵,m. Corollary 1 Fix 2 (0, 1). For any m 2 N consider a -probably correct strategy that flips each coin exactly m times. If N m is the number of coins considered before declaring a coin as heavy then min m2N E[mN m ] (1 ) log ⇣ log(1/ ) ↵ ⌘ ↵ ✓ 0 (1 ✓ 0 ) (✓ 1 ✓ 0 ) 2 . One can show the existence of such a strategy with a nearly matching upperbound when ↵, ✓ 0 , ✓ 1 are known (see Appendix B.1). Note that this is at least log(1/↵) larger than the sample complexity of (1) that can be achieved by an adaptive algorithm when the parameters are known.If ↵, ✓ 0 , and ✓ 1 are unknown, we cannot test f ✓ 0 against the mixture (1 ↵)f ✓ 0 + ↵f ✓ 1 . Instead, we have the general composite test of any individual distribution against any mixture, which is at least as hard as the hypothesis test of Problem P2 with e⇥ = {✓} for some particular worst-case setting of ✓. Without any specific form of f ✓ , it is difficult to pick a worst case ✓ that will produce a tight bound. Consequently, in this section we consider single parameter exponential families (defined formally below) to provide us with a class of distributions in which we can reason about different possible values for ✓. Since exponential families include Bernoulli, Gaussian, exponential, and many other distributions, the following theorem is general enough to be useful in a wide variety of settings. The constant C referred to in the next theorem is an absolute constant under certain conditions that we outline in the following remark and corollary, its explicit form is given in the proof. Theorem 3 Suppose f ✓ for ✓ 2 ⇥ ⇢ R is a single parameter exponential family so that f ✓ (x) = h(x) exp(⌘(✓)x b(⌘(✓))) for some scalar functions h, b, ⌘ where ⌘ is strictly increasing. If e⇥ = {✓⇤} where ✓⇤ = ⌘ 1 (1 ↵)⌘(✓ 0 ) + ↵⌘(✓ 1 ) and N is the stopping time of any procedure that satisfies P 0 (N <1)  and P 1 ([N i=1 {⇠ i = 1}) 1 , then E 1 [N ] max n 1 ↵ , log( 1 ) C ( 1 2 ↵(1 ↵)(⌘(✓ 1 ) ⌘(✓ 0 )) 2 ) 2 o . where C is a constant that may depend on ↵, ✓ 0 , ✓ 1 . The following remark and corollary apply Theorem 3 to the special cases of Gaussian mixture model detection and the most biased coin problem, respectively. Remark 1 When ↵, ✓ 0 , ✓ 1 are unknown, any procedure has no knowledge of e ⇥ in Problem P2 and consequently it cannot rule out ✓ = ✓⇤ for H0 where ✓⇤ is defined in Theorem 3. If f✓ = N (✓, 2) for known , then whenever (✓1 ✓0) 2 2  1 the constant C in Theorem 3 is an absolute constant and consequently, E 1 [N ] = ⌦ 2 ↵(✓ 1 ✓ 0 ) 2 2 log(1/ ) . Conversely, when ↵, ✓ 0 , ✓ 1 are known, then we simply need to determine whether samples came from N (✓ 0 , 2) or (1 ↵)N (✓ 0 , 2)+↵N (✓ 1 , 2), and we show that it is sufficient to take just O ⇣ 2 ↵ 2 (✓ 1 ✓ 0 ) 2 log(1/ ) ⌘ samples (see Appendix C). Corollary 2 Fix 2 [0, 1] and assume ✓ 0 , ✓ 1 are bounded sufficiently far from {0, 1} such that 2(✓ 1 ✓ 0 )  min{✓ 0 (1 ✓ 0 ), ✓ 1 (1 ✓ 1 )}. For any m let N m be the number of coins a -probably correct estimate-then-explore strategy that flips each coin m times in the exploration step. Then mE[N m ] c0 min{ 1 m , ✓⇤(1 ✓⇤)} ⇣ ↵(1 ↵) (✓1 ✓0)2 ✓⇤(1 ✓⇤) ⌘ 2 log( 1 ) whenever m  ✓⇤(1 ✓⇤) (✓ 1 ✓ 0 ) 2 . where c0 is an absolute constant and ✓⇤ = ⌘ 1 ((1 ↵)⌘(✓0) + ↵⌘(✓1)) 2 [✓0, ✓1]. Remark 2 If ↵, ✓ 0 , ✓ 1 are unknown, any estimate-then-explore strategy (or the strategy described in Corollary 1) would be unable to choose an m that depended on these parameters, so we can treat it as a constant. Thus, for the case when ✓ 0 and ✓ 1 are bounded away from {0, 1} (e.g. ✓ 0 , ✓ 1 2 [1/8, 7/8]), the above corollary states that for any fixed m, whenever ✓ 1 ✓ 0 is sufficiently small the number of samples necessary for these strategies to identify a heavy coin scales like 1 ↵(✓ 1 ✓ 0 ) 2 2 log(1/ ). This is striking example of the difference when parameters are known versus when they are not and effectively rules out an estimate-then-explore strategy for practical purposes.In this section we propose an algorithm that has no prior knowledge of the parameters ↵, ✓ 0 , ✓ 1 yet yields an upper bound that matches the lower bound of Theorem 1 up to logarithmic factors. We assume that samples from heavy or light distributions are supported on [0, 1], and that drawn samples are independent and unbiased estimators of the mean, i.e., E[X i,j ] = µ i for µ i 2 {✓ 0 , ✓ 1 }. All results can be easily extended to sub-Gaussian distributions. Consider Algorithm 2, an SPRT-like procedure [18] for finding a heavy distribution given and lower bounds on ↵ and ✏ = ✓ 1 ✓ 0 . It improves upon prior work by supporting arbitrary distributions on [0, 1] and requires only bounds ↵, ✏. Algorithm 2 Adaptive strategy for heavy distribution identification with inputs ↵ 0 , ✏ 0 , Given 2 (0, 1/4),↵ 0 2 (0, 1/2), ✏ 0 2 (0, 1). Initialize n = d2 log(9)/↵ 0 e,m = d64✏ 2 0 log(14n/ )e, A = 8✏ 1 0 log(21), B = 8✏ 1 0 log(14n/ ), k 1 = 5, k 2 = d8✏ 2 0 log(2k 1 /min{ /8,m 1✏ 2 0 })e. Draw k 1 distributions and sample them each k 2 times. Estimate b✓ 0 = min i=1,...,k 1 bµ i,k 2 , ̂ = b✓ 0 + ✏ 0 /2. Repeat for i = 1, . . . , n: Draw distribution i. Repeat for j = 1, . . . ,m: Sample distribution i and observe X i,j . If P j k=1 (X i,k ̂) > B: Declare distribution i to be heavy and Output distribution i. Else if P j k=1 (X i,k ̂) < A: break. Output null. Theorem 4 If Algorithm 2 is run with 2 (0, 1/4),↵ 0 2 (0, 1/2), ✏ 0 2 (0, 1), then the expected number of total samples taken by the algorithm is no more than c0↵ log(1/↵ 0 ) + c00 log 1 ↵ 0 ✏2 0 (3) for some absolute constants c0,c00, and all of the following hold: 1) with probability at least 1 , a light distribution is not returned, 2) if ✏ 0  ✓ 1 ✓ 0 and ↵ 0  ↵, then with probability 4 5 a heavy distribution is returned, and 3) the procedure takes no more than c log(1/(↵ 0 )) ↵ 0 ✏ 2 0 total samples. The second claim of the theorem holds only with constant probability (versus with probability 1 ) since the probability of observing a heavy distribution among the n = d2 log(4)/↵ 0 e distributions only occurs with constant probability. One can show that if the outer loop of algorithm is allowed to run indefinitely (with m and n defined as is), ✏ 0 = ✓ 1 ✓ 0 , ↵ 0 = ↵, and b✓ 0 = ✓ 0 , then a heavy coin is returned with probability at least 1 and the expected number of samples is bounded by (3). If a tight lower bound is known on either ✏ = ✓ 1 ✓ 0 or ↵, there is only one parameter that is unknown and the “doubling trick”, along with Theorem 4, can be used to identify a heavy coin with just log(log(✏ 2 )/ ) ↵✏ 2 and log(log(↵ 1 )/ ) ↵✏ 2 samples, respectively (see Appendix B.3). Now consider Algorithm 3 that assumes no prior knowledge of ↵, ✓ 0 , ✓ 1 , the first result for this setting that we are aware of. We remark that while the placing of “landmarks” (↵ k , ✏ k ) throughout the search space as is done in Algorithm 3 appears elementary in hindsight, it is surprising that so few can cover this two dimensional space since one has to balance the exploration of ↵ and ✏. We believe similar a similar approach may be generalized for more generic infinite armed bandit problems. Algorithm 3 Adaptive strategy for heavy distribution identification with unknown parameters Given > 0. Initialize ` = 1, heavy distribution h = null. Repeat until h is not null: Set ` = 2 `, ` = /(2`3) Repeat for k = 0, . . . , `: Set ↵ k = 2 k ` , ✏ k = q 1 2↵ k ` Run Algorithm 2 with ↵ 0 = ↵ k , ✏ 0 = ✏ k , = ` and Set h to its output. If h is not null break Set ` = `+ 1 Output h Theorem 5 (Unknown ↵, ✓ 0 , ✓ 1 ) Fix 2 (0, 1). If Algorithm 3 is run with then with probability at least 1 a heavy distribution is returned and the expected number of total samples taken is bounded by c log 2 ( 1 ↵✏ 2 ) ↵✏2 (↵ log 2 ( 1 ✏ 2 ) + log(log 2 ( 1 ↵✏ 2 )) + log(1/ )) for an absolute constant c.While all prior works have required at least partial knowledge of ↵, ✓ 0 , ✓ 1 to solve the most biased coin problem, our algorithm requires no knowledge of these parameters yet obtain the near-optimal sample complexity. In addition, we have proved lower bounds on the sample complexity of detecting the presence of a mixture distribution when the parameters are known or unknown, with consequences for any estimate-then-explore strategy, an approach previously proposed for an infinite armed bandit problem. Extending our adaptive algorithm to arbitrary arm reservoir distributions is of significant interest. We believe a successful algorithm in this vein could have a significant impact on how researchers think about sequential decision processes in both finite and uncountable action spaces. Acknowledgments Kevin Jamieson is generously supported by ONR awards N00014-15-1-2620, and N0001413-1-0129. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Apple Inc., Arimo, Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm and VMware.
Sequence-to-Sequence (S2S) models are state-of-the-art for tasks where source and target sequences have different lengths, including automatic speech recognition, machine translation, speech translation, text-to-speech synthesis, etc. The most common models are composed of an encoder that reads the entire input sequence, while a decoder (often equipped with an attention mechanism) iteratively produces the next output token given the input and the partial output decoded so far. While these models perform very well in the typical offline decoding use case, few studies consider how S2S models are affected by low-latency constraints, and which architectures and strategies are the most efficient. Low-latency decoding is desirable for applications such as online speech recognition, and as-you-type machine translation. In such scenarios, the decoding process starts before the entire input sequence is available, and the output sequence is produced in an on-the-fly manner. However, if we consider for instance machine translation, online prediction generally comes at the cost of reduced translation quality and more research is needed to reach the grail of natural and high-quality online speech-to-speech interpretation. In this paper we consider deterministic “wait-k” decoders that are state of the art for low-latency decoding (Ma et al., 2019; Zheng et al., 2019b). These decoders first read k tokens from the source, after which they proceed to alternatingly produce a target symbol and read another source symbol. We compare two architectures to implement such models: one based on a 2D-convolutional sequence-to-sequence model (Elbayad et al., 2018), and one based on the attention-based transformer architecture (Vaswani et al., 2017). For these models, we investigate the impact of the choice of k when training the models, and when using them to generate translations. For the transformer model, we also consider the effect of updating the hidden states of previous target symbols based on the full source context that is available at any moment. These updates are inspired from the 2D convolutional model, where such “updates” are an inherent consequence of the architecture. In summary, our contributions are the following: 1. We compare transformer and 2D convolutional architectures for online machine translation. 2. We propose improved training techniques for wait-k by first using uni-directional encoders and training across multiple values of k 3. We boost the translation quality in test-time by updating the prefix hidden-states in transformer decoders. 4. We set a new state of the art for online translation on WMT15 German-English, improving over the recent state-of-the-art results by Ma et al. (2019) and Zheng et al. (2019b) across a full range of latency levels. The rest of this paper is organized as follows: Section 2 presents related work on low-latency machine translation; Section 3 details our low-latency models, which we evaluate in Section 4 for German-English machine translation on the IWSLT14 dataset as well as the larger WMT15 dataset. Section 5 concludes the paper.In order to study differences between translated and simultaneously interpreted text, He et al. (2016a) produced a parallel corpus between both and showed that human interpreters regularly apply several tactics to reduce translation latency, including sentence segmentation and passivization. In machine translation, after pioneering work from Fügen et al. (2007), Yarmohammadi et al. (2013) and He et al. (2015) proposed methods to increase the alignment monotonicity for statistical machine translation. To allow for a more flexible segmentation, Grissom et al. (2014) introduced a trainable segmentation module that decides whether to write or read tokens based on the prediction of the final verb and the next token in the source sequence. Similarly, Oda et al. (2015) use output of a syntax-based statistical translation system to find the optimal segmentation strategy that maximizes the quality of the translations via greedy search and dynamic programming. One of the first works on online translation to use attention-based sequence-to-sequence models is that of Cho & Esipova (2016), which uses manually designed non-trainable waiting criteria that dictate whether the model should commit to a read/write operation. The neural transducer of Jaitly et al. (2016) reads equally-sized chunks of the source sequence and generates output sub-sequences of variable lengths, each ending with a special token marking the end of the writing. For training, a single segmentation is chosen to optimize the likelihood of the full output sequence. Raffel et al. (2017) propose an alternative to attention using monotonic alignments. Whereas attention requires access to the full source sequence to compute the weights, monotonic alignments enable linear time computation of the weights and online decoding. Another line of research treats general alignments of source and target sequence as a latent variable. Inspired from the HMM word alignment model used in statistical machine translation (Vogel et al., 1996), the segment-to-segment neural transducer model of Yu et al. (2016) integrates a latent alignment variable into an LSTM-based sequence-to-sequence model where the transition probabilities are conditioned on the encoder-decoder hidden states. During training the alignment is marginalized out with a forward-backward algorithm (Rabiner, 1989). However, their approach is used as an alternative to attention-based models, not for low-latency translation. Luo et al. (2017) introduce a recurrent sequence-to-sequence model with binary stochastic decision variables to either emit the next output token or advance in encoding the source sequence with a unidirectional LSTM that jointly encodes the partially generated output sequence. The stochastic decisions are optimized using a standard policy gradient reinforcement learning approach. Gu et al. (2017) propose a trainable agent emitting read/write decisions modeled as a recurrent neural network fed with the encoder and decoder current hidden states. In their framework, a left-to-right recurrent sequence-to-sequence model is first pre-trained on the full bi-texts and then fine-tuned with policy gradient to optimize a reward balancing the quality and the latency of the translations. Dalvi et al. (2018) used a static decoding algorithm that starts with k read operations then alternates between blocks of l write operations and l read operations. Albeit simple, this approach outperforms the information based criteria of Cho & Esipova (2016) and allows for complete control of the translation delay. Their attempt to integrate incrementality in the training by pre-aligning the sourcetarget sequences and then attending over a constrained span of source positions failed to improve the translation quality. The work of Press & Smith (2018), although not tackling the simultaneous translation task, allows for emitting target tokens before reading the full input sequence. Similar to the incremental training of Dalvi et al. (2018), it requires pre-aligning the bitexts. More recently, Ma et al. (2019) trained a sequence-to-sequence model based on the transformer architecture (Vaswani et al., 2017) with an integrated “wait-k” agent that first reads k source tokens then alternate single read-writes, similar to Dalvi et al. (2018) but using l “ 1. Wait-k approaches, were found most effective by Zheng et al. (2019b) when trained for the specific k that is used to generate translations. This, however, requires training separate models for each potential value of k used for translation. As an alternative, Zheng et al. (2019a;b) both learn an adaptive policy to produce read/write decisions in a pre-trained offline translation model. Zheng et al. (2019a) train the model for decoding along two wait-k paths and Zheng et al. (2019b) use supervised training based on an oracle read/write sequence derived from the pre-trained offline translation model. Different latency levels are achieved by thresholding the policy’s confidence, however, on most regimes their model performs worse than using transformer models directly trained for wait-k online translation. In our work, we also use wait-k decoders, but unlike Ma et al. (2019); Zheng et al. (2019a;b) we opted for uni-directional encoders and show that they are efficient to train in an online setup and can achieve better results if we update the prefix decoder states during inference.Let px,yq be a pair of source-target sequences of respective lengths |x| and |y|. Low-latency decoding consists of executing a sequence of interleaved reading and writing operations, consuming tokens from the source x and producing tokens of the target y. Low-latency decoding paths can be represented on a |y| ˆ |x| grid, where we advance from the top left to the bottom right in a total of |x| read steps and |y| write steps. See Figure 1 for an illustration. We consider two recent sequence-to-sequence models for low-latency decoding: the transformer architecture of Vaswani et al. (2017) and the 2D convolutional architecture of Elbayad et al. (2018). We describe in Section 3.1 how each of these architectures is adapted for the task of simultaneous translation. In Section 3.2 we describe how these architectures can be trained for online decoding, based on one or more decoding paths.To formalize the low-latency decoding process we model it as a sequence of |y| write steps. At step t P t1, . . . , |y|u we decode the t-th target token yt, conditioning on the target prefix yăt and the source sequence read up that point xďzt . The value of zt is dictated by the decoding path. The decoder computes the distribution over the next target token given the source and target contexts, pθpyt|yăt,xďzt , zătq, (1) where θ denotes the parameters of the writer. It depends on the architecture of the decoder whether, given zt, the prediction of yt further depends on the full decoding zăt path that led up to this point.In order for the decoder to be trained efficiently for different source/target context sizes, we build upon the “pervasive attention” architecture of Elbayad et al. (2018). Their machine translation model uses masked 2D convolutions across a source-target grid, as in Figure 1. The masked convolutions ensure that only information from past target tokens is used to predict the next one. Here we adapt the masking pattern so as to construct the field-of-view in a way that it only extends over previous positions in both source and target dimensions. More concretely, we use a 11ˆ11 filter where only the top-left 6ˆ6 weights are non-zero. In this manner features computed by a sequence of convolutional layers at a position pt, jq in the grid can be used to define pθpyt|yăt,xďjq. As input, at each site pt, jq, the network takes the concatenation of the (sub-)word embedding of the t-th token in target and the j-th token in the source sequence. The input is then progressively transformed along a number of convolutional network layers. Let H “ phtjq for 1 ď t ď |y| and 1 ď j ď |x| be the final features of the masked convolutional neural network. Elbayad et al. (2018) apply max-pooling across the source dimension to map H to a fixed-sized vector for each target position. To define pθpyt|yăt,xďjq in our low-latency model, we use ftj “ PoolpHďt,ďjq, (2) where Poolp¨q poolsHďt,ďj in the source and/or target direction, or neither in which case ftj “ htj . Preliminary experiments with the pervasive attention architecture showed that max-pooling across the source dimension only is the best option. For the remainder of this paper we have: ftj “ Max-poolpHt,ďjq. (3) From there we generate the emission probabilities by linearly projecting the feature ftj to the dimension of the output vocabulary with the matrix W , followed by a soft-max normalization: pθpyt|yăt,xďjq “ softmaxpWftjq. (4) Due to the 2D convolutional structure of the network, the path leading up to position pt, jq in the network does not impact the prediction of yt. Therefore, we drop the dependence on zăt in Eq. (4).The second online decoder we consider is based on the “transformer” model of Vaswani et al. (2017). Both the encoder and decoder consist of a stack of blocks. Each block consists of a multi-head selfattention followed by a position-wise fully connected feed-forward network, or equivalently a 1ˆ1 convolution. The decoder uses an additional multi-head attention block that ranges over the source token encodings. Given a query vector qt, an attention component aggregates a set of value vectors vj in a weighted sum, based on scoring a corresponding set of key vectors kj against the query. Using the dot-product as the score, we obtain the attention aggregate at as at “ ÿ j αtjvj , αtj “ exp etjř j exp etj , etj “ qJt kj . (5) The self-attention block is designed such that only previous output tokens can be attended over, since future tokens are not available when generating targets, ensuring the decoder is autoregressive (AR). In our work, the transformer model for online translation has the following properties: 1. A uni-directional encoder: we make the self-attention in the encoder autoregressive, so that it can encode all source prefixes in parallel during training, and encode them progressively during generation. This is achieved by substituting the mask used in Ma et al. (2019) (referred to as STACL in Eq. (6)) with the AR mask of Eq. (7). 2. A constrained encoder-decoder interaction: we mask the source attention in the decoder, so that when producing the t-th output token attention is limited to the zt source tokens read so far. This is guaranteed with the enc-dec mask of Eq. (7). @t, eSTACLij “ " qJi kj , If i, j ď zt ´8, otherwise , (6) eARij “ " qJi kj , If j ď i ´8, otherwise , e enc-dec tj “ " qJt kj , If j ď zt ´8, otherwise . (7) In this transformer-based decoder, the prediction of yt depends on the full decoding path zăt. This is because the decoder self-attention ranges over decoder hidden states of previous time steps t1 ă t, which themselves used zt1 to attend over the source encoding. In addition to the transformer-decoder described above, we consider a second transformer-based variant which removes the dependency of yt on zăt. To achieve this, each time a source token is read, we update all hidden states in the decoder based on the source context xzt available at that point. Essentially, we will re-run the decoder across the entire output sequence, using the new source context. This makes the decoder self-attention insensitive to zăt given zt, increasing the decoding cost from Op|x| ˆ |y|q to Op|x|2 ˆ |y|q. Note that the encoder still needs to be run only once, since its self-attention has been constrained to be autoregressive. We refer to this decoder as “transformer-update”, see Figure 2 for an illustration.For our decoders we use pre-defined “wait-k” schedules (Dalvi et al., 2018; Ma et al., 2019). First k source tokens are read, before alternating reading and writing a single token at a time, until either the full source has been read, or the target generation has been terminated. To train our models, we consider several strategies. The first, similar to Ma et al. (2019) is to train the model using the same wait-k decoding path that will be used for generation. That is, to train the model, we optimize a sum of loss terms, each of which corresponds to the negative log-likelihood of predicting a target token: Lpθ,x,y, zq “ ´ |y| ÿ t“1 ln pθpyt|yăt,xďzt , zătq. (8) Training with k“8 corresponds to an offline (wait-until-the-end) decoder, with the exception that here we still use autoregressive dependencies in the encoder or along the source dimension in the 2D convolutional network. In addition to training according to a single wait-k decoding path, we can use losses associated with multiple paths. The additional loss terms may provide a richer training signal, and potentially yield models that could perform well across a range of values for k during decoding. Due to the dependence of yt on the full decoding path zăt in the transformer-based model, it is not possible to improve over simply training in parallel across the different values of k. When training for multiple values of k, we encode the source sentence once, and then forward it multiple times in the decoder, once for each value of k. The 2D CNN-based architecture, however, does not include this full dependency, and allows parallel computation of all pθpyt|yăt,xďjq in single evaluation of the CNN. For this architecture we therefore use a loss of the form Lpθ,x,y, qq “ ´ |y| ÿ t“1 |x| ÿ j“1 qtj ln pθpyt|yăt,xďjq, (9) where qtj weight the terms corresponding to different positions in the decoding grid of Figure 1. The weights qtj can be set to zero/one to reflect all positions corresponding to wait-k decoding paths for one or more values of k. In our experiments we also consider the option of activating all loss terms above a certain wait-k decoding path. The latter corresponds to the sum of all loss terms on all wait-k1 decoding paths with k ď k1 ď |x|.1 1Summing the losses for all paths, though, would weight shared loss terms more importantly. The transformer-update architecture also avoids the full dependency on zăt, but in order to train from all loss terms above a certain decoding path, it requires a separate decoder run for each source context size, i.e. for each column of the decoding grid in Figure 1.In this section we present our experimental setup, followed by quantitative results.Datasets. We evaluate our approach on IWSLT14 En-De (Cettolo et al., 2014) and on the WMT15 De-En datasets.2 Replicating the setup of Elbayad et al. (2018) for IWSLT14 De-En, we train on 160K sentence pairs, develop on 7K held-out pairs and test on 6.7K pairs. The vocabulary consists of 8.8K types on the source side and 6.6K types on the target side that were obtained from a joint source and target byte pair encoding (BPE; Sennrich et al. 2016). For WMT15 De-En we use a joint vocabulary of 32k BPE types. We train on 4.5M pairs, develop on newstest2013 (3K pairs) and test on newstest15 (2.2K pairs). Evaluation metrics. We use beam-search decoding, with a beam of size 5, for offline models and only greedy decoding online. We evaluate the translation quality by measuring case-sensitive tokenized BLEU (Papineni et al., 2002) with multi-bleu.pl.3 Cho & Esipova (2016) measure the decoding latency using the proportion of the source sentence which has been read when producing target tokens, and average this proportion across the tokens in the generated sentence: AP “ 1|y| |y| ÿ t zt{|x|. (10) More recently, average lagging (AL) and differentiable average lagging (DAL), were proposed to measure translation latency (Ma et al., 2019; Cherry & Foster, 2019; Arivazhagan et al., 2019): AL “ 1 τ τ ÿ t zt ´ t´ 1 γ , DAL “ 1|y| |y| ÿ t z1t ´ t´ 1 γ , z1t “ # zt if t “ 1 maxpzt, z1t´1 ` 1γ q , where γ “ |y|{|x| and τ “ mintt | gptq “ |x|u. These metrics handle differing source and target lengths more properly, and have an intuitive interpretation as the average by which the system lags behind an ideal instantaneous translator. We refer the reader to (Cherry & Foster, 2019) for details. We report the mean of these metric across translations. In this section we report the average lagging (AL) metric, while the other metrics (AP, DAL, leading to a similar trend) are given in Appendix B. AL is chosen to make our results comparable with those of Ma et al. (2019) and Zheng et al. (2019b) for German-English WMT15 dataset. Architectures. For the pervasive attention model, to reduce the memory footprint, we use residual skip connections (He et al., 2016b) rather than the dense connections (Huang et al., 2017) used by Elbayad et al. (2018). See Appendix A for more details about other minor changes made to the original architecture. We consider an offline pervasive attention baseline (PA) that reads the source sequence bi-directionally via asymmetric convolution filters that are only masking the future targets. Note that this baseline is not directly applicable for low-latency decoding, due to the bidirectional source encoding. We use a second baseline where convolutions are masked along the source dimension, which we refer to as masked-PA baseline (MPA). For the Transformer model, we use a small architecture on IWSLT with an embedding dimension denc “ 512 for the encoder, ddec “ 256 for the decoder, and N “ 6, dff “ 1024,h “ 4,Pdrop “ 0.3. On WMT, we use a Transformer base (Vaswani et al., 2017) with tied embeddings. Similar to the pervasive attention baseline, we consider the offline baseline (T) with bidirectional self-attention in the source side and a masked baseline (MT) with left-to-right self-attention in the encoder. 2http://www.statmt.org/wmt15/ 3https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/ multi-bleu.perlIn Table 1 we report the offline performance of the Pervasive Attention (PA), Masked Pervasive Attention (MPA), Transformer (T) and Masked Transformer (MT) baselines. Each with greedy decoding (G) and with beam search (BS). Interestingly, the masked and non-masked versions perform quite similar, across both datasets and architectures. This validates the use of autoregressive source encoders in the low-latency models we consider below. The transformer and convolutional architectures perform similarly on IWSLT14, while the former performs significantly better on WMT15.In our first experiment we evaluate the wait-k online decoding for different MT and MPA models, trained for different wait-k decoding paths. We denote with k“8 the wait-until-end training where the full source is read before decoding. In each figure the offline results are added for reference, the offline model has a latency of AL “ |x|. Here we show results for IWSLT De-En, similar trends are observed for IWSLT En-De (see Appendix B.3). Impact of the architecture. Figure 3 presents the performance of models trained for wait-k decoding across a range of latencies keval P t1, 3, 5, 7, 9u. Each trained model is represented by a curve, by evaluating it across different wait-k decoding paths. For the convolutional architecture we observe that a single model trained for a relatively large value of k, e.g. 7 or 9, provides good performance when using it to generate using different k values for generation. The performance of the transformer-based model is more sensitive to the value of k used for training. This difference in sensitivity could be due to the use of input-independent convolution filters vs. self-attention which relies on scoring context to weight its features. Given an appropriate value for k during training, the transformer-based models yield better translations for a given latency level. 0 2 4 6 818 20 22 24 26 28 30 32 1 3 5 7 9 Average lagging B LE U k “ 1 k ě 1 k “ keval k ě keval 23 18 20 22 24 26 28 30 32Offline (a) MPA above path 0 2 4 6 818 20 22 24 26 28 30 32 Average lagging B LE U k “ 1 k “ 3 k “ 5 k P r1..5s 23 18 20 22 24 26 28 30 32 Offline (b) MT k P r1..5s 0 2 4 6 818 20 22 24 26 28 30 32 Average lagging B LE U k “ 9 k P r1..5s k P r1..9s 23 18 20 22 24 26 28 30 32 Offline (c) MT k P r1..9s Impact of multiple path training. For the convolutional model (MPA), we consider activating loss terms in the area above the wait-k path. Training on this area covers a combinatorially large number of paths with latency higher than k. The results in Figure 4a show that for the MPA architecture, multiple path training can be beneficial compared to single path (k“keval vs. k ě keval). More importantly, the model trained with all loss terms above the diagonal of the decoding grid (kě 1), performs equal or better than models trained with only a subset of the terms, across all tested latency levels. For transformer model (MT), training for all decoding paths above a certain wait-k path is prohibitively costly (since the cost is linear in the number of training paths). Therefore, we opted instead for joint training for a selected set of wait-k paths. Results in Figure 4b show that jointly training on the wait-k paths from k“1 to k“5 improves over the training on individual paths in lowlatency regimes (keval ď 3). For higher latency a relatively small drop in performance is observed compared to training with k “ 5. Where the convolutional model could be trained near optimal by training across all values of k, in Figure 4c we see that training the paths from k“1 to k“9 performs similarly to just training with k“9. The latter model is (near) optimal for ALě 3, but for very low-latency the model trained on k P r1..5s is better. Impact of hidden state updates. The standard decoding approach with a Transformer is to use previously evaluated hidden states to encode the target prefix written so far. This is not the case for MPA where new source context is integrated in the convolution for all previous time-steps. To bridge the gap between the two decoding paradigms, we update previous hidden states to account for new source contexts. In Table 2 we compare transformer models with and without hidden state updates, trained along one or multiple decoding paths. Among the 20 comparisons (4 models, and 5 wait-k decoders) we observe only a single case where state updates drop the BLEU score by 0.1. In all other cases, the state updates lead to improvements, by up to 2.4 BLEU points in very low latency settings.4 Experiments on the WMT15 De-En corpus. In Figure 5a we present our results on the WMT15 De-En task using transformer model, which gave the best results in the IWSLT experiments. The results confirm observations made on IWSLT: (i) Hidden state updates consistently improve the performance of the tested models. (ii) When using hidden state updates, a single low-latency model (k“7) performs better or comparable to using separate models trained specifically for the value of k used for decoding (keval). The main differences w.r.t. the IWSLT experimental results are observed for the model trained for offline decoding (k“8). Whereas on IWSLT it was competitive with the best models trained for online decoding, for this larger corpus with longer sentences, this is no longer the case and the offline model performs significantly worse than the ones specifically trained for online decoding. Comparison to state-of-the-art. Figure 5b compares our results with state-of-the-art performance reported by Ma et al. (2019) and Zheng et al. (2019b) for German-English translation on the WMT15 dataset. Our model (k“7, with update) establishes a new state of the art for this task, significantly improving over previous work across the full range of latency levels. It is important to note that to obtain our results we use a single trained model regardless of the latency level considered at decoding time, like Zheng et al. (2019b) but unlike Ma et al. (2019). Moreover, we use simple wait-k decoders to attain different latency levels. Zheng et al. (2019b), on the other hand, trained a data adaptive controller to decide on the read/write actions, but used an underlying decoder trained for offline translation. Given our results, we expect that further gains are possible by training controllers on top of our model, which is trained specifically for online decoding. Training time. We compare in Table 3 the training time of our models to the training time of the baseline and to that of our implementation of the original wait-k paper (Ma et al., 2019). If training on a wait-k path, we report the average training time of k P t1, 3, 5, 7, 9u. Updating either the encoder states to use bi-directionally or the prefix decoder states increases the training time dramatically. In fact, when training along the wait-k path, the encoder update scales the encoder forward-passing time by minp|x|, |y|q ´ k ` 1 to produce states for each context size. Similarly, the decoder update increases the decoder forward-passing time by the same factor to re-compute the states with growing context. Decoding speed. We compare in Table 4 the decoding speed of our models with and without updating the decoder states. We also include in the comparison the decoding speed if we update the encoder states as we advance in reading the source sequence, similar to Ma et al. (2019). To factor out the training approach, we only consider the models trained for offline decoding (MT and T of 4Table 2 does not report the AL values, which are comparable across different models given a fixed wait-k decoder, i.e. per column. See Appendix B for results with AL values. Table 1) and evaluate them on the wait-k paths. The reported speeds are the average of evaluating with k P t1, 3, 5, 7, 9u. The decoding speeds show that updating the prefix decoder states costs less than updating the encoder states with a bi-directional encoder on both GPU (IWSLT: ˆ1.8, WMT: ˆ1.2 speed-up) and CPU (IWSLT: ˆ1.8 , WMT: ˆ1.3 speed-up).We compared transformer and 2D convolutional architectures for online machine translation with “wait-k” decoders, and proposed improved training techniques for these models. We find the transformer architecture to perform best, and improved its performance using hidden state updates inspired from the 2D convolutional architecture. We find that training a single model for relatively high values of k, e.g. 7 or 9, yields a performance that is comparable to using separate models trained for each specific value of k. Training a single model for multiple values of k improves performance for very low-latency regimes. Our results establish a new state of the art for online machine translation on WMT15 De-En dataset. We improve over recent results obtained with “wait-k” decoders (Ma et al., 2019) and trained dataadaptive controllers (Zheng et al., 2019b) to schedule read/write actions. Our results, together with those of Zheng et al. (2019b), suggest that further performance improvements are possible by integrating our models trained for online decoding with data-adaptive controllers.In this section we provide more details on the modifications we made to the convolutional “pervasive attention” architecture of Elbayad et al. (2018). In Figure 6 we provide a schematic overview of the original convolutional architecture of Elbayad et al. (2018). In our work, we made the following modifications to reduce the memory footprint of the model and to improve its performance. 1. We added feed-forward 1ˆ1 convolutional layers after the masked convolutions for their important role in existing encoder architectures as they boost the representational power of the model. 2. We substituted the dense layer connections with residual ones to reduce the size of the features that keep increasing and costing more memory. 3. We opted for layer-normalization (Ba et al., 2016) instead of batch-normalization as it is more stable and more appropriate for causal sequences. 4. We use depth-wise separable convolutions for Conv2D(k) instead of ordinary convolutions. In Figure 7 we provide a schematic overview of the adapted convolutional architecture. Finally, we found that removing the layer normalizations all-together and summing the block outputs before projecting on the target vocabulary to give best results. The final high-level architecture with block output addition is given in Figure 8.In this appendix we provide additional evaluation results. 1. In addition to results in the main paper measuring latency with AL, in Appendix B.1 we provide results measuring latency using the AP and DAL metrics. 2. In Appendix B.2 we provide the numerical underlying the plots for IWSLT De-En in the main paper and appendix. 3. Finally, we provide results for the reverse En-De translation direction for both IWSLT in Appendix B.3 and WMT in Appendix B.4. B.1 IWSLT DE-EN: AP AND DAL LATENCY 0.5 0.6 0.7 0.8 0.918 20 22 24 26 28 30 32 1 3 5 7 9 Average proportion B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 2318 20 22 24 26 28 30 32Offline (a) MPA wait-k (AP) 0.5 0.6 0.7 0.8 0.918 20 22 24 26 28 30 32 1 3 5 7 9 Average proportion B LE U k “ 1 k ě 1 k “ keval k ě keval 1 18 20 22 24 26 28 30 32Offline (b) MPA above path (AP) 0 2 4 6 8 1018 20 22 24 26 28 30 32 1 3 5 7 9 Differentiable average lagging B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 23 18 20 22 24 26 28 30 32Offline (c) MPA wait-k (DAL) 0 2 4 6 8 1018 20 22 24 26 28 30 32 1 3 5 7 9 Differentiable average lagging B LE U k “ 1 k ě 1 k “ keval k ě keval 23 18 20 22 24 26 28 30 32Offline (d) MPA above path (DAL) 0.5 0.6 0.7 0.8 0.9 18 20 22 24 26 28 30 32 1 3 5 7 9 Average proportion B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 1 18 20 22 24 26 28 30 32 Offline (e) MT wait-k (AP) 0.5 0.6 0.7 0.8 0.918 20 22 24 26 28 30 32 Average proportion B LE U k “ 1 k “ 3 k “ 5 k P r1..5s 1 18 20 22 24 26 28 30 32 Offline (f) MT k P r1..5s (AP) 0.5 0.6 0.7 0.8 0.918 20 22 24 26 28 30 32 Average proportion B LE U k “ 9 k P r1..5s k P r1..9s 1 18 20 22 24 26 28 30 32 Offline (g) MT k P r1..9s (AP) 0 2 4 6 8 10 18 20 22 24 26 28 30 32 1 3 5 7 9 Differentiable average lagging B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 23 18 20 22 24 26 28 30 32 Offline (h) MT wait-k (DAL) 0 2 4 6 818 20 22 24 26 28 30 32 Differentiable average lagging B LE U k “ 1 k “ 3 k “ 5 k P r1..5s 23 18 20 22 24 26 28 30 32 Offline (i) MT k P r1..5s (DAL) 0 2 4 6 818 20 22 24 26 28 30 32 Differentiable average lagging B LE U k “ 9 k P r1..5s k P r1..9s 23 18 20 22 24 26 28 30 32 Offline (j) MT k P r1..9s (DAL) Figure 9: IWSLT De-En: Online decoding with single and multiple paths training. Measuring latency with AP and DAL, corresponding to figures 3 and 4 in the main paper where AL is used. B.2 IWSLT DE-E: NUMERICAL RESULTS B.3 IWSLT EN-DE k BLEU AP AL DAL 1 18.5 0.61 2.5 2.8 3 22.8 0.69 3.8 4.1 5 25.0 0.75 5.5 5.7 7 25.9 0.81 7.2 7.5 9 26.2 0.86 8.9 9.2 (a) MT, k “ 8 k BLEU AP AL DAL 1 18.7 0.62 2.6 3.0 3 23.2 0.69 3.9 4.2 5 25.4 0.75 5.4 5.8 7 26.4 0.81 7.1 7.4 9 26.4 0.85 8.7 9.1 (b) MT, k “ 7 k BLEU AP AL DAL 1 16.4 0.59 1.9 2.5 3 22.1 0.68 3.7 4.0 5 25.4 0.75 5.3 5.7 7 26.4 0.81 7.1 7.4 9 26.7 0.85 8.8 9.1 (c) MT, k “ keval k BLEU AP AL DAL 1 21.2 0.60 2.3 2.6 3 24.3 0.68 3.8 4.0 5 25.8 0.75 5.4 5.7 7 26.2 0.81 7.2 7.5 9 26.4 0.86 8.9 9.2 (d) MT, k “ 8 - Update k BLEU AP AL DAL 1 21.4 0.60 2.3 2.5 3 24.7 0.68 3.7 3.9 5 26.2 0.75 5.3 5.6 7 26.3 0.81 7.0 7.3 9 26.0 0.85 8.6 9.0 (e) MT, k “ 7 - Update k BLEU AP AL DAL 1 16.6 0.57 1.6 2.1 3 22.9 0.67 3.5 3.7 5 25.6 0.75 5.2 5.5 7 26.3 0.81 7.0 7.3 9 26.2 0.85 8.6 9.0 (f) MT, k “ keval - Update Table 6: Numerical results of IWSLT En-De 0.5 0.6 0.7 0.8 0.916 18 20 22 24 26 1 3 5 7 9 Average proportion B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 2216 18 20 22 24 26 Offline (a) MPA wait-k (AP) 0 2 4 6 816 18 20 22 24 26 1 3 5 7 9 Average lagging B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 22 16 18 20 22 24 26 Offline (b) MPA wait-k (AL) 0 2 4 6 8 1016 18 20 22 24 26 1 3 5 7 9 Differentiable average lagging B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 22 16 18 20 22 24 26 Offline (c) MPA wait-k (DAL) 0.5 0.6 0.7 0.8 0.916 18 20 22 24 26 1 3 5 7 9 Average proportion B LE U k “ 1 k ě 1 k “ keval k ě keval 1 16 18 20 22 24 26 Offline (d) MPA above path (AP) 0 2 4 6 8 1016 18 20 22 24 26 1 3 5 7 9 Average lagging B LE U k “ 1 k ě 1 k “ keval k ě keval 22 16 18 20 22 24 26 Offline (e) MPA above path (AL) 0 2 4 6 8 1016 18 20 22 24 26 1 3 5 7 9 Differentiable average lagging B LE U k “ 1 k ě 1 k “ keval k ě keval 22 16 18 20 22 24 26 Offline (f) MPA above path (DAL) 0.5 0.6 0.7 0.8 0.916 18 20 22 24 26 1 3 5 Average proportion B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 1 16 18 20 22 24 26 Offline (g) MT wait-k (AP) 0 2 4 6 8 1016 18 20 22 24 26 1 3 5 Average lagging B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 22 16 18 20 22 24 26 Offline (h) MT wait-k (AL) 0 2 4 6 8 1016 18 20 22 24 26 1 3 5 Differentiable average lagging B LE U k “ 1 k “ 3 k “ 5 k “ 7 k “ 9 k “ 8 22 16 18 20 22 24 26 Offline (i) MT wait-k (DAL) Figure 10: IWSLT En-De: Online decoding with single and multiple paths training. Measuring latency with AP, AL and DAL B.4 WMT EN-DE